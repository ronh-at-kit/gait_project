{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters defined\n"
     ]
    }
   ],
   "source": [
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2 \n",
    "IMAGE_INPUT_SIZE_W = 640\n",
    "IMAGE_INPUT_SIZE_H = 480\n",
    "\n",
    "IMAGE_AFTER_CONV_SIZE_W = 18\n",
    "IMAGE_AFTER_CONV_SIZE_H = 13\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_IO_SIZE = 18*13\n",
    "LSTM_HIDDEN_SIZE = 18*13\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 10 # size videos\n",
    "BATCH_SIZE = 4 #until now just batch_size = 1\n",
    "\n",
    "NR_EPOCHS = 1\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "learning_rate = 0.01 # reduce factos of 10 .. some epoch later.\n",
    "momentum = 0.9\n",
    "print(\"Hyperparameters defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  flows\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "crop_im_size = [186,250]\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "c.config['transformers']['DimensionResize']['dimension'] = TIMESTEPS\n",
    "#c.config['indexing']['people selection'] = [1]\n",
    "#c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "c.config['pose']['load'] = False\n",
    "c.config['flow']['load'] = True\n",
    "c.config['heatmaps']['load'] = False\n",
    "#c.config['scenes']['sequences'] = ['nm']\n",
    "#c.config['scenes']['angles'] = ['108']\n",
    "c.config['dataset_output'] = {\n",
    "#         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "        'data': ['flows'],\n",
    "        'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings \n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5\n",
      "Indices size: 5\n",
      "Split: 1\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined\n"
     ]
    }
   ],
   "source": [
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.avialable_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #input 640x480\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #input 638x478 output 319x239\n",
    "        self.conv2 = nn.Conv2d(6,16,3) # input 319x239 output 317x237\n",
    "        self.pool2 = nn.MaxPool2d(2,2) # input 317x237 output 158x118\n",
    "        self.conv3 = nn.Conv2d(16,6,3) # input 158x118 output 156x116\n",
    "        self.pool3 = nn.MaxPool2d(2,2) # input 156x116 output 78x58\n",
    "        self.conv4 = nn.Conv2d(6,3,3)  # input 78x58 output 76x56\n",
    "        self.pool4 = nn.MaxPool2d(2,2) # input 76x56 output 39x29\n",
    "        self.conv5 = nn.Conv2d(3,1,3)  # input 39x29 output 37x27\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #output 37x27 output 18x13\n",
    "        self.lstm1 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.lstm2 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.fc1 = nn.Linear(LSTM_IO_SIZE,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of LSTM\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        #print(\"Hidden:\", _hidden)\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device), \n",
    "                torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device))\n",
    "    def forward(self,x):\n",
    "#         print(\"Input list len:\",len(x))\n",
    "#         print(\"Input elemens size:\", x[0].size())\n",
    "#         batch_size = x[0].size()[0]\n",
    "\n",
    "        x_arr = torch.zeros(TIMESTEPS,BATCH_SIZE,1,IMAGE_AFTER_CONV_SIZE_H,IMAGE_AFTER_CONV_SIZE_W).to(self.avialable_device)\n",
    "        #print(\"X arr size\", x_arr.size())\n",
    "        #print(\"X size\", len(x))\n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i].float())))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = x_tmp_c5 # torch.squeeze(x_tmp_c5)\n",
    "        \n",
    "        x, hidden = self.lstm1(x_arr.view(TIMESTEPS,BATCH_SIZE,-1), self.hidden)\n",
    "        x, hidden = self.lstm2(x, self.hidden)\n",
    "        # the reshaping was taken from the documentation... and makes scense\n",
    "        x = x.view(TIMESTEPS,BATCH_SIZE,LSTM_HIDDEN_SIZE) #output.view(seq_len, batch, num_dir*hidden_size)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        #x = x.squeeze(1)\n",
    "        x = x.permute(1,2,0)\n",
    "        #print (\"Size network output\", x.shape)\n",
    "        return x\n",
    "print(\"Class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition done\n"
     ]
    }
   ],
   "source": [
    "### define and execute testing function\n",
    "def test_all_preds(model):\n",
    "    n_batches_test = len(test_loader)\n",
    "\n",
    "    #Time for printing\n",
    "    testing_start_time = time.time()\n",
    "\n",
    "    print('Start testing...')\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            data_in = [s.to(device) for s in inputs['flows']]\n",
    "            labels = labels.to(device)\n",
    "            if not labels.size()[0] == BATCH_SIZE:\n",
    "                # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "                continue\n",
    "            outputs = model(data_in)\n",
    "#             print(\"Out:\", len(outputs), outputs.size())\n",
    "#             print(\"Labels:\", len(labels), labels.size())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "            n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "            total += predicted.numel()\n",
    "            # print('predicted',predicted)\n",
    "            correct += predicted.numel() - n_errors\n",
    "            # print('labels',labels)\n",
    "    print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "    print('...testing finished')\n",
    "print(\"Definition done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set and evaluate computing time etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Expected loss with 3 different classes and 4 data elements: 4.394449154672439\n",
      "Batch size: 4\n",
      "Evaluating first element...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.conda/envs/gait_37/lib/python3.7/site-packages/pandas/core/computation/expressions.py:183: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  .format(op=op_str, alt_op=unsupported[op_str]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time steps:10, input sequence length:10\n",
      "Out: 4 torch.Size([4, 3, 10])\n",
      "Labels: 4 torch.Size([4, 10])\n",
      "Loss:1.088744044303894, expected loss:1.0986122886681098\n",
      "Time needed:33.65627980232239s\n",
      "Expected loss for total training data:  4.394449154672439\n",
      "Expected training time per epoch:0.5609379967053731 min\n",
      "Estimated total training time:0.009348966611756219 hours\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Start training...')\n",
    "print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "running_loss = 0.0\n",
    "#print(\"Data set length:\", len((train_loader)), \"Validation length:\", len(test_loader))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Evaluating first element...\")\n",
    "start_time = time.time()\n",
    "i, batch = next(iter(enumerate(train_loader)))\n",
    "inputs, labels = batch\n",
    "data_in = [s.to(device) for s in inputs['flows']]\n",
    "labels = labels.to(device)\n",
    "print(\"Time steps:{}, input sequence length:{}\".format(TIMESTEPS,len(data_in)))\n",
    "#print(\"NN input: \",len(flows),len(flows[0]),len(flows[0][0]),len(flows[0][0][0]),len(flows[0][0][0][0]))\n",
    "optimizer.zero_grad() \n",
    "outputs = test_net(data_in)\n",
    "print(\"Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\")\n",
    "print(\"Output format:\", len(outputs), outputs.size())\n",
    "print(\"Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\")\n",
    "print(\"Labels:\", len(labels), labels.size())\n",
    "#print(\"Labels content:\", labels)\n",
    "loss = criterion(outputs.float(),labels.long())\n",
    "loss.backward() \n",
    "optimizer.step()\n",
    "\n",
    "running_loss += loss.data.item()\n",
    "elapsed_time = time.time() - start_time;\n",
    "print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "print(\"Time needed:{}s\".format(elapsed_time))\n",
    "print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)/60))\n",
    "print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*NR_EPOCHS/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARATION FOR TRAINING\n",
    "loss_array = []\n",
    "learning_rate_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss epoch 0: 1.087401270866394, took 17.441821336746216s\n",
      "...Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd5e41aa160>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(NR_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        outputs = test_net(data_in)\n",
    "        loss = criterion(outputs.float(),labels.long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "    print(\"Loss epoch {}: {}, took {}s\".format(epoch, running_loss,time.time()-start_time))\n",
    "    loss_array.append(running_loss)\n",
    "    learning_rate_array.append(learning_rate)\n",
    "\n",
    "#test_all_preds(test_net) \n",
    "print('...Training finished')\n",
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DYNAMIC CHANGES:\n",
    "NR_EPOCHS = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss:4.394449154672439, last loss:1.087401270866394\n",
      "Batch size: 4\n",
      "Sequence length: 10\n",
      "Total epochs learnt: 1\n",
      "Start testing...\n",
      "Accuracy 35.00%\n",
      "...testing finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#EVALUATION\n",
    "print(\"Expected loss:{}, last loss:{}\".format((len(dataset)-split)*np.log(3),loss_array[-1]))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Sequence length:\",TIMESTEPS)\n",
    "print(\"Total epochs learnt:\", len(loss_array))\n",
    "plt.plot(learning_rate)\n",
    "plt.plot(loss_array)\n",
    "test_all_preds(test_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected loss for untrained set with Cross Entropy:\n",
    "k = number of classes\n",
    "N = number of labeled data in dataset\n",
    "loss_per_prediction = -log(1/k) = log(k)\n",
    "total_loss = sum(log(k)) = N*log(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)-split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
