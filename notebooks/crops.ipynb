{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use this script:\n",
    "This script can be used for two purposes:\n",
    "1. Determine optimal size for scenes to be cropped (2nd cell executed with CALC_CROP_SIZE = True)\n",
    "2. Generate crops of scenes (3rd cell with predetermined crop size or calculated crop size on the fly). \n",
    "3. Generate crops of flows\n",
    "----------\n",
    "There are two options (indicated by the flag PADDING): \n",
    "- Padded crops (region of interest changing size and the remaining space filled with zeros)(recommended for scenes?)\n",
    "- Non-padded crops (corresponds a \"cropped frame\" with the camera panning and following the acter as he moves along) (recommended for flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. crops\n",
      "loading configuration  crops\n",
      "[OK]\n",
      "image output size to be determined with [CROP_SIZE[0]+2*MARGIN,CROP_SIZE[1]+2*MARGIN]\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. crops\")\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "import numpy as np\n",
    "\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "\n",
    "# CROP_SIZE = [156.0, 230.0] # for five angles and five persons\n",
    "\n",
    "MARGIN = 10\n",
    "CROP_SIZE = [156.0, 230.0]\n",
    "CALC_CROP_SIZE = True\n",
    "CROP_FLOWS = True\n",
    "if not CALC_CROP_SIZE:\n",
    "    IMAGE_OUTPUT_SIZE = [CROP_SIZE[0]+2*MARGIN,CROP_SIZE[1]+2*MARGIN]\n",
    "    print(\"image output size\",IMAGE_OUTPUT_SIZE)\n",
    "else:\n",
    "    print(\"image output size to be determined with [CROP_SIZE[0]+2*MARGIN,CROP_SIZE[1]+2*MARGIN]\")\n",
    "PADDING = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calculation of crop sizes\n",
      "Using 12 poses\n",
      "Problematic pose in \n",
      "Problematic pose in \n",
      "Problematic pose in \n",
      "Crop size without margin: [156.0, 230.0]\n",
      "image output size [176.0, 250.0]\n"
     ]
    }
   ],
   "source": [
    "dataset = CasiaDataset()\n",
    "\n",
    "if CALC_CROP_SIZE:\n",
    "    print(\"Starting calculation of crop sizes\")\n",
    "    x_max_list = []\n",
    "    y_max_list = []\n",
    "    invalid_pose_counter = 0\n",
    "    print(\"Using\",dataset[0]['poses'].shape[0],\"poses\")\n",
    "    for item in dataset: \n",
    "    #for i in range(5):\n",
    "        #for listitem in dataset: print(listitem)\n",
    "        annotations = item['annotations']\n",
    "        scenes = item['scenes']\n",
    "        poses = item['poses']\n",
    "    #     print(\"Nr scenes, poses:\", len(scenes),poses.shape)\n",
    "        #print(\"Currently in\", dataset.dataset_items[i])\n",
    "        #print(\"Shapes:\")\n",
    "        #print(poses.shape)\n",
    "        x_tmp = []\n",
    "        y_tmp = []\n",
    "        for scene,nr in zip(scenes,range(poses.shape[2])):\n",
    "            pose = poses[:,:,nr]\n",
    "            p_x = pose[:,0]\n",
    "            p_y = pose[:,1]\n",
    "    #         print(p_x)\n",
    "    #         print(p_y)\n",
    "            if (all(p_x == 0) or all(p_y == 0)):\n",
    "                print(\"Problematic pose in \")\n",
    "            else:\n",
    "                x_max = np.max([p for p in p_x if p != 0])\n",
    "                x_min = np.min([p for p in p_x if p != 0])\n",
    "                abs_delta_x = x_max-x_min\n",
    "                x_max_list.append(abs_delta_x)\n",
    "                y_max = np.max([p for p in p_y if p != 0])\n",
    "                y_min = np.min([p for p in p_y if p != 0])\n",
    "                abs_delta_y = y_max-y_min\n",
    "                y_max_list.append(abs_delta_y) \n",
    "#                 if (abs_delta_x > 640):\n",
    "#                     print(\"Wrong pose\",pose)\n",
    "\n",
    "    CROP_SIZE = [np.ceil(max(x_max_list)), np.ceil(max(y_max_list))]\n",
    "    # print(\"Crop size as double:\",max(x_max_list),max(y_max_list))\n",
    "    print(\"Crop size without margin:\",CROP_SIZE)\n",
    "    IMAGE_OUTPUT_SIZE = [CROP_SIZE[0]+2*MARGIN,CROP_SIZE[1]+2*MARGIN]\n",
    "print(\"image output size\",IMAGE_OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 001\n",
      "Person 002\n",
      "Person 003\n",
      "Person 004\n",
      "Person 005\n",
      "WARNING: Invalid poses in  /mnt/DATA/HIWI/IBT/CASIA/images/005/cl-01/054/ pose nr 65 taking last available pose (or crop size if first)\n",
      "WARNING: Invalid poses in  /mnt/DATA/HIWI/IBT/CASIA/images/005/cl-02/018/ pose nr 3 taking last available pose (or crop size if first)\n",
      "WARNING: Invalid poses in  /mnt/DATA/HIWI/IBT/CASIA/images/005/cl-02/162/ pose nr 31 taking last available pose (or crop size if first)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# print(\"Using\", poses.shape[0],\"poses\")\n",
    "invalid_pose_counter = 0\n",
    "last_person = '000'\n",
    "\n",
    "for item, i in zip(dataset, range(len(dataset))):  \n",
    "    annotations = item['annotations']\n",
    "    \n",
    "    if CROP_FLOWS:\n",
    "        data_in = item['flows']\n",
    "    else:\n",
    "        data_in = item['scenes']\n",
    "    \n",
    "    poses = item['poses']\n",
    "    \n",
    "    #since valid scenes have an offset respect to thier annotation number\n",
    "    annotations_offset = int(annotations[''][0])\n",
    "    \n",
    "    person = '{:03d}'.format(dataset.dataset_items[i][0]) \n",
    "    sequence = dataset.dataset_items[i][1]\n",
    "    angle = '{:03d}'.format(dataset.dataset_items[i][2])\n",
    "    origin = '/mnt/DATA/HIWI/IBT/CASIA/'\n",
    "    folderpath_for_debug_only = origin + 'images/'+ person + '/'+ sequence + '/' + angle + '/'\n",
    "    if CROP_FLOWS:\n",
    "        pathlib.Path(origin + 'preprocessing/crops_flow/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'\n",
    ").mkdir(parents=True, exist_ok = True)\n",
    "    else:\n",
    "        pathlib.Path(origin + 'preprocessing/crops/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'\n",
    ").mkdir(parents=True, exist_ok = True)\n",
    "#     print(\"current folder: \", folderpath)\n",
    "    \n",
    "    if last_person != person:\n",
    "        print(\"Person\", person)\n",
    "    last_person = person\n",
    "\n",
    "    x0_last = 1\n",
    "    x1_last = 1\n",
    "    y0_last = 1 + CROP_SIZE[0]\n",
    "    y1_last = 1 + CROP_SIZE[1]\n",
    "    \n",
    "    for j in range(len(data_in)):\n",
    "    #for j in range(1):    \n",
    "        if (all(x == 0 for x in poses[:,0,j]) or all(y == 0 for y in poses[:,1,j])):\n",
    "            print(\"WARNING: Invalid poses in \", folderpath_for_debug_only, \"pose nr\",j, \"taking last available pose (or crop size if first)\")\n",
    "            x0 = x0_last\n",
    "            x1 = x1_last\n",
    "            y0 = y0_last\n",
    "            y1 = y1_last\n",
    "            invalid_pose_counter += 1\n",
    "        else:\n",
    "            # HERE x is horizontal direction and y is vertical direction\n",
    "            x0 = np.floor(np.min(poses[np.nonzero(poses[:,0,j]),0,j])) - MARGIN\n",
    "            x1 = np.floor(np.max(poses[np.nonzero(poses[:,0,j]),0,j])) + MARGIN\n",
    "            y0 = np.floor(np.min(poses[np.nonzero(poses[:,1,j]),1,j])) - MARGIN\n",
    "            y1 = np.floor(np.max(poses[np.nonzero(poses[:,1,j]),1,j])) + MARGIN\n",
    "            \n",
    "        #make sure image borders stay in range\n",
    "        x0 = max(0,x0)\n",
    "        x1 = min(x1,data_in[j].shape[1])\n",
    "        y0 = max(0,y0)\n",
    "        y1 = min(y1,data_in[j].shape[0])\n",
    "\n",
    "#       padding image until it gets the desired size\n",
    "        x_to_pad = IMAGE_OUTPUT_SIZE[0] - (x1 - x0)\n",
    "        y_to_pad = IMAGE_OUTPUT_SIZE[1] - (y1 - y0)\n",
    "        x_pad_l = np.floor(x_to_pad/2)\n",
    "        x_pad_r = x_to_pad - x_pad_l\n",
    "        y_pad_l = np.floor(y_to_pad/2)\n",
    "        y_pad_r = y_to_pad - y_pad_l\n",
    "        \n",
    "        if PADDING:\n",
    "            im_tmp = data_in[j][int(y0):int(y1),int(x0):int(x1)]\n",
    "            im_final = np.pad(im_tmp,[(int(y_pad_l),int(y_pad_r)),(int(x_pad_l),int(x_pad_r)),(0,0)],'constant')#,'constant', constant_values=((0, 0),(0,0)))\n",
    "        else: #NOT PADDING IMAGE WITH ZEROS\n",
    "            x0 = x0-x_pad_l\n",
    "            x1 = x1+x_pad_r\n",
    "            y0 = y0-y_pad_l\n",
    "            y1 = y1+y_pad_r\n",
    "#             print(\"Coordinates before\",x0_n,x1_n,y0_n,y1_n)\n",
    "            if x0 < 0:\n",
    "                x1 = x1 - x0\n",
    "                x0 = 0\n",
    "            elif data_in[j].shape[1] < x1:\n",
    "                x0 = x0 - x1 + data_in[j].shape[1]\n",
    "                x1 = data_in[j].shape[1]\n",
    "            if y0 < 0:\n",
    "                y1 = y1 - y0\n",
    "                y1 = 0\n",
    "            elif data_in[j].shape[0] < y1:\n",
    "                y0 = y0 - y1 + data_in[j].shape[0]\n",
    "                y1 = data_in[j].shape[0]            \n",
    "            \n",
    "            im_final = data_in[j][int(y0):int(y1),int(x0):int(x1)]\n",
    "\n",
    "            if (x1-x0 != IMAGE_OUTPUT_SIZE[0] or y1-y0 != IMAGE_OUTPUT_SIZE[1]):\n",
    "                print(\"WRONG IMAGE COORDINATES DETECTED \", folderpath_for_debug_only, \"pose nr\",j)\n",
    "                print(\"Coordinates after\",x0,x1,y0,y1)\n",
    "        plt.imshow(im_final)\n",
    "        framename = '{:03d}'.format(j+annotations_offset)\n",
    "        if CROP_FLOWS:\n",
    "            saving_path = origin + 'preprocessing/crops_flow/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'    \n",
    "            filename = person + '-' + sequence + '-' + angle + '_frame_' + framename + '_flow_crop.png'\n",
    "        else:\n",
    "            saving_path = origin + 'preprocessing/crops/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'\n",
    "            filename = person + '-' + sequence + '-' + angle + '_frame_' + framename + '_crop.png'\n",
    "        totalpath = saving_path + filename\n",
    "#         print(\"Image name\", filename)\n",
    "#         print(\"Total path\", totalpath)\n",
    "        Image.fromarray(im_final).save(totalpath)\n",
    "        x0_last = x0\n",
    "        x1_last = x1\n",
    "        y0_last = y0\n",
    "        y1_last = y1\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
