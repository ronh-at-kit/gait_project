{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. cnn_flows_prtrain\n",
      "Hyperparameters defined\n",
      "Bug with learning rate when sequence length is not the same for each element of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. cnn_flows_prtrain\")\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "\n",
    "VALIDATION_SPLIT = 0.5 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "TRAINING_PREPARATION = False\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "TIME_STEPS = 40\n",
    "\n",
    "SLICE_FROM_TIME_STEP = 0 #slices from timestep SLICE_FROM_TIMESTEP to the last one\n",
    "\n",
    "NR_EPOCHS = 200\n",
    "\n",
    "LR = 0.0001\n",
    "print(\"Hyperparameters defined\")\n",
    "print(\"Bug with learning rate when sequence length is not the same for each element of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "from gait_analysis import WeightWatcher\n",
    "from gait_analysis import AccuracyTrackerTrainTest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gait_analysis.Models import PretrainConvFlow\n",
    "from gait_analysis.Models.TransferConvLSTMFlow import TransferConvLSTMFlow\n",
    "\n",
    "print(\"done\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gait_analysis.settings as settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.configuration = 'cnn_flows_pretrain'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  cnn_flows_pretrain\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "cc = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.config['scenes']['load']=True\n",
    "cc.config['dataset_output']['data']=['flows','scenes']\n",
    "cc.config['transformers']['DimensionResize'] =  {'start': 5, 'dimension': 40, 'target': [\"scenes\",\"flows\",\"annotations\"],'annotations_offset': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing people selection:\n",
    "# cc.config['indexing']['people_selection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1655\n",
      "Indices size: 1655\n",
      "Split: 827\n"
     ]
    }
   ],
   "source": [
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    indices = np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in the input dataset dict_keys(['flows', 'scenes'])\n"
     ]
    }
   ],
   "source": [
    "print('keys in the input dataset',dataset[0][0].keys())\n",
    "# print(dataset.dataset_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: doesn't x = x.view(BATCH_SIZE,TIME_STEPS*LSTM_HIDDEN_FEATURES) mix up batch size order?\n",
      "TODO: BATCH_SIZE is currently fixed and one\n",
      "TODO: Is x_arr in device?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransferConvLSTMFlow(\n",
       "  (conv_net): PretrainConvFlow(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (4): ReLU()\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (7): ReLU()\n",
       "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (9): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (10): ReLU()\n",
       "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (12): Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (13): ReLU()\n",
       "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=90, out_features=120, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=120, out_features=20, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=20, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(90, 40, batch_first=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net_test = net.eval()\n",
    "net_test = TransferConvLSTMFlow()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net_test.load_state_dict(torch.load('/home/ron/PycharmProjects/Gait2019/gait_project/saved_models/TransferConvLSTMFlow/TransferConvLSTMFlow.pth'))\n",
    "net_test.to(device)\n",
    "net_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40, 3)\n",
      "[1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, labels = next((iter(test_loader)))\n",
    "    data_in = [s.to(device) for s in inputs['flows']]\n",
    "    labels = labels\n",
    "    labels = labels.detach().cpu().numpy()[0]\n",
    "    preds = net_test(data_in)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    print(preds.shape)\n",
    "    preds = np.argmax(preds, axis=2).astype(float)[0]\n",
    "    print(preds)\n",
    "    print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4cf422d630>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.stem(labels)\n",
    "plt.stem(preds-0.1,linefmt='r-',markerfmt='rx')\n",
    "plt.legend({'labels','predictions'},loc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-79afe7315bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mper_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mii\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/CasiaDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'scenes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_indices'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0min_frame_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scenes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'flows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_indices'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0min_frame_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/ScenesCasia.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Loading scene images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/ScenesCasia.py\u001b[0m in \u001b[0;36m_load_scene\u001b[0;34m(self, dataset_item)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mscene_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscene_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/ScenesCasia.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mscene_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscene_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/ScenesCasia.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(im_file)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} don\\'t exist.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# finding error to plot\n",
    "cases = []\n",
    "cnt=0\n",
    "ii = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        per_seq = dataset.dataset_items[test_indices[ii]]\n",
    "        ii+=1\n",
    "        data_in = [s.to(device) for s in inputs['flows']]\n",
    "        scenes = inputs['scenes']\n",
    "        # labels = labelslabel_element\n",
    "        labels = labels.detach().cpu().numpy()[0]\n",
    "        preds = net_test(data_in)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=2).astype(float)[0]\n",
    "        if not sum(preds-labels)==0:\n",
    "            preds_wrong=preds\n",
    "            labels_wrong = labels\n",
    "            scenes_wrong = scenes\n",
    "            per_seq_wrong = per_seq\n",
    "            cases.append((preds_wrong, labels_wrong, scenes_wrong, per_seq_wrong))\n",
    "            cnt +=1 \n",
    "            if cnt==100:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_opts = {0:\"LA-RG\",1:\"LG-RA\",2:'LG-RG'}\n",
    "PLOT_ALL = False\n",
    "for i, case in enumerate(cases[40:50]):\n",
    "    preds_wrong, labels_wrong, scenes_wrong, per_seq_wrong  = case[0], case[1], case[2], case[3]\n",
    "    \n",
    "    for i in range(len(preds_wrong)):\n",
    "        if PLOT_ALL:\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(scenes_wrong[i].squeeze().cpu().numpy()) \n",
    "            if preds_wrong[i]==labels_wrong[i]:\n",
    "                plt.title('label true {} vs label pred: {}, sequence: {}'.format(labels_opts[labels_wrong[i]],\n",
    "                                                              labels_opts[preds_wrong[i]],per_seq_wrong))\n",
    "            else:\n",
    "                plt.title('ERROR: label true {} vs label pred: {}, sequence: {}'.format(labels_opts[labels_wrong[i]],\n",
    "                                                          labels_opts[preds_wrong[i]], per_seq_wrong))\n",
    "        else:      \n",
    "            if preds_wrong[i] == labels_wrong[i]:\n",
    "                continue\n",
    "\n",
    "            index_after = i+1 if i+1<len(preds_wrong) else i\n",
    "            index_before = i-1 if i-1>0 else i\n",
    "            index_after_more = i+2 if i+2<len(preds_wrong) else i\n",
    "            index_before_more = i-2 if i-2>0 else i\n",
    "            plt.figure(figsize=(20,20))\n",
    "            plt.subplot(1,5,2)\n",
    "            plt.imshow(scenes_wrong[index_before].squeeze().cpu().numpy()) \n",
    "            plt.title('label true {} vs label pred: {}'.format(labels_opts[labels_wrong[index_before]],\n",
    "                                                              labels_opts[preds_wrong[index_before]]))\n",
    "            plt.subplot(1,5,3)\n",
    "            plt.imshow(scenes_wrong[i].squeeze().cpu().numpy())\n",
    "            plt.title('label true {} vs label pred: {}'.format(labels_opts[labels_wrong[i]],\n",
    "                                                              labels_opts[preds_wrong[i]]))\n",
    "            plt.subplot(1,5,4)\n",
    "            plt.imshow(scenes_wrong[index_after].squeeze().cpu().numpy()) \n",
    "            plt.title('label true {} vs label pred: {}'.format(labels_opts[labels_wrong[index_after]],\n",
    "                                                              labels_opts[preds_wrong[index_after]]))\n",
    "            plt.subplot(1,5,5)\n",
    "            plt.imshow(scenes_wrong[index_after_more].squeeze().cpu().numpy()) \n",
    "            plt.title('label true {} vs label pred: {}'.format(labels_opts[labels_wrong[index_after_more]],\n",
    "                                                              labels_opts[preds_wrong[index_after_more]]))\n",
    "            plt.subplot(1,5,1)\n",
    "            plt.imshow(scenes_wrong[index_before_more].squeeze().cpu().numpy()) \n",
    "            plt.title('label true {} vs label pred: {}'.format(labels_opts[labels_wrong[index_before_more]],\n",
    "                                                              labels_opts[preds_wrong[index_before_more]]))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.stem(labels_wrong)\n",
    "    plt.stem(preds_wrong-0.1,linefmt='r-',markerfmt='rx')\n",
    "    plt.legend({'preds','labels'},loc=1)\n",
    "    plt.title('case {}, persor/seq{}'.format(i,per_seq_wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_wrong[0].squeeze().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
