{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. cnn_flows_prtrain\n",
      "Hyperparameters defined\n",
      "Bug with learning rate when sequence length is not the same for each element of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. cnn_flows_prtrain\")\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "\n",
    "VALIDATION_SPLIT = 0.5 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "TRAINING_PREPARATION = False\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "TIME_STEPS = 40\n",
    "\n",
    "SLICE_FROM_TIME_STEP = 0 #slices from timestep SLICE_FROM_TIMESTEP to the last one\n",
    "\n",
    "NR_EPOCHS = 100\n",
    "\n",
    "LR = 0.0001\n",
    "print(\"Hyperparameters defined\")\n",
    "print(\"Bug with learning rate when sequence length is not the same for each element of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "from gait_analysis import WeightWatcher\n",
    "from gait_analysis import AccuracyTrackerTrainTest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gait_analysis.Models import PretrainConvFlow\n",
    "from gait_analysis.Models.TransferConvLSTMFlow import TransferConvLSTMFlow\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: doesn't x = x.view(BATCH_SIZE,TIME_STEPS*LSTM_HIDDEN_FEATURES) mix up batch size order?\n",
      "TODO: BATCH_SIZE is currently fixed and one\n",
      "TODO: Is x_arr in device?\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "net = TransferConvLSTMFlow()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "# monitor = WeightWatcher(net,['conv5','fc1'])\n",
    "tt_acc_tracker = AccuracyTrackerTrainTest([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  crops\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "# c.config['transformers']['DimensionResize']['dimension'] = TIME_STEPS\n",
    "# #c.config['indexing']['people selection'] = [1]\n",
    "# #c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "# c.config['pose']['load'] = False\n",
    "# c.config['flow']['load'] = True\n",
    "# c.config['heatmaps']['load'] = False\n",
    "# #c.config['scenes']['sequences'] = ['nm']\n",
    "# #c.config['scenes']['angles'] = ['108']\n",
    "# c.config['dataset_output'] = {\n",
    "# #         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "#         'data': ['flows'],\n",
    "#         'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1945\n",
      "Indices size: 1945\n",
      "Split: 972\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PREPARATION FOR TRAINING\n",
    "# loss_array = []\n",
    "# learning_rate_array = []\n",
    "\n",
    "# print('Start training...')\n",
    "# print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "# running_loss = 0.0\n",
    "# #print(\"Data set length:\", len((train_loader)), \"Validation length:\", len(test_loader))\n",
    "# print(\"Batch size:\", BATCH_SIZE)\n",
    "# print(\"Evaluating first element...\")\n",
    "# start_time = time.time()\n",
    "# i, batch = next(iter(enumerate(train_loader)))\n",
    "# inputs, labels = batch\n",
    "# data_in = [s.to(device) for s in inputs['flows']]\n",
    "# # print(\"Data in original\", data_in)\n",
    "\n",
    "\n",
    "# # print(\"Proof for normalized data:\", data_in[0][0])\n",
    "# labels = labels.to(device)\n",
    "# print(\"Time steps:{}, input sequence length:{}\".format(TIME_STEPS,len(data_in)))\n",
    "# #print(\"NN input: \",len(flows),len(flows[0]),len(flows[0][0]),len(flows[0][0][0]),len(flows[0][0][0][0]))\n",
    "# optimizer.zero_grad() \n",
    "# outputs = net(data_in)\n",
    "# print(\"Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\")\n",
    "# # print(\"Output format:\", len(outputs), outputs.size())\n",
    "# print(\"Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\")\n",
    "# # print(\"Labels:\", len(labels), labels.size())\n",
    "# # print(\"Slicing loss. Using loss from:\",SLICE_FROM_TIME_STEP,\"to\",TIME_STEPS)\n",
    "# print(\"Final label: \",labels[:,SLICE_FROM_TIME_STEP:TIME_STEPS].size())\n",
    "# print(\"Final output:\", outputs[:,:,SLICE_FROM_TIME_STEP:TIME_STEPS].size())\n",
    "# # print(\"Labels content:\", labels)\n",
    "# labels = labels.squeeze(0)\n",
    "# outputs = outputs.squeeze(0)\n",
    "# print(\"Dimensions:\",\"Labels\",labels.size(),\"Pred\",outputs.size())\n",
    "# # before: loss = criterion(outputs[:,:,SLICE_FROM_TIME_STEP:TIME_STEPS].float(),labels[:,SLICE_FROM_TIME_STEP:TIME_STEPS].long())\n",
    "# loss = criterion(outputs[SLICE_FROM_TIME_STEP:TIME_STEPS,:].float(),labels[SLICE_FROM_TIME_STEP:TIME_STEPS].long())\n",
    "# loss.backward() \n",
    "# optimizer.step()\n",
    "\n",
    "# running_loss += loss.data.item()\n",
    "# elapsed_time = time.time() - start_time;\n",
    "# loss_array.append(running_loss)\n",
    "# learning_rate_array.append(LR)\n",
    "# print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "# print(\"Time needed:{}s\".format(elapsed_time))\n",
    "# print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "# print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)/60))\n",
    "# print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*NR_EPOCHS/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "-----------------------------\n",
      "Epoch 0: Loss [0.05386544566429339,0.05094058904014215], Acc [0.987,0.987] took 102.17s\n",
      "Accuracy by class [[0.987, 0.991, 0.983],[0.992, 0.983, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 1: Loss [0.05247973655976609,0.049985915164173646], Acc [0.987,0.987] took 100.72s\n",
      "Accuracy by class [[0.987, 0.992, 0.984],[0.992, 0.982, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 2: Loss [0.051166922198373745,0.04872658070834246], Acc [0.988,0.988] took 103.1s\n",
      "Accuracy by class [[0.988, 0.992, 0.984],[0.992, 0.984, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 3: Loss [0.04981992291012665,0.04799404841292056], Acc [0.989,0.988] took 103.58s\n",
      "Accuracy by class [[0.988, 0.993, 0.985],[0.992, 0.984, 0.989]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 4: Loss [0.04855457424311386,0.04672599870304599], Acc [0.989,0.988] took 103.17s\n",
      "Accuracy by class [[0.989, 0.993, 0.986],[0.993, 0.984, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 5: Loss [0.047235862805875925,0.045711661399412566], Acc [0.99,0.988] took 103.02s\n",
      "Accuracy by class [[0.99, 0.993, 0.986],[0.994, 0.984, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 6: Loss [0.04599187503952311,0.0447933829757346], Acc [0.99,0.989] took 102.89s\n",
      "Accuracy by class [[0.99, 0.994, 0.987],[0.995, 0.984, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 7: Loss [0.044695333359536044,0.043763999310674276], Acc [0.991,0.989] took 102.78s\n",
      "Accuracy by class [[0.99, 0.994, 0.988],[0.995, 0.985, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 8: Loss [0.04346982571061609,0.042907438733998465], Acc [0.991,0.99] took 102.34s\n",
      "Accuracy by class [[0.991, 0.994, 0.989],[0.996, 0.986, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 9: Loss [0.04224534896535002,0.041863914328713364], Acc [0.992,0.99] took 102.64s\n",
      "Accuracy by class [[0.992, 0.994, 0.989],[0.996, 0.986, 0.989]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 10: Loss [0.04106097245107586,0.041183753737381514], Acc [0.992,0.991] took 103.94s\n",
      "Accuracy by class [[0.993, 0.995, 0.989],[0.997, 0.986, 0.99]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 11: Loss [0.03987280065545955,0.039979985386853184], Acc [0.993,0.992] took 103.0s\n",
      "Accuracy by class [[0.993, 0.995, 0.99],[0.997, 0.987, 0.992]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 12: Loss [0.03866755860239392,0.03904113101874704], Acc [0.993,0.991] took 102.65s\n",
      "Accuracy by class [[0.993, 0.995, 0.991],[0.997, 0.987, 0.991]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 13: Loss [0.03753938372618669,0.03788550554563394], Acc [0.993,0.992] took 101.27s\n",
      "Accuracy by class [[0.993, 0.995, 0.991],[0.997, 0.987, 0.992]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 14: Loss [0.03639472131773893,0.03709201167627167], Acc [0.993,0.992] took 102.88s\n",
      "Accuracy by class [[0.994, 0.995, 0.991],[0.997, 0.987, 0.991]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 15: Loss [0.035271819788539986,0.036034990258577496], Acc [0.994,0.993] took 103.19s\n",
      "Accuracy by class [[0.994, 0.996, 0.992],[0.997, 0.988, 0.993]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 16: Loss [0.03419771927267999,0.03496839941925362], Acc [0.994,0.992] took 104.11s\n",
      "Accuracy by class [[0.994, 0.995, 0.992],[0.998, 0.988, 0.992]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 17: Loss [0.0331674788353628,0.03403187565067235], Acc [0.994,0.993] took 103.67s\n",
      "Accuracy by class [[0.994, 0.996, 0.993],[0.998, 0.989, 0.994]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 18: Loss [0.03216022349397079,0.032951171646880745], Acc [0.994,0.993] took 103.94s\n",
      "Accuracy by class [[0.995, 0.996, 0.993],[0.998, 0.989, 0.992]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 19: Loss [0.0311356103615131,0.03211309156623206], Acc [0.995,0.994] took 102.54s\n",
      "Accuracy by class [[0.995, 0.996, 0.993],[0.998, 0.99, 0.995]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 20: Loss [0.030128264503295265,0.03121176786611185], Acc [0.995,0.994] took 102.97s\n",
      "Accuracy by class [[0.995, 0.996, 0.994],[0.998, 0.989, 0.994]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 21: Loss [0.029102295504084642,0.030118126582604744], Acc [0.995,0.994] took 102.54s\n",
      "Accuracy by class [[0.996, 0.997, 0.994],[0.998, 0.99, 0.995]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 22: Loss [0.028134752257664472,0.02899296612267484], Acc [0.995,0.995] took 103.58s\n",
      "Accuracy by class [[0.996, 0.997, 0.994],[0.998, 0.992, 0.994]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 23: Loss [0.02715237684080283,0.02828055004072728], Acc [0.996,0.995] took 103.89s\n",
      "Accuracy by class [[0.996, 0.997, 0.994],[0.998, 0.992, 0.996]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 24: Loss [0.026213611645365776,0.02739327039741794], Acc [0.996,0.995] took 102.08s\n",
      "Accuracy by class [[0.996, 0.997, 0.995],[0.998, 0.992, 0.995]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 25: Loss [0.025265433949304895,0.02712460326132592], Acc [0.996,0.995] took 102.57s\n",
      "Accuracy by class [[0.996, 0.997, 0.995],[0.999, 0.992, 0.996]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 26: Loss [0.02422154150627349,0.025811024456277108], Acc [0.996,0.995] took 103.12s\n",
      "Accuracy by class [[0.996, 0.997, 0.995],[0.999, 0.991, 0.995]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 27: Loss [0.02330845655010574,0.02478770748516466], Acc [0.996,0.995] took 103.88s\n",
      "Accuracy by class [[0.996, 0.997, 0.996],[0.999, 0.992, 0.996]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 28: Loss [0.02239651208207358,0.023785970545522554], Acc [0.997,0.996] took 103.35s\n",
      "Accuracy by class [[0.997, 0.997, 0.996],[0.999, 0.993, 0.996]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 29: Loss [0.021539111697983726,0.023064502992299427], Acc [0.997,0.996] took 102.2s\n",
      "Accuracy by class [[0.997, 0.997, 0.996],[0.999, 0.992, 0.996]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 30: Loss [0.020853516147540444,0.022706745211974154], Acc [0.997,0.996] took 102.15s\n",
      "Accuracy by class [[0.997, 0.997, 0.997],[0.999, 0.992, 0.996]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 31: Loss [0.02008018687444724,0.022558136841188348], Acc [0.997,0.996] took 102.15s\n",
      "Accuracy by class [[0.997, 0.998, 0.997],[0.999, 0.992, 0.998]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 32: Loss [0.019271204231929108,0.02089798174399351], Acc [0.998,0.997] took 102.57s\n",
      "Accuracy by class [[0.997, 0.998, 0.997],[0.999, 0.994, 0.998]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 33: Loss [0.01870667200120747,0.0206754452220219], Acc [0.998,0.997] took 101.87s\n",
      "Accuracy by class [[0.997, 0.999, 0.997],[0.999, 0.994, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 34: Loss [0.01794447864434179,0.02019850476331883], Acc [0.998,0.997] took 103.04s\n",
      "Accuracy by class [[0.998, 0.999, 0.997],[0.999, 0.994, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 35: Loss [0.01736030747997574,0.01977860281525666], Acc [0.998,0.997] took 104.35s\n",
      "Accuracy by class [[0.998, 0.999, 0.997],[0.999, 0.994, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 36: Loss [0.016938934309316128,0.019352756560967927], Acc [0.998,0.997] took 102.81s\n",
      "Accuracy by class [[0.998, 0.998, 0.998],[0.999, 0.992, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 37: Loss [0.015924510036100164,0.018383080007688405], Acc [0.998,0.998] took 103.03s\n",
      "Accuracy by class [[0.998, 0.999, 0.998],[0.999, 0.996, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 38: Loss [0.015077887640648685,0.01760617713248279], Acc [0.998,0.998] took 101.88s\n",
      "Accuracy by class [[0.998, 0.999, 0.998],[0.999, 0.995, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 39: Loss [0.014294493822879815,0.016819791464299405], Acc [0.999,0.998] took 101.31s\n",
      "Accuracy by class [[0.998, 0.999, 0.998],[0.999, 0.995, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 40: Loss [0.013653650242388212,0.016506628711520054], Acc [0.999,0.998] took 102.74s\n",
      "Accuracy by class [[0.998, 0.999, 0.999],[0.999, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 41: Loss [0.013017618731290344,0.016395032783469356], Acc [0.999,0.998] took 103.1s\n",
      "Accuracy by class [[0.998, 0.999, 0.999],[0.999, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 42: Loss [0.01251315321047407,0.015566817746975086], Acc [0.999,0.998] took 104.11s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.995, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 43: Loss [0.01231805954953682,0.01688595317077174], Acc [0.999,0.998] took 103.95s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.995, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 44: Loss [0.011956768627170902,0.016563385960786846], Acc [0.999,0.997] took 101.69s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.992, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 45: Loss [0.011350738930177464,0.015804165878387937], Acc [0.999,0.998] took 103.05s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 46: Loss [0.010909552238108742,0.015168687126192291], Acc [0.999,0.998] took 102.69s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 47: Loss [0.010370001838368378,0.015394878418169167], Acc [0.999,0.998] took 102.73s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 48: Loss [0.009662768918362511,0.014338389088931893], Acc [0.999,0.997] took 103.53s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.992, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 49: Loss [0.008798583744273777,0.013270329081398602], Acc [0.999,0.998] took 103.39s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 50: Loss [0.00836130256051424,0.012773350591403469], Acc [0.999,0.998] took 102.57s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.999, 0.995, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 51: Loss [0.007900303759000233,0.012095326154214964], Acc [0.999,0.998] took 103.72s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 52: Loss [0.007470628359534061,0.012666736919718534], Acc [1.0,0.998] took 103.29s\n",
      "Accuracy by class [[0.999, 1.0, 0.999],[1.0, 0.993, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 53: Loss [0.007126644826513722,0.013261025150290864], Acc [1.0,0.998] took 102.3s\n",
      "Accuracy by class [[0.999, 1.0, 0.999],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 54: Loss [0.00790032825085184,0.016010307234383247], Acc [0.999,0.997] took 103.12s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[1.0, 0.991, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 55: Loss [0.012686249297190486,0.019117416821795852], Acc [0.998,0.995] took 102.83s\n",
      "Accuracy by class [[0.998, 0.998, 0.997],[1.0, 0.989, 0.998]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 56: Loss [0.008362184588048025,0.01916846191459687], Acc [0.999,0.995] took 103.47s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[1.0, 0.987, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 57: Loss [0.007731411942846268,0.01407403183099227], Acc [0.999,0.997] took 103.17s\n",
      "Accuracy by class [[1.0, 0.999, 0.999],[0.999, 0.992, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 58: Loss [0.006670274955177478,0.01103228383390389], Acc [1.0,0.998] took 101.96s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 59: Loss [0.006213408391634117,0.01332262170035392], Acc [1.0,0.998] took 102.79s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 60: Loss [0.005715190285000904,0.012653141855770923], Acc [1.0,0.998] took 102.59s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.993, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 61: Loss [0.004975156472631954,0.011709746978531329], Acc [1.0,0.998] took 102.54s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 62: Loss [0.0046408874347870525,0.011184070775864507], Acc [1.0,0.998] took 102.81s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 63: Loss [0.004371336481520244,0.010932066248428516], Acc [1.0,0.998] took 103.82s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 64: Loss [0.004119885766166043,0.010536128916276791], Acc [1.0,0.998] took 103.47s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 65: Loss [0.00388262543648515,0.010376389618246903], Acc [1.0,0.998] took 101.19s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.995, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 66: Loss [0.003644215591895557,0.010145297498997898], Acc [1.0,0.998] took 102.71s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.995, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 67: Loss [0.0034198797847307663,0.010208596539499725], Acc [1.0,0.998] took 103.18s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 68: Loss [0.003198172194511972,0.0101301994252003], Acc [1.0,0.998] took 102.63s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 69: Loss [0.0030229414520714783,0.010095048830071625], Acc [1.0,0.998] took 102.86s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 70: Loss [0.002942115559715664,0.014987185721661302], Acc [1.0,0.996] took 103.36s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.989, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 71: Loss [0.017376624503267827,0.029725714219820155], Acc [0.995,0.989] took 102.57s\n",
      "Accuracy by class [[0.997, 0.995, 0.994],[0.999, 0.97, 0.998]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 72: Loss [0.007094058735215918,0.019315053346398596], Acc [0.999,0.995] took 103.12s\n",
      "Accuracy by class [[0.999, 0.998, 0.998],[1.0, 0.987, 0.998]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 73: Loss [0.003931258998673309,0.014106945609654293], Acc [1.0,0.996] took 102.67s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 0.995]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 74: Loss [0.003412316454641234,0.015572708724712274], Acc [1.0,0.996] took 103.48s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.988, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 75: Loss [0.002852371390222939,0.013256775880987682], Acc [1.0,0.997] took 103.39s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.991, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 76: Loss [0.0025719691833988438,0.011564002176668465], Acc [1.0,0.997] took 103.95s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.992, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 77: Loss [0.0023645239912375555,0.010178053659140414], Acc [1.0,0.999] took 103.48s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 78: Loss [0.0021901220647765414,0.009373027460792438], Acc [1.0,0.999] took 102.39s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 79: Loss [0.0020379183828512044,0.008812479350516312], Acc [1.0,0.999] took 103.42s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 80: Loss [0.0019032445074147846,0.008431374165571898], Acc [1.0,0.999] took 103.66s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 81: Loss [0.0017800637016852053,0.008190407901229489], Acc [1.0,0.999] took 104.17s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 82: Loss [0.0016648883199707628,0.007968817236872407], Acc [1.0,0.999] took 105.03s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 83: Loss [0.0015603529182426595,0.007956707817815371], Acc [1.0,0.999] took 102.43s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 84: Loss [0.0014567101211201473,0.008098651737560705], Acc [1.0,0.999] took 102.7s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 85: Loss [0.0014270154523644636,0.00811467177521715], Acc [1.0,0.998] took 102.95s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.995, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 86: Loss [0.009258154470169285,0.04762826197092936], Acc [0.997,0.984] took 103.06s\n",
      "Accuracy by class [[0.998, 0.998, 0.996],[0.994, 0.967, 0.991]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 87: Loss [0.013745837353536544,0.02358350408079588], Acc [0.996,0.991] took 103.37s\n",
      "Accuracy by class [[0.998, 0.995, 0.995],[1.0, 0.98, 0.995]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 88: Loss [0.004575472059514147,0.014636689002252429], Acc [0.999,0.996] took 102.35s\n",
      "Accuracy by class [[1.0, 0.999, 0.999],[1.0, 0.992, 0.998]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 89: Loss [0.0035723303716461946,0.02224976140316068], Acc [0.999,0.994] took 104.66s\n",
      "Accuracy by class [[1.0, 0.999, 1.0],[1.0, 0.984, 0.998]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 90: Loss [0.0025045383823881443,0.012687259054588187], Acc [1.0,0.997] took 102.59s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.993, 0.999]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 91: Loss [0.0017289703568732736,0.009700768408458483], Acc [1.0,0.998] took 102.48s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.994, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 92: Loss [0.0014788564796709427,0.006687895465786998], Acc [1.0,0.999] took 102.9s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.996, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 93: Loss [0.0013237434997653275,0.00560263374846276], Acc [1.0,0.999] took 102.09s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.997, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 94: Loss [0.0012079380595736932,0.005081281071076826], Acc [1.0,0.999] took 102.78s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.997, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 95: Loss [0.0011108297199642332,0.004594912336006401], Acc [1.0,0.999] took 103.62s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.997, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 96: Loss [0.0010269914988277696,0.004172085036480596], Acc [1.0,0.999] took 103.0s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.997, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 97: Loss [0.000953186741524314,0.00387660355357054], Acc [1.0,0.999] took 102.94s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.997, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 98: Loss [0.0008862238070427896,0.0036636174221360723], Acc [1.0,0.999] took 102.91s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.997, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 99: Loss [0.0008260939807272545,0.0035264086894917957], Acc [1.0,0.999] took 103.56s\n",
      "Accuracy by class [[1.0, 1.0, 1.0],[1.0, 0.998, 1.0]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "...Training finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-28f7127003b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gait_project/gait_analysis/Models/TransferConvLSTMFlow.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIME_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# print(\"X[i]\",x[i].size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mx_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;31m# print(\"X_arr[i]\", x[i].size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    start_time = time.time()\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_test = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "#         print(\"Time steps:{}, input sequence length:{}\".format(TIME_STEPS,len(data_in)))\n",
    "        #print(\"NN input: \",len(flows),len(flows[0]),len(flows[0][0]),len(flows[0][0][0]),len(flows[0][0][0][0]))\n",
    "        optimizer.zero_grad() \n",
    "        outputs = net(data_in)\n",
    "        labels = labels.squeeze(0).long()\n",
    "        outputs = outputs.squeeze(0).float()\n",
    "        \n",
    "#         print(\"Size output\",outputs.size(),\"and label\", labels.size())\n",
    "#         print(outputs)\n",
    "        \n",
    "        loss = criterion(outputs[:,SLICE_FROM_TIME_STEP:TIME_STEPS].float(),labels[SLICE_FROM_TIME_STEP:TIME_STEPS].long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_train += loss.data.item()/((1-VALIDATION_SPLIT)*len(dataset))\n",
    "        elapsed_time = time.time() - start_time;\n",
    "        \n",
    "        tt_acc_tracker.update_loss(running_loss_train,\"TRAIN\")\n",
    "#         acc_tracker.update_lr(LR)\n",
    "\n",
    "        prediction = torch.max(outputs,1)[1]\n",
    "        label_element = labels\n",
    "    \n",
    "        tt_acc_tracker.update_acc(prediction,label_element,\"TRAIN\")\n",
    "        #         print(\"Prediction\", labels)\n",
    "#         acc_tracker.update_acc(prediction,labels)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "            data_in = [s.to(device) for s in inputs['flows']]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(data_in)\n",
    "            labels = labels.squeeze(0).long()\n",
    "            outputs = outputs.squeeze(0).float()\n",
    "            loss = criterion(outputs[:,SLICE_FROM_TIME_STEP:TIME_STEPS],labels[SLICE_FROM_TIME_STEP:TIME_STEPS])\n",
    "            running_loss_test += loss.data.item()/(VALIDATION_SPLIT*len(dataset))\n",
    "            tt_acc_tracker.update_loss(running_loss_train,\"TEST\")\n",
    "\n",
    "            prediction = torch.max(outputs,1)[1]\n",
    "            label_element = labels\n",
    "\n",
    "            tt_acc_tracker.update_acc(prediction,label_element,\"TEST\")\n",
    "   \n",
    "              \n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Epoch {}: Loss [{},{}], Acc [{},{}] took {}s\".format(epoch, running_loss_train,running_loss_test,tt_acc_tracker.get_acc_tot(\"TRAIN\"),tt_acc_tracker.get_acc_tot(\"TEST\"), np.around(time.time()-start_time,decimals=2)))\n",
    "    print(\"Accuracy by class [{},{}] Total elements: [{},{}]\".format(tt_acc_tracker.get_acc(\"TRAIN\"),tt_acc_tracker.get_acc(\"TEST\"), tt_acc_tracker.get_labels_distribution(\"TRAIN\"),tt_acc_tracker.get_labels_distribution(\"TEST\")))\n",
    "\n",
    "    tt_acc_tracker.update_loss(running_loss_train,\"TRAIN\")\n",
    "    tt_acc_tracker.update_loss(running_loss_test,\"TEST\")\n",
    "    tt_acc_tracker.update_graph()\n",
    "    tt_acc_tracker.reset_acc_both()\n",
    "    tt_acc_tracker.update_lr(LR)\n",
    "\n",
    "print('...Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), '/home/ron/PycharmProjects/Gait2019/gait_project/saved_models/TransferConvLSTMFlow/TransferConvLSTMFlow.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
