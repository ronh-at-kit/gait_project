{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. cnn_flows_prtrain\n",
      "Hyperparameters defined\n",
      "Bug with learning rate when sequence length is not the same for each element of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. cnn_flows_prtrain\")\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "TRAINING_PREPARATION = False\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEPS = 40\n",
    "\n",
    "NR_EPOCHS = 20\n",
    "\n",
    "LR = 0.001\n",
    "# MOMENTUM = 0.9\n",
    "print(\"Hyperparameters defined\")\n",
    "print(\"Bug with learning rate when sequence length is not the same for each element of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "from gait_analysis import WeightWatcher\n",
    "from gait_analysis import AccuracyTracker\n",
    "from gait_analysis import AccuracyTrackerTrainTest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gait_analysis.Models import PretrainConvFlow\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  cnn_flows_pretrain\n",
      "[OK]\n",
      "Dataset size: 595\n",
      "Indices size: 595\n",
      "Split: 119\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "net = PretrainConvFlow.PretrainConvFlow()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "# monitor = WeightWatcher(net,['conv5','fc1'])\n",
    "tt_acc_tracker = AccuracyTrackerTrainTest([0,1,2])\n",
    "# test_acc_tracker = AccuracyTracker([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAINING_PREPARATION:\n",
    "#     print('Start training...')\n",
    "#     print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "#     running_loss = 0.0\n",
    "#     print(\"Batch size:\", BATCH_SIZE)\n",
    "#     print(\"Evaluating first element...\")\n",
    "#     start_time = time.time()\n",
    "#     i, batch = next(iter(enumerate(train_loader)))\n",
    "#     inputs, labels = batch\n",
    "#     #TODO make list disappear\n",
    "\n",
    "#     data_in_list = [s.to(device) for s in inputs['flows']]\n",
    "#     labels = labels.to(device)\n",
    "\n",
    "#     print(\"--------------------------\\nOriginal input:\")\n",
    "#     print(\"Data in: List length\", len(data_in_list), \"List element:\", data_in_list[0].size())\n",
    "#     print(\"Labels:\", labels.size())\n",
    "#     print(\"--------------------------\\nSingle element:\")\n",
    "#     data_element = data_in_list[0]\n",
    "#     label_element = labels[:,0].long()\n",
    "#     print(\"Data element:\", data_element.size())\n",
    "#     print(\"Label element:\", label_element.size())\n",
    "\n",
    "#     optimizer.zero_grad() \n",
    "#     output_element = net(data_element)\n",
    "\n",
    "#     loss = criterion(output_element,label_element)\n",
    "#     loss.backward() \n",
    "#     optimizer.step()\n",
    "\n",
    "#     running_loss += loss.data.item()\n",
    "#     elapsed_time = time.time() - start_time;\n",
    "#     print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "#     print(\"Time needed:{}s\".format(elapsed_time))\n",
    "#     print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "#     print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)*TIME_STEPS/BATCH_SIZE/60))\n",
    "#     print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*TIME_STEPS/BATCH_SIZE*NR_EPOCHS/3600))\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 0: Loss [0.22408502109624354,0.05944418907164847], Acc [0.907,0.858] took 122.88s\n",
      "Accuracy by class [[0.929, 0.9, 0.894],[0.899, 0.814, 0.863]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 1: Loss [0.22512031483451664,0.00018262863159180067], Acc [0.906,0.87] took 122.62s\n",
      "Accuracy by class [[0.926, 0.903, 0.893],[0.884, 0.821, 0.902]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 2: Loss [0.22170198557125506,1.4305114746095178e-05], Acc [0.909,0.864] took 122.61s\n",
      "Accuracy by class [[0.928, 0.901, 0.9],[0.897, 0.809, 0.888]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 3: Loss [0.221026215895506,0.009994745254516388], Acc [0.91,0.87] took 122.81s\n",
      "Accuracy by class [[0.931, 0.905, 0.895],[0.903, 0.824, 0.885]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 4: Loss [0.21706031355265348,0.00149559974670403], Acc [0.911,0.869] took 122.91s\n",
      "Accuracy by class [[0.929, 0.909, 0.897],[0.906, 0.821, 0.882]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 5: Loss [0.21107089554928848,9.012222290038962e-05], Acc [0.912,0.863] took 122.22s\n",
      "Accuracy by class [[0.935, 0.907, 0.897],[0.875, 0.804, 0.909]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 6: Loss [0.21453765780623746,0.0022506713867188528], Acc [0.909,0.877] took 122.46s\n",
      "Accuracy by class [[0.933, 0.904, 0.893],[0.899, 0.828, 0.903]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 7: Loss [0.21394572479044524,5.817413330077943e-05], Acc [0.911,0.865] took 122.28s\n",
      "Accuracy by class [[0.93, 0.904, 0.901],[0.899, 0.788, 0.909]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 8: Loss [0.20609898830413292,0.00041246414184569695], Acc [0.914,0.883] took 122.84s\n",
      "Accuracy by class [[0.934, 0.907, 0.902],[0.909, 0.837, 0.905]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 9: Loss [0.20502529047193135,0.7743753194808072], Acc [0.915,0.89] took 122.34s\n",
      "Accuracy by class [[0.938, 0.908, 0.903],[0.903, 0.865, 0.902]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 10: Loss [0.20306785848895118,2.8610229492187403e-06], Acc [0.915,0.852] took 122.47s\n",
      "Accuracy by class [[0.938, 0.907, 0.902],[0.929, 0.806, 0.83]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 11: Loss [0.2052653604930947,8.106231689453037e-06], Acc [0.915,0.875] took 122.45s\n",
      "Accuracy by class [[0.938, 0.912, 0.898],[0.907, 0.811, 0.909]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 12: Loss [0.1912114272750503,4.959106445312421e-05], Acc [0.921,0.888] took 122.41s\n",
      "Accuracy by class [[0.94, 0.913, 0.911],[0.923, 0.85, 0.894]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 13: Loss [0.1934784956799954,0.0016670227050780296], Acc [0.919,0.866] took 123.02s\n",
      "Accuracy by class [[0.938, 0.912, 0.909],[0.866, 0.806, 0.92]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 14: Loss [0.1873802644024899,8.225440979003838e-05], Acc [0.921,0.874] took 103.3s\n",
      "Accuracy by class [[0.943, 0.914, 0.909],[0.901, 0.815, 0.904]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 15: Loss [0.18958061687359762,8.583068847656892e-06], Acc [0.92,0.856] took 98.93s\n",
      "Accuracy by class [[0.941, 0.911, 0.909],[0.899, 0.771, 0.898]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 16: Loss [0.18551899790632392,0.0], Acc [0.923,0.876] took 98.77s\n",
      "Accuracy by class [[0.942, 0.917, 0.912],[0.927, 0.798, 0.905]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 17: Loss [0.18307176551123747,1.6212463378906074e-05], Acc [0.924,0.866] took 98.64s\n",
      "Accuracy by class [[0.942, 0.917, 0.915],[0.905, 0.78, 0.912]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 18: Loss [0.1847826429284757,7.152557373046851e-07], Acc [0.925,0.882] took 98.4s\n",
      "Accuracy by class [[0.95, 0.918, 0.909],[0.905, 0.82, 0.919]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 19: Loss [0.18143338366870462,0.16876101493836196], Acc [0.925,0.886] took 98.68s\n",
      "Accuracy by class [[0.948, 0.921, 0.908],[0.923, 0.849, 0.888]] Total elements: [[6091, 6090, 6884],[1460, 1591, 1709]]\n",
      "...Training finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    start_time = time.time()\n",
    "#     print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    running_test_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in_list = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "#         print(\"--------------------------\\nOriginal input:\")\n",
    "#         print(\"Data in: List length\", len(data_in_list), \"List element:\", data_in_list[0].size())\n",
    "#         print(\"Labels:\", labels.size())\n",
    "#         if (labels.size(1) != len(data_in_list)):\n",
    "#                 print(\"WARNING:\",labels.size(),len(data_in_list))\n",
    "        for j, data_element in enumerate(data_in_list):\n",
    "            label_element = labels[0:len(data_in_list),j].long()\n",
    "#             print(\"--------------------------\\nSingle element:\")\n",
    "#             print(\"Data element size:\", data_element.size())\n",
    "#             print(\"Label element size:\", label_element.size())\n",
    "#             print(\"Data element\", data_element)\n",
    "#             print(\"Label element\", label_element)\n",
    "#             print(\"Label element after\",label_element)\n",
    "\n",
    "#             print(\"Data element\", data_element[0][0])\n",
    "    \n",
    "            optimizer.zero_grad() \n",
    "            output_element = net(data_element)\n",
    "#             print(\"Output element\",output_element)\n",
    "#             print(\"Label element after\",label_element)\n",
    "            loss = criterion(output_element,label_element)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.data.item()/(len(dataset)*(1-VALIDATION_SPLIT))/TIME_STEPS*BATCH_SIZE\n",
    "#             print(\"losses:\",loss.data.item(),len(dataset),TIMESTEPS)\n",
    "#             print(\"Prediction:\", torch.max(output_element,1)[1])\n",
    "#             print(\"Labels:\", label_element.long())\n",
    "            prediction = torch.max(output_element,1)[1]\n",
    "            tt_acc_tracker.update_acc(prediction,label_element,\"TRAIN\")\n",
    "        \n",
    "    #TEST SET\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in_list = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        for j, data_element in enumerate(data_in_list):\n",
    "#             if (labels.size(1) != len(data_in_list)):\n",
    "#                 print(\"WARNING, labels and data_element length do not match in TEST Set\",i, \"element\", j)\n",
    "#                 print(labels[0:len(data_in_list),j].size())\n",
    "            label_element = labels[0:len(data_in_list),j].long()\n",
    "\n",
    "            output_element = net(data_element)\n",
    "            test_loss = criterion(output_element,label_element)\n",
    "            running_test_loss += loss.data.item()/(len(dataset)*VALIDATION_SPLIT)/TIME_STEPS\n",
    "\n",
    "            prediction = torch.max(output_element,1)[1]\n",
    "            tt_acc_tracker.update_acc(prediction,label_element,\"TEST\")\n",
    "\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Training set:\")\n",
    "    print(\"Epoch {}: Loss [{},{}], Acc [{},{}] took {}s\".format(epoch, running_loss,running_test_loss,tt_acc_tracker.get_acc_tot(\"TRAIN\"),tt_acc_tracker.get_acc_tot(\"TEST\"), np.around(time.time()-start_time,decimals=2)))\n",
    "    print(\"Accuracy by class [{},{}] Total elements: [{},{}]\".format(tt_acc_tracker.get_acc(\"TRAIN\"),tt_acc_tracker.get_acc(\"TEST\"), tt_acc_tracker.get_labels_distribution(\"TRAIN\"),tt_acc_tracker.get_labels_distribution(\"TEST\")))\n",
    "\n",
    "    tt_acc_tracker.update_graph()\n",
    "    tt_acc_tracker.reset_acc_both()\n",
    "    tt_acc_tracker.update_loss(running_loss)\n",
    "    tt_acc_tracker.update_lr(LR)\n",
    "\n",
    "#     monitor.update_weights(net,['conv5','fc1'])\n",
    "\n",
    "#test_all_preds(test_net) \n",
    "print('...Training finished')\n",
    "# plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_EPOCHS = 20\n",
    "LR = 0.0001\n",
    "tt_acc_tracker.reset_acc_both()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), '/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/001_all_seq_ts40_test.pt')\n",
    "# acc_tracker.write_to_csv('/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/001_all_seq_ts40_test/')\n",
    "# Later:\n",
    "# loaded_model = PretrainConvFlow.PretrainConvFlow()\n",
    "# loaded_model.load_state_dict(torch.load('/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/first_draft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# path = '/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/001_all_seq_ts40/'\n",
    "# with open(path + \"acc_tot.csv\", 'w') as file:\n",
    "#     wr = csv.writer(file)\n",
    "#     wr.writerow(acc_tracker.get_acc_tot_graph())\n",
    "# with open(path + \"loss.csv\", 'w') as file:\n",
    "#     wr = csv.writer(file)\n",
    "#     wr.writerow(acc_tracker.get_loss_graph())\n",
    "# with open(path + \"lr.csv\", 'w') as file:\n",
    "#     wr = csv.writer(file)\n",
    "#     wr.writerow(acc_tracker.get_lr_graph())\n",
    "# acc = acc_tracker.get_acc_graph()\n",
    "# for i in range(len(acc)):\n",
    "#     with open(path + \"acc\" + str(i) + \".csv\", 'w') as file:\n",
    "#         wr = csv.writer(file)\n",
    "#         wr.writerow(acc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_flows_pretrain = {\n",
    "#     'indexing':{\n",
    "#         #'grouping': 'person_sequence_angle',\n",
    "#         'selection': 'manual_people_sequence',     #  => 'auto'= by final annotation or\n",
    "#                                  #  => 'manual_people' = uses 'people' list\n",
    "#                                  #  => 'manual_people_sequence' uses combination of two lists 'people' and 'sequences'\n",
    "#         'people_selection': [1],\n",
    "#         # 'sequences_selection': ['nm-01']\n",
    "#         'sequences_selection': ['bg-01','bg-02','cl-01','cl-02','nm-01','nm-02','nm-03','nm-04','nm-05','nm-06']\n",
    "#         },\n",
    "#     'pose': {\n",
    "#         'load': False,\n",
    "#         'preprocess': False,\n",
    "#         'D': 2 ,\n",
    "#         # the complete list is:\n",
    "#         #'body_keypoints_include_list': ['LAnkle' , 'RAnkle' , 'LKnee' , 'RKnee' , 'RHip' , 'LHip' , 'RBigToe' ,\n",
    "#         #                                'LBigToe' , 'RSmallToe' , 'LSmallToe' , 'RHeel' , 'LHeel']\n",
    "#         'body_keypoints_include_list': ['LAnkle','RAnkle','LKnee','RKnee','RHip','LHip']\n",
    "#         },\n",
    "#     'flow': {\n",
    "#         'load':True,\n",
    "#         'preprocess' : True,\n",
    "#         'crops': True,\n",
    "#         'method' : 'dense',\n",
    "#         'load_patches' : True,\n",
    "#         'patch_size' : 5\n",
    "#         },\n",
    "#     'scenes':{\n",
    "#         'load':False,\n",
    "#         'preprocess': False,\n",
    "#         'crops' : False,\n",
    "#         'gray_scale' : False,\n",
    "#         'load_tracked' : False,\n",
    "#         'sequences': ['nm'],\n",
    "#         'angles': [90]\n",
    "#     },\n",
    "#     'heatmaps':{\n",
    "#         'load':False,\n",
    "#         'preprocess': False,\n",
    "#         'body_keypoints_include_list' : ['LAnkle','RAnkle']\n",
    "#     },\n",
    "#     'dataset_output' : {\n",
    "#         'data': [\"flows\"],\n",
    "#         'label': \"annotations\"\n",
    "#     },\n",
    "#     'transformers':{\n",
    "#         # 'Crop':{'include list':['LAnkle','RAnkle'],'output_size':256,'target':'flows'}\n",
    "#         # 'SpanImagesList': {'remove':True, 'names': [\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],'target': [\"heatmaps\"]},\n",
    "#         # 'Rescale': {'output_size' : (640,480), 'target': [\"flows\"]},\n",
    "#         'AnnotationToLabel': {'target': [\"annotations\"]},\n",
    "#         'Transpose' : {'swapping': (2, 0, 1) , 'target': [\"flows\"]},\n",
    "#         'Normalize': {'target': [\"flows\"]},\n",
    "#         'DimensionResize' : {'start': 5, 'dimension': 40, 'target': [\"flows\",\"annotations\"],'annotations_offset': 1},\n",
    "#         'ToTensor': {'target':[\"flows\",\"annotations\"]}\n",
    "#     }\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
