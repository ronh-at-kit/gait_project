{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use this script:\n",
    "This script can be used for two purposes:\n",
    "1. Determine optimal size for scenes to be cropped (2nd cell executed with CALC_CROP_SIZE = True)\n",
    "2. Generate crops of scenes (3rd cell with predetermined crop size or calculated crop size on the fly). \n",
    "3. Generate crops of flows\n",
    "----------\n",
    "There are two options (indicated by the flag PADDING): \n",
    "- Padded crops (region of interest changing size and the remaining space filled with zeros)(recommended for scenes?)\n",
    "- Non-padded crops (corresponds a \"cropped frame\" with the camera panning and following the acter as he moves along) (recommended for flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. crops\n",
      "loading configuration  crops\n",
      "[OK]\n",
      "image output size [176.0, 250.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. crops\")\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "import numpy as np\n",
    "\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "\n",
    "MARGIN = 10\n",
    "CROP_SIZE = [156.0, 230.0]\n",
    "CALC_CROP_SIZE = False\n",
    "CROP_FLOWS = True\n",
    "if not CALC_CROP_SIZE:\n",
    "    IMAGE_OUTPUT_SIZE = [CROP_SIZE[0]+2*MARGIN,CROP_SIZE[1]+2*MARGIN]\n",
    "    print(\"image output size\",IMAGE_OUTPUT_SIZE)\n",
    "else:\n",
    "    print(\"image output size to be determined with [CROP_SIZE[0]+2*MARGIN,CROP_SIZE[1]+2*MARGIN]\")\n",
    "PADDING = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image output size [176.0, 250.0]\n"
     ]
    }
   ],
   "source": [
    "dataset = CasiaDataset()\n",
    "\n",
    "if CALC_CROP_SIZE:\n",
    "    print(\"Starting calculation of crop sizes\")\n",
    "    x_max_list = []\n",
    "    y_max_list = []\n",
    "    invalid_pose_counter = 0\n",
    "    print(\"Using\",dataset[0]['poses'].shape[0],\"poses\")\n",
    "    for item in dataset: \n",
    "    #for i in range(5):\n",
    "        #for listitem in dataset: print(listitem)\n",
    "        annotations = item['annotations']\n",
    "        scenes = item['scenes']\n",
    "        poses = item['poses']\n",
    "    #     print(\"Nr scenes, poses:\", len(scenes),poses.shape)\n",
    "        #print(\"Currently in\", dataset.dataset_items[i])\n",
    "        #print(\"Shapes:\")\n",
    "        #print(poses.shape)\n",
    "        x_tmp = []\n",
    "        y_tmp = []\n",
    "        for scene,nr in zip(scenes,range(poses.shape[2])):\n",
    "            pose = poses[:,:,nr]\n",
    "            p_x = pose[:,0]\n",
    "            p_y = pose[:,1]\n",
    "    #         print(p_x)\n",
    "    #         print(p_y)\n",
    "            if (all(p_x == 0) or all(p_y == 0)):\n",
    "                print(\"Problematic pose in \")\n",
    "            else:\n",
    "                x_max = np.max([p for p in p_x if p != 0])\n",
    "                x_min = np.min([p for p in p_x if p != 0])\n",
    "                abs_delta_x = x_max-x_min\n",
    "                x_max_list.append(abs_delta_x)\n",
    "                y_max = np.max([p for p in p_y if p != 0])\n",
    "                y_min = np.min([p for p in p_y if p != 0])\n",
    "                abs_delta_y = y_max-y_min\n",
    "                y_max_list.append(abs_delta_y) \n",
    "#                 if (abs_delta_x > 640):\n",
    "#                     print(\"Wrong pose\",pose)\n",
    "\n",
    "    CROP_SIZE = [np.ceil(max(x_max_list)), np.ceil(max(y_max_list))]\n",
    "    # print(\"Crop size as double:\",max(x_max_list),max(y_max_list))\n",
    "    print(\"Crop size without margin:\",CROP_SIZE)\n",
    "    IMAGE_OUTPUT_SIZE = [CROP_SIZE[0]+2*MARGIN,CROP_SIZE[1]+2*MARGIN]\n",
    "print(\"image output size\",IMAGE_OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 040\n",
      "Person 041\n",
      "Person 042\n",
      "Person 043\n",
      "WARNING: Invalid poses in  /home/ron/PycharmProjects/Gait2019/CASIA/images/043/bg-02/054/ pose nr 60 taking last available pose (or crop size if first)\n",
      "Person 044\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "WARNING: Invalid poses in  /home/ron/PycharmProjects/Gait2019/CASIA/images/044/cl-02/126/ pose nr 0 taking last available pose (or crop size if first)\n",
      "Person 045\n",
      "WARNING: Invalid poses in  /home/ron/PycharmProjects/Gait2019/CASIA/images/045/cl-01/162/ pose nr 56 taking last available pose (or crop size if first)\n",
      "WARNING: Invalid poses in  /home/ron/PycharmProjects/Gait2019/CASIA/images/045/nm-02/054/ pose nr 73 taking last available pose (or crop size if first)\n",
      "Person 046\n",
      "Person 047\n",
      "Person 048\n",
      "Person 049\n",
      "Person 050\n",
      "Person 051\n",
      "Person 052\n",
      "Person 053\n",
      "Person 054\n",
      "Person 055\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "Len flows and poses 65 64\n",
      "WARNING: Invalid poses in  /home/ron/PycharmProjects/Gait2019/CASIA/images/055/nm-02/054/ pose nr 61 taking last available pose (or crop size if first)\n",
      "Person 056\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "/home/ron/PycharmProjects/Gait2019/CASIA/preprocessing/flow/056/nm-02/nm-02-018/056-nm-02-018_frame_093_flow.png don't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7ba78c01d7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlast_person\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'000'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/CasiaDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'flows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_indices'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0min_frame_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flows'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'heatmaps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_indices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_frame_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Loading scene images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36m_load_flow\u001b[0;34m(self, dataset_item)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m#im = im.astype('uint8')#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mscene_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscene_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m#im = im.astype('uint8')#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mscene_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscene_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Gait2019/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(im_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} don\\'t exist.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m#im = cv2.astype('uint8')#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: /home/ron/PycharmProjects/Gait2019/CASIA/preprocessing/flow/056/nm-02/nm-02-018/056-nm-02-018_frame_093_flow.png don't exist."
     ]
    }
   ],
   "source": [
    "# print(\"Using\", poses.shape[0],\"poses\")\n",
    "invalid_pose_counter = 0\n",
    "last_person = '000'\n",
    "\n",
    "for item, i in zip(dataset, range(len(dataset))):  \n",
    "    annotations = item['annotations']\n",
    "    \n",
    "    if CROP_FLOWS:\n",
    "        data_in = item['flows']\n",
    "    else:\n",
    "        data_in = item['scenes']\n",
    "    \n",
    "    poses = item['poses']\n",
    "    \n",
    "    #since valid scenes have an offset respect to thier annotation number\n",
    "    annotations_offset = int(annotations[''][0])\n",
    "    \n",
    "    person = '{:03d}'.format(dataset.dataset_items[i][0]) \n",
    "    sequence = dataset.dataset_items[i][1]\n",
    "    angle = '{:03d}'.format(dataset.dataset_items[i][2])\n",
    "    origin = '/home/ron/PycharmProjects/Gait2019/CASIA/'\n",
    "    folderpath_for_debug_only = origin + 'images/'+ person + '/'+ sequence + '/' + angle + '/'\n",
    "    if CROP_FLOWS:\n",
    "        pathlib.Path(origin + 'preprocessing/crops_flow/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'\n",
    ").mkdir(parents=True, exist_ok = True)\n",
    "    else:\n",
    "        pathlib.Path(origin + 'preprocessing/crops/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'\n",
    ").mkdir(parents=True, exist_ok = True)\n",
    "#     print(\"current folder: \", folderpath)\n",
    "    \n",
    "    if last_person != person:\n",
    "        print(\"Person\", person)\n",
    "    last_person = person\n",
    "\n",
    "    x0_last = 1\n",
    "    x1_last = 1\n",
    "    y0_last = 1 + CROP_SIZE[0]\n",
    "    y1_last = 1 + CROP_SIZE[1]\n",
    "    if len(data_in) != len(poses):\n",
    "        print(\"Warning, unequal length of flows and poses:\",len(data_in),len(poses))\n",
    "    for j in range(min(len(data_in),len(poses))):\n",
    "        pose_x = poses[j][:,0]\n",
    "        pose_y = poses[j][:,1]\n",
    "        if (all(x == 0 for x in pose_x) or all(y == 0 for y in pose_y)):\n",
    "            print(\"WARNING: Invalid poses in \", folderpath_for_debug_only, \"pose nr\",j, \"taking last available pose (or crop size if first)\")\n",
    "            x0 = x0_last\n",
    "            x1 = x1_last\n",
    "            y0 = y0_last\n",
    "            y1 = y1_last\n",
    "            invalid_pose_counter += 1\n",
    "        else:\n",
    "            # HERE x is horizontal direction and y is vertical direction\n",
    "            x0 = np.floor(np.min(pose_x)) - MARGIN\n",
    "            x1 = np.floor(np.max(pose_x)) + MARGIN\n",
    "            y0 = np.floor(np.min(pose_y)) - MARGIN\n",
    "            y1 = np.floor(np.max(pose_y)) + MARGIN\n",
    "            \n",
    "        #make sure image borders stay in range\n",
    "        x0 = max(0,x0)\n",
    "        x1 = min(x1,data_in[j].shape[1])\n",
    "        y0 = max(0,y0)\n",
    "        y1 = min(y1,data_in[j].shape[0])\n",
    "\n",
    "#       padding image until it gets the desired size\n",
    "        x_to_pad = IMAGE_OUTPUT_SIZE[0] - (x1 - x0)\n",
    "        y_to_pad = IMAGE_OUTPUT_SIZE[1] - (y1 - y0)\n",
    "        x_pad_l = np.floor(x_to_pad/2)\n",
    "        x_pad_r = x_to_pad - x_pad_l\n",
    "        y_pad_l = np.floor(y_to_pad/2)\n",
    "        y_pad_r = y_to_pad - y_pad_l\n",
    "        \n",
    "        if PADDING:\n",
    "            im_tmp = data_in[j][int(y0):int(y1),int(x0):int(x1)]\n",
    "            im_final = np.pad(im_tmp,[(int(y_pad_l),int(y_pad_r)),(int(x_pad_l),int(x_pad_r)),(0,0)],'constant')#,'constant', constant_values=((0, 0),(0,0)))\n",
    "        else: #NOT PADDING IMAGE WITH ZEROS\n",
    "            x0 = x0-x_pad_l\n",
    "            x1 = x1+x_pad_r\n",
    "            y0 = y0-y_pad_l\n",
    "            y1 = y1+y_pad_r\n",
    "#             print(\"Coordinates before\",x0_n,x1_n,y0_n,y1_n)\n",
    "            if x0 < 0:\n",
    "                x1 = x1 - x0\n",
    "                x0 = 0\n",
    "            elif data_in[j].shape[1] < x1:\n",
    "                x0 = x0 - x1 + data_in[j].shape[1]\n",
    "                x1 = data_in[j].shape[1]\n",
    "            if y0 < 0:\n",
    "                y1 = y1 - y0\n",
    "                y1 = 0\n",
    "            elif data_in[j].shape[0] < y1:\n",
    "                y0 = y0 - y1 + data_in[j].shape[0]\n",
    "                y1 = data_in[j].shape[0]            \n",
    "            \n",
    "            im_final = data_in[j][int(y0):int(y1),int(x0):int(x1)]\n",
    "\n",
    "            if (x1-x0 != IMAGE_OUTPUT_SIZE[0] or y1-y0 != IMAGE_OUTPUT_SIZE[1]):\n",
    "                print(\"WRONG IMAGE COORDINATES DETECTED \", folderpath_for_debug_only, \"pose nr\",j)\n",
    "                print(\"Coordinates after\",x0,x1,y0,y1)\n",
    "        plt.imshow(im_final)\n",
    "        framename = '{:03d}'.format(j+annotations_offset)\n",
    "        if CROP_FLOWS:\n",
    "            saving_path = origin + 'preprocessing/crops_flow/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'    \n",
    "        else:\n",
    "            saving_path = origin + 'preprocessing/crops/'+ person + '/' + sequence + '/' + sequence + '-' + angle + '/'\n",
    "        filename = person + '-' + sequence + '-' + angle + '_frame_' + framename + '_flow.png'\n",
    "        totalpath = saving_path + filename\n",
    "#         print(\"Image name\", filename)\n",
    "#         print(\"Total path\", totalpath)\n",
    "        Image.fromarray(im_final).save(totalpath)\n",
    "        x0_last = x0\n",
    "        x1_last = x1\n",
    "        y0_last = y0\n",
    "        y1_last = y1\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(poses[:,0,j])\n",
    "pose_tmp = poses[1][:,j]\n",
    "print(pose_tmp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
