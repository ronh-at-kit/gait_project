{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configuration needed\n",
      "Class defined\n",
      "Shape (10, 3, 228, 228)\n"
     ]
    }
   ],
   "source": [
    "print(\"No configuration needed\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2\n",
    "IMAGE_INPUT_SIZE = 228\n",
    "IMAGE_AFTER_CONV_SIZE = 5\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_IO_SIZE = IMAGE_AFTER_CONV_SIZE*IMAGE_AFTER_CONV_SIZE\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 10\n",
    "BATCH_SIZE = 1 #until now just batch_size = 1\n",
    "NR_EPOCHS = 400\n",
    "LEARNING_RATE = 0.1\n",
    "loss_array = []\n",
    "\n",
    "#USE RANDOM IMAGES TO SET UP WORKING EXAMPLE\n",
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #input 388x388\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #input 48x48 output 24x24\n",
    "        self.conv2 = nn.Conv2d(6,16,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.conv3 = nn.Conv2d(16,6,3)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(6,3,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "        self.conv5 = nn.Conv2d(3,1,3)\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #output 5x5\n",
    "        self.lstm = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_IO_SIZE,\n",
    "                            NR_LSTM_UNITS)\n",
    "        self.fc1 = nn.Linear(LSTM_IO_SIZE,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of LSTM\n",
    "        self._hidden = (torch.randn(NR_LSTM_UNITS, BATCH_SIZE, LSTM_IO_SIZE), \n",
    "                        torch.randn(NR_LSTM_UNITS, BATCH_SIZE, LSTM_IO_SIZE))\n",
    "        #print(\"Hidden:\", _hidden)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(\"Input:\", x.size())\n",
    "        x = x.float() #necessary for some reason\n",
    "        x_arr = torch.zeros(TIMESTEPS,IMAGE_AFTER_CONV_SIZE,IMAGE_AFTER_CONV_SIZE)\n",
    "        #print(\"X arr size\", x_arr.size())\n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i].unsqueeze(0))))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = torch.squeeze(x_tmp_c5)\n",
    "        #\n",
    "        x, _hidden = self.lstm(x_arr.view(TIMESTEPS,BATCH_SIZE,-1), self._hidden)\n",
    "        x = x.view(-1,LSTM_IO_SIZE)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "print(\"Class defined\")\n",
    "\n",
    "std_dev = 0.1\n",
    "#rand_arr = np.random.rand(TIMESTEPS,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)\n",
    "arr_1 = np.full((1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE),-0.5)\n",
    "arr_2 = np.full((1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE),0.0)\n",
    "arr_3 = np.full((1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE),0.5)\n",
    "arr_full = np.concatenate(\n",
    "    (arr_1 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_2 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_3 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_1 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_2 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_3 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_1 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_2 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_3 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)),\n",
    "     arr_1 + np.random.normal(0,std_dev,(1,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE)))\n",
    ")\n",
    "print(\"Shape\", np.shape(arr_full))\n",
    "# print(arr_full[0])\n",
    "test_images = torch.from_numpy(arr_full)\n",
    "test_labels = torch.tensor([0,1,2,0,1,2,0,1,2,0]) #DIFFICULT\n",
    "# test_labels = torch.tensor([0,0,0,1,1,1,2,2,2,2])#EASY\n",
    "\n",
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 0\n",
      "Loss: 1.0976779460906982\n",
      "Epoch: 1\n",
      "Loss: 1.0966031551361084\n",
      "Epoch: 2\n",
      "Loss: 1.0947812795639038\n",
      "Epoch: 3\n",
      "Loss: 1.0924937725067139\n",
      "Epoch: 4\n",
      "Loss: 1.0901482105255127\n",
      "Epoch: 5\n",
      "Loss: 1.0880153179168701\n",
      "Epoch: 6\n",
      "Loss: 1.0863227844238281\n",
      "Epoch: 7\n",
      "Loss: 1.0850980281829834\n",
      "Epoch: 8\n",
      "Loss: 1.0843178033828735\n",
      "Epoch: 9\n",
      "Loss: 1.0838254690170288\n",
      "Epoch: 10\n",
      "Loss: 1.0834438800811768\n",
      "Epoch: 11\n",
      "Loss: 1.0829765796661377\n",
      "Epoch: 12\n",
      "Loss: 1.0823498964309692\n",
      "Epoch: 13\n",
      "Loss: 1.081400752067566\n",
      "Epoch: 14\n",
      "Loss: 1.0801050662994385\n",
      "Epoch: 15\n",
      "Loss: 1.078506588935852\n",
      "Epoch: 16\n",
      "Loss: 1.0766942501068115\n",
      "Epoch: 17\n",
      "Loss: 1.0747320652008057\n",
      "Epoch: 18\n",
      "Loss: 1.0727479457855225\n",
      "Epoch: 19\n",
      "Loss: 1.070787787437439\n",
      "Epoch: 20\n",
      "Loss: 1.068805456161499\n",
      "Epoch: 21\n",
      "Loss: 1.066819429397583\n",
      "Epoch: 22\n",
      "Loss: 1.0647982358932495\n",
      "Epoch: 23\n",
      "Loss: 1.0626342296600342\n",
      "Epoch: 24\n",
      "Loss: 1.0602073669433594\n",
      "Epoch: 25\n",
      "Loss: 1.0575836896896362\n",
      "Epoch: 26\n",
      "Loss: 1.0547447204589844\n",
      "Epoch: 27\n",
      "Loss: 1.051781415939331\n",
      "Epoch: 28\n",
      "Loss: 1.0486960411071777\n",
      "Epoch: 29\n",
      "Loss: 1.0455315113067627\n",
      "Epoch: 30\n",
      "Loss: 1.0423510074615479\n",
      "Epoch: 31\n",
      "Loss: 1.0393129587173462\n",
      "Epoch: 32\n",
      "Loss: 1.036377191543579\n",
      "Epoch: 33\n",
      "Loss: 1.0334699153900146\n",
      "Epoch: 34\n",
      "Loss: 1.0306425094604492\n",
      "Epoch: 35\n",
      "Loss: 1.0279295444488525\n",
      "Epoch: 36\n",
      "Loss: 1.025259256362915\n",
      "Epoch: 37\n",
      "Loss: 1.0225131511688232\n",
      "Epoch: 38\n",
      "Loss: 1.019914150238037\n",
      "Epoch: 39\n",
      "Loss: 1.016987919807434\n",
      "Epoch: 40\n",
      "Loss: 1.0121179819107056\n",
      "Epoch: 41\n",
      "Loss: 1.00614333152771\n",
      "Epoch: 42\n",
      "Loss: 1.0028712749481201\n",
      "Epoch: 43\n",
      "Loss: 1.000553846359253\n",
      "Epoch: 44\n",
      "Loss: 0.998180091381073\n",
      "Epoch: 45\n",
      "Loss: 0.9956920742988586\n",
      "Epoch: 46\n",
      "Loss: 0.9931303262710571\n",
      "Epoch: 47\n",
      "Loss: 0.9906976819038391\n",
      "Epoch: 48\n",
      "Loss: 0.9880636930465698\n",
      "Epoch: 49\n",
      "Loss: 0.9856315851211548\n",
      "Epoch: 50\n",
      "Loss: 0.9835175275802612\n",
      "Epoch: 51\n",
      "Loss: 0.9815633893013\n",
      "Epoch: 52\n",
      "Loss: 0.9796099662780762\n",
      "Epoch: 53\n",
      "Loss: 0.9777476191520691\n",
      "Epoch: 54\n",
      "Loss: 0.9757531881332397\n",
      "Epoch: 55\n",
      "Loss: 0.9737054705619812\n",
      "Epoch: 56\n",
      "Loss: 0.9715196490287781\n",
      "Epoch: 57\n",
      "Loss: 0.9692203402519226\n",
      "Epoch: 58\n",
      "Loss: 0.96673983335495\n",
      "Epoch: 59\n",
      "Loss: 0.9640359878540039\n",
      "Epoch: 60\n",
      "Loss: 0.9615377187728882\n",
      "Epoch: 61\n",
      "Loss: 0.9587208032608032\n",
      "Epoch: 62\n",
      "Loss: 0.9556663632392883\n",
      "Epoch: 63\n",
      "Loss: 0.9523434638977051\n",
      "Epoch: 64\n",
      "Loss: 0.9489986300468445\n",
      "Epoch: 65\n",
      "Loss: 0.9454733729362488\n",
      "Epoch: 66\n",
      "Loss: 0.9416121244430542\n",
      "Epoch: 67\n",
      "Loss: 0.9376459121704102\n",
      "Epoch: 68\n",
      "Loss: 0.9341737031936646\n",
      "Epoch: 69\n",
      "Loss: 0.9306026697158813\n",
      "Epoch: 70\n",
      "Loss: 0.9265300631523132\n",
      "Epoch: 71\n",
      "Loss: 0.9228178262710571\n",
      "Epoch: 72\n",
      "Loss: 0.91954505443573\n",
      "Epoch: 73\n",
      "Loss: 0.916325569152832\n",
      "Epoch: 74\n",
      "Loss: 0.9132693409919739\n",
      "Epoch: 75\n",
      "Loss: 0.9104134440422058\n",
      "Epoch: 76\n",
      "Loss: 0.9076951742172241\n",
      "Epoch: 77\n",
      "Loss: 0.9053362607955933\n",
      "Epoch: 78\n",
      "Loss: 0.9034789204597473\n",
      "Epoch: 79\n",
      "Loss: 0.9012419581413269\n",
      "Epoch: 80\n",
      "Loss: 0.8993903994560242\n",
      "Epoch: 81\n",
      "Loss: 0.8975318074226379\n",
      "Epoch: 82\n",
      "Loss: 0.895682156085968\n",
      "Epoch: 83\n",
      "Loss: 0.8939070701599121\n",
      "Epoch: 84\n",
      "Loss: 0.8920974731445312\n",
      "Epoch: 85\n",
      "Loss: 0.8902411460876465\n",
      "Epoch: 86\n",
      "Loss: 0.8883600234985352\n",
      "Epoch: 87\n",
      "Loss: 0.8865364193916321\n",
      "Epoch: 88\n",
      "Loss: 0.8847439885139465\n",
      "Epoch: 89\n",
      "Loss: 0.8829535245895386\n",
      "Epoch: 90\n",
      "Loss: 0.8812040090560913\n",
      "Epoch: 91\n",
      "Loss: 0.8794633746147156\n",
      "Epoch: 92\n",
      "Loss: 0.8777468800544739\n",
      "Epoch: 93\n",
      "Loss: 0.8760741949081421\n",
      "Epoch: 94\n",
      "Loss: 0.8743670582771301\n",
      "Epoch: 95\n",
      "Loss: 0.8727032542228699\n",
      "Epoch: 96\n",
      "Loss: 0.87139892578125\n",
      "Epoch: 97\n",
      "Loss: 0.869470477104187\n",
      "Epoch: 98\n",
      "Loss: 0.8678603172302246\n",
      "Epoch: 99\n",
      "Loss: 0.8662047386169434\n",
      "Epoch: 100\n",
      "Loss: 0.8645536303520203\n",
      "Epoch: 101\n",
      "Loss: 0.8628405332565308\n",
      "Epoch: 102\n",
      "Loss: 0.8611409068107605\n",
      "Epoch: 103\n",
      "Loss: 0.8594256639480591\n",
      "Epoch: 104\n",
      "Loss: 0.8577931523323059\n",
      "Epoch: 105\n",
      "Loss: 0.8560788035392761\n",
      "Epoch: 106\n",
      "Loss: 0.8544396162033081\n",
      "Epoch: 107\n",
      "Loss: 0.8526673316955566\n",
      "Epoch: 108\n",
      "Loss: 0.85101318359375\n",
      "Epoch: 109\n",
      "Loss: 0.849455714225769\n",
      "Epoch: 110\n",
      "Loss: 0.8477190136909485\n",
      "Epoch: 111\n",
      "Loss: 0.8459603190422058\n",
      "Epoch: 112\n",
      "Loss: 0.8443287014961243\n",
      "Epoch: 113\n",
      "Loss: 0.8422611355781555\n",
      "Epoch: 114\n",
      "Loss: 0.8407498598098755\n",
      "Epoch: 115\n",
      "Loss: 0.8390693664550781\n",
      "Epoch: 116\n",
      "Loss: 0.8370545506477356\n",
      "Epoch: 117\n",
      "Loss: 0.8351246118545532\n",
      "Epoch: 118\n",
      "Loss: 0.8330234289169312\n",
      "Epoch: 119\n",
      "Loss: 0.831041157245636\n",
      "Epoch: 120\n",
      "Loss: 0.8290982246398926\n",
      "Epoch: 121\n",
      "Loss: 0.8274483680725098\n",
      "Epoch: 122\n",
      "Loss: 0.8252655267715454\n",
      "Epoch: 123\n",
      "Loss: 0.8233019113540649\n",
      "Epoch: 124\n",
      "Loss: 0.8214696645736694\n",
      "Epoch: 125\n",
      "Loss: 0.8195648193359375\n",
      "Epoch: 126\n",
      "Loss: 0.8175832629203796\n",
      "Epoch: 127\n",
      "Loss: 0.815494179725647\n",
      "Epoch: 128\n",
      "Loss: 0.8134005665779114\n",
      "Epoch: 129\n",
      "Loss: 0.8111945390701294\n",
      "Epoch: 130\n",
      "Loss: 0.8090208768844604\n",
      "Epoch: 131\n",
      "Loss: 0.8067803382873535\n",
      "Epoch: 132\n",
      "Loss: 0.8044465184211731\n",
      "Epoch: 133\n",
      "Loss: 0.8019757270812988\n",
      "Epoch: 134\n",
      "Loss: 0.7995251417160034\n",
      "Epoch: 135\n",
      "Loss: 0.7967777848243713\n",
      "Epoch: 136\n",
      "Loss: 0.793695330619812\n",
      "Epoch: 137\n",
      "Loss: 0.7906025052070618\n",
      "Epoch: 138\n",
      "Loss: 0.7871451377868652\n",
      "Epoch: 139\n",
      "Loss: 0.7833633422851562\n",
      "Epoch: 140\n",
      "Loss: 0.7789681553840637\n",
      "Epoch: 141\n",
      "Loss: 0.773945689201355\n",
      "Epoch: 142\n",
      "Loss: 0.7681297063827515\n",
      "Epoch: 143\n",
      "Loss: 0.7610704898834229\n",
      "Epoch: 144\n",
      "Loss: 0.7524335384368896\n",
      "Epoch: 145\n",
      "Loss: 0.741913914680481\n",
      "Epoch: 146\n",
      "Loss: 0.729138970375061\n",
      "Epoch: 147\n",
      "Loss: 0.7133678197860718\n",
      "Epoch: 148\n",
      "Loss: 0.6945984959602356\n",
      "Epoch: 149\n",
      "Loss: 0.6722986102104187\n",
      "Epoch: 150\n",
      "Loss: 0.6435573101043701\n",
      "Epoch: 151\n",
      "Loss: 0.6078688502311707\n",
      "Epoch: 152\n",
      "Loss: 0.5675085186958313\n",
      "Epoch: 153\n",
      "Loss: 0.5248688459396362\n",
      "Epoch: 154\n",
      "Loss: 0.4786308705806732\n",
      "Epoch: 155\n",
      "Loss: 0.4304622709751129\n",
      "Epoch: 156\n",
      "Loss: 0.3803531527519226\n",
      "Epoch: 157\n",
      "Loss: 0.32858985662460327\n",
      "Epoch: 158\n",
      "Loss: 0.27687761187553406\n",
      "Epoch: 159\n",
      "Loss: 0.23061101138591766\n",
      "Epoch: 160\n",
      "Loss: 0.18710541725158691\n",
      "Epoch: 161\n",
      "Loss: 0.14867059886455536\n",
      "Epoch: 162\n",
      "Loss: 0.11667947471141815\n",
      "Epoch: 163\n",
      "Loss: 0.09013476222753525\n",
      "Epoch: 164\n",
      "Loss: 0.06905243545770645\n",
      "Epoch: 165\n",
      "Loss: 0.05228162929415703\n",
      "Epoch: 166\n",
      "Loss: 0.03913229703903198\n",
      "Epoch: 167\n",
      "Loss: 0.029357338324189186\n",
      "Epoch: 168\n",
      "Loss: 0.022371435537934303\n",
      "Epoch: 169\n",
      "Loss: 0.01731104776263237\n",
      "Epoch: 170\n",
      "Loss: 0.013428902253508568\n",
      "Epoch: 171\n",
      "Loss: 0.010442185215651989\n",
      "Epoch: 172\n",
      "Loss: 0.008107972331345081\n",
      "Epoch: 173\n",
      "Loss: 0.006316423416137695\n",
      "Epoch: 174\n",
      "Loss: 0.005010461900383234\n",
      "Epoch: 175\n",
      "Loss: 0.0040222881361842155\n",
      "Epoch: 176\n",
      "Loss: 0.003272533416748047\n",
      "Epoch: 177\n",
      "Loss: 0.0027113198302686214\n",
      "Epoch: 178\n",
      "Loss: 0.0022734166122972965\n",
      "Epoch: 179\n",
      "Loss: 0.0019258022075518966\n",
      "Epoch: 180\n",
      "Loss: 0.0016476630698889494\n",
      "Epoch: 181\n",
      "Loss: 0.0014234542613849044\n",
      "Epoch: 182\n",
      "Loss: 0.0012442588340491056\n",
      "Epoch: 183\n",
      "Loss: 0.0010971069568768144\n",
      "Epoch: 184\n",
      "Loss: 0.000976133334916085\n",
      "Epoch: 185\n",
      "Loss: 0.0008759498596191406\n",
      "Epoch: 186\n",
      "Loss: 0.0007927417755126953\n",
      "Epoch: 187\n",
      "Loss: 0.0007235527154989541\n",
      "Epoch: 188\n",
      "Loss: 0.0006650447612628341\n",
      "Epoch: 189\n",
      "Loss: 0.0006150722620077431\n",
      "Epoch: 190\n",
      "Loss: 0.0005725383525714278\n",
      "Epoch: 191\n",
      "Loss: 0.0005358218913897872\n",
      "Epoch: 192\n",
      "Loss: 0.0005038738017901778\n",
      "Epoch: 193\n",
      "Loss: 0.00047597885713912547\n",
      "Epoch: 194\n",
      "Loss: 0.00045142174349166453\n",
      "Epoch: 195\n",
      "Loss: 0.00043001174344681203\n",
      "Epoch: 196\n",
      "Loss: 0.0004108905850443989\n",
      "Epoch: 197\n",
      "Loss: 0.0003940582391805947\n",
      "Epoch: 198\n",
      "Loss: 0.00037894249544478953\n",
      "Epoch: 199\n",
      "Loss: 0.0003653049352578819\n",
      "Epoch: 200\n",
      "Loss: 0.00035309791564941406\n",
      "Epoch: 201\n",
      "Loss: 0.00034193991450592875\n",
      "Epoch: 202\n",
      "Loss: 0.0003321647527627647\n",
      "Epoch: 203\n",
      "Loss: 0.0003231048467569053\n",
      "Epoch: 204\n",
      "Loss: 0.0003150939883198589\n",
      "Epoch: 205\n",
      "Loss: 0.0003074645937886089\n",
      "Epoch: 206\n",
      "Loss: 0.0003005981561727822\n",
      "Epoch: 207\n",
      "Loss: 0.0002944469451904297\n",
      "Epoch: 208\n",
      "Loss: 0.00028867722721770406\n",
      "Epoch: 209\n",
      "Loss: 0.00028357506380416453\n",
      "Epoch: 210\n",
      "Loss: 0.00027861594571731985\n",
      "Epoch: 211\n",
      "Loss: 0.00027403832064010203\n",
      "Epoch: 212\n",
      "Loss: 0.0002699375036172569\n",
      "Epoch: 213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3d2598f112ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     print(\"Labels:\", len(labels),labels.size() , labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    #for i in range(TIMESTEP):\n",
    "    inputs = test_images\n",
    "    labels = test_labels\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    outputs = test_net(inputs)\n",
    "#     print(\"Out:\", len(outputs),outputs.size(),  outputs)\n",
    "#     print(\"Labels:\", len(labels),labels.size() , labels)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward() \n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    print(\"Loss:\", running_loss)\n",
    "    loss_array.append(running_loss)\n",
    "print('...Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_EPOCHS = 100\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbfb4bcac18>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
