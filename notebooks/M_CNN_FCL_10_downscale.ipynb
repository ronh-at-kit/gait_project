{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters defined\n",
      "Imports done\n",
      "loading configuration  flows_downscale\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib widget\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2 \n",
    "IMAGE_INPUT_SIZE_W = 640\n",
    "IMAGE_INPUT_SIZE_H = 480\n",
    "\n",
    "IMAGE_AFTER_CONV_SIZE_W = 18\n",
    "IMAGE_AFTER_CONV_SIZE_H = 13\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "# LSTM_IO_SIZE = 18*13\n",
    "# LSTM_HIDDEN_SIZE = 18*13\n",
    "CONV_OUT = 100\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 10 # size videos\n",
    "BATCH_SIZE = 4\n",
    "SLICE_FROM_TIMESTEP = 1 #slices from timestep SLICE_FROM_TIMESTEP to the last one\n",
    "\n",
    "NR_EPOCHS = 10\n",
    "\n",
    "VALIDATION_SPLIT = 0.0 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "learning_rate = 0.001 # reduce factos of 10 .. some epoch later.\n",
    "momentum = 0.9\n",
    "print(\"Hyperparameters defined\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "#import memory_profiler\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('Qt4Agg')\n",
    "#import matplotlib.gridspec as grd\n",
    "\n",
    "print(\"Imports done\")\n",
    "\n",
    "#change configuration in settings.py\n",
    "crop_im_size = [186,250]\n",
    "c = Config()\n",
    "# c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "# c.config['transformers']['DimensionResize']['dimension'] = TIMESTEPS\n",
    "# #c.config['indexing']['people selection'] = [1]\n",
    "# #c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "# c.config['pose']['load'] = False\n",
    "# c.config['flow']['load'] = False\n",
    "# c.config['heatmaps']['load'] = False\n",
    "# #c.config['scenes']['sequences'] = ['nm']\n",
    "# #c.config['scenes']['angles'] = ['108']\n",
    "# c.config['dataset_output'] = {\n",
    "# #         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "#         'data': ['scenes'],\n",
    "#         'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network class defined\n",
      "cpu\n",
      "Neural network defined\n",
      "Input: 94x94, 3 conv layer\n"
     ]
    }
   ],
   "source": [
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.avialable_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #input 382x382\n",
    "        self.conv1 = nn.Conv2d(30,60,3) #output 380x380\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #output 190x190\n",
    "        self.conv2 = nn.Conv2d(60,30,3) # output 188x188\n",
    "        self.pool2 = nn.MaxPool2d(2,2) # output 94x94\n",
    "        self.conv3 = nn.Conv2d(30,1,3) # output 92x92\n",
    "        self.pool3 = nn.MaxPool2d(2,2) # output 46x46\n",
    "#         self.conv4 = nn.Conv2d(30,1,3)  # output 44x44\n",
    "#         self.pool4 = nn.MaxPool2d(2,2) #  output 22x22\n",
    "#         self.conv5 = nn.Conv2d(30,1,3)  # output 20x20\n",
    "#         self.pool5 = nn.MaxPool2d(2,2) # output 10x10\n",
    "        \n",
    "        self.fc1 = nn.Linear(CONV_OUT,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         print(\"Input list len:\",len(x))\n",
    "#         print(\"Input elemens size:\", x.size())\n",
    "        x = x.float()\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "#         x = self.pool5(F.relu(self.conv5(x)))\n",
    "#         print(\"Inputs after conv:\", x.size())\n",
    "        x = x.view(BATCH_SIZE,CONV_OUT) #output.view(seq_len, batch, num_dir*hidden_size)\n",
    "#         x = torch.squeeze(x)\n",
    "#         print(\"Input before FC:\", x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "#         print(\"Input after FC:\", x.size())\n",
    "        #x = x.squeeze(1)\n",
    "#         x = x.permute(1,2,0)\n",
    "        #print (\"Size network output\", x.shape)\n",
    "        return x\n",
    "print(\"Network class defined\")\n",
    "\n",
    "test_net = TEST_CNN_LSTM()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "print(\"Neural network defined\")\n",
    "print(\"Input: 94x94, 3 conv layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 24\n",
      "Indices size: 24\n",
      "Split: 0\n",
      "Training data loaded\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "print(\"Training data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1: Mean 0.0011104866 Variance 0.0033620333\n",
      "Saved initial weights and biases from net\n"
     ]
    }
   ],
   "source": [
    "parameters_conv1_init = list(test_net.conv1.parameters())\n",
    "bias_conv1_init = parameters_conv1_init[1].detach().numpy()\n",
    "weights_conv1_init = parameters_conv1_init[0].detach().numpy().flatten()\n",
    "\n",
    "parameters_fc1_init = list(test_net.fc1.parameters())\n",
    "bias_fc1_init = parameters_fc1_init[1].detach().numpy()\n",
    "weights_fc1_init = parameters_fc1_init[0].detach().numpy().flatten()\n",
    "\n",
    "print(\"FC1: Mean\",np.mean(weights_fc1_init),\"Variance\",np.var(weights_fc1_init))\n",
    "print(\"Saved initial weights and biases from net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot function defined\n"
     ]
    }
   ],
   "source": [
    "def plot_input(data_in):\n",
    "    if BATCH_SIZE == 1:\n",
    "        print(\"First input:\")\n",
    "        fig,axs  = plt.subplots(nrows=1, ncols=10,figsize=(len(data_in),1))\n",
    "        for item,ax in zip(data_in,axs):\n",
    "            #item.imshow(data_in.permute(1,2,0))\n",
    "            #print(item.squeeze(0))\n",
    "            ax.imshow(item.squeeze(0).permute(2,1,0))\n",
    "        #plt.show()\n",
    "    else: \n",
    "        print(\"Batch size not suitable for plot\")\n",
    "print(\"Plot function defined\")\n",
    "# plot_input(images_in_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_labels(dataset):\n",
    "    print(\"Number elements:\", len(dataset))\n",
    "    print(\"Batch size:\",BATCH_SIZE)\n",
    "    print(\"Containing labels:\")\n",
    "    for dataset_item in dataset:\n",
    "        _, labels = dataset_item\n",
    "        print(labels)\n",
    "#check_input_labels(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set and evaluate computing time etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Expected loss with 3 different classes and 24 data elements: 26.366694928034633\n",
      "Batch size: 4\n",
      "Evaluating first element...\n",
      "Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\n",
      "Output format: torch.Size([4, 3])\n",
      "Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\n",
      "Labels: 4 torch.Size([4])\n",
      "Labels content: tensor([1, 1, 1, 1])\n",
      "Output and Label sizes: torch.Size([4, 3]) torch.Size([4])\n",
      "Output and Label: tensor([[ 0.4782, -1.4257, -1.5516],\n",
      "        [ 0.5177, -1.4276, -1.6040],\n",
      "        [ 0.5544, -1.3932, -1.6783],\n",
      "        [ 0.5732, -1.4071, -1.7052]], grad_fn=<AddmmBackward>) tensor([1, 1, 1, 1])\n",
      "Loss:2.1740198135375977, expected loss:1.0986122886681098\n",
      "Time needed:17.94371271133423s\n",
      "Expected loss for total training data:  26.366694928034633\n",
      "Expected training time per epoch:1.7943712711334228 min\n",
      "Estimated total training time:0.29906187852223715 hours\n"
     ]
    }
   ],
   "source": [
    "#PREPARATION FOR TRAINING\n",
    "loss_array = []\n",
    "learning_rate_array = []\n",
    "\n",
    "print('Start training...')\n",
    "print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "running_loss = 0.0\n",
    "#print(\"Data set length:\", len((train_loader)), \"Validation length:\", len(test_loader))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Evaluating first element...\")\n",
    "start_time = time.time()\n",
    "i, batch = next(iter(enumerate(train_loader)))\n",
    "inputs, labels = batch\n",
    "images_in_first = inputs\n",
    "# data_in = [s.to(device) for s in inputs['flows']]\n",
    "data_in = inputs['flows'].to(device)\n",
    "labels = labels.to(device)\n",
    "# print(\"Time steps:{}, input sequence length:{}\".format(TIMESTEPS,len(data_in)))\n",
    "# print(\"NN input: \",len(data_in),len(data_in[0]),len(data_in[0][0]),len(data_in[0][0][0]),len(data_in[0][0][0][0]))\n",
    "optimizer.zero_grad() \n",
    "#\n",
    "\n",
    "\n",
    "# plot_input(images_in_first)\n",
    "# print(\"Data in [-1]:\")\n",
    "# print(data_in[-1].shape)\n",
    "# print(\"Data in pure:\")\n",
    "# print(\"List with elementes \" ,len(data_in), \"Obj\", data_in[0].size())\n",
    "# print(\"Input length:\", (data_in.size()))\n",
    "outputs = test_net(data_in)\n",
    "print(\"Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\")\n",
    "print(\"Output format:\", outputs.size())\n",
    "print(\"Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\")\n",
    "print(\"Labels:\", len(labels), labels.size())\n",
    "print(\"Labels content:\", labels)\n",
    "print(\"Output and Label sizes:\",outputs.shape, labels.shape)\n",
    "print(\"Output and Label:\", outputs, labels)\n",
    "loss = criterion(outputs.float(),labels.long())\n",
    "loss.backward() \n",
    "optimizer.step()\n",
    "running_loss += loss.data.item()\n",
    "elapsed_time = time.time() - start_time;\n",
    "#loss_array.append(running_loss)\n",
    "#learning_rate_array.append(learning_rate)\n",
    "print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "print(\"Time needed:{}s\".format(elapsed_time))\n",
    "print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)/60))\n",
    "print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*NR_EPOCHS/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Accuracy 0.00%\n",
      "Accuracy 0.00%\n",
      "Accuracy 0.00%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NR_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in = inputs['flows'].to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print(\"Labels\",labels.size())\n",
    "            \n",
    "        outputs = test_net(data_in)\n",
    "        loss = criterion(outputs.float(),labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "        \n",
    "#         total = 0;\n",
    "#         correct = 0;\n",
    "#         for item in labels:\n",
    "#             labels = torch.tensor([[item]])\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "#         total += predicted.numel()\n",
    "#         correct += predicted.numel() - n_errors\n",
    "#         print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "\n",
    "    print(\"Loss epoch {}: {}, took {}s\".format(epoch, running_loss,time.time()-start_time))\n",
    "    loss_array.append(running_loss)\n",
    "    learning_rate_array.append(learning_rate)\n",
    "print('...Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DYNAMIC CHANGES:\n",
    "NR_EPOCHS = 20\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "print(\"Expected loss:{}, last loss:{}\".format((len(dataset)-split)*np.log(3),loss_array[-1]))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Sequence length:\",TIMESTEPS)\n",
    "print(\"Total epochs learnt:\", len(loss_array))\n",
    "#plt.plot(learning_rate_array)\n",
    "#plt.plot(loss_array)\n",
    "### define and execute testing function\n",
    "def test_all_preds(model):\n",
    "    n_batches_test = len(test_loader)\n",
    "\n",
    "    #Time for printing\n",
    "    testing_start_time = time.time()\n",
    "\n",
    "    print('Start testing...')\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            data_in = inputs['flows'].to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "#             if not labels.size()[0] == BATCH_SIZE:\n",
    "#                 # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "#                 continue\n",
    "            outputs = model(data_in)\n",
    "            for item in labels:\n",
    "                labels = torch.tensor([[item]])\n",
    "#             print(\"Out:\", len(outputs), outputs.size())\n",
    "#             print(\"Labels:\", len(labels), labels.size())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "            n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "            total += predicted.numel()\n",
    "            # print('predicted',predicted)\n",
    "            correct += predicted.numel() - n_errors\n",
    "            # print('labels',labels)\n",
    "    print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "    print('...testing finished')\n",
    "print(\"Definition Test predictions done\")\n",
    "test_all_preds(test_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_fc1 = list(test_net.fc1.parameters())\n",
    "bias_fc1 = parameters_fc1[1].detach().numpy()\n",
    "weights_fc1 = parameters_fc1[0].detach().numpy().flatten()\n",
    "\n",
    "parameters_conv1 = list(test_net.conv1.parameters())\n",
    "bias_conv1 = parameters_conv1[1].detach().numpy()\n",
    "weights_conv1 = parameters_conv1[0].detach().numpy().flatten()\n",
    "\n",
    "weights_fc1_m = np.mean(weights_fc1-weights_fc1_init)\n",
    "weights_fc1_v = np.var(weights_fc1-weights_fc1_init)\n",
    "\n",
    "print(\"FC1: Mean\",np.mean(weights_fc1_init),\"Variance\",np.var(weights_fc1_init))\n",
    "print(\"FC1: Mean\",np.mean(weights_fc1),\"Var\",np.var(weights_fc1))\n",
    "print(\"FC1 Diff: Var\",np.var(weights_fc1_init-weights_fc1))\n",
    "#plt.figure()\n",
    "#plt.plot(range(len(weights_conv1)),weights_conv1_init,range(len(weights_conv1)),weights_conv1)\n",
    "#plt.figure()\n",
    "#plt.plot(range(len(bias_conv1)),bias_conv1_init,range(len(bias_conv1)),bias_conv1)\n",
    "\n",
    "##FC layers\n",
    "#plt.figure()\n",
    "#plt.plot(range(len(weights_conv1)),weights_conv1_init,range(len(weights_conv1)),weights_conv1)\n",
    "#plt.figure()\n",
    "#plt.plot(range(len(weights_fc1)),weights_fc1_init,range(len(weights_fc1)),weights_fc1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Difference in FC-Layers\")\n",
    "plt.plot(range(len(weights_fc1)),weights_fc1_init-weights_fc1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Difference in Conv-Layers\")\n",
    "plt.plot(range(len(weights_conv1)),weights_conv1_init-weights_conv1)#,range(len(weights_conv5)),weights_conv5)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(weights_lstm2_init-weights_lstm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Difference in FC-Layers\")\n",
    "plt.plot(range(len(weights_fc1)),weights_fc1_init-weights_fc1)\n",
    "\n",
    "plt.draw()\n",
    "#plt.show()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected loss for untrained set with Cross Entropy:\n",
    "k = number of classes\n",
    "N = number of labeled data in dataset\n",
    "loss_per_prediction = -log(1/k) = log(k)\n",
    "total_loss = sum(log(k)) = N*log(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plot_path = \"/home/matthias/Desktop/\"\n",
    "loss_str = \"{0:.3f}\".format(loss_array[-1])\n",
    "information = \"flows_L\" + loss_str + \"_timesteps{}_nrsequences{}_batch{}\".format(TIMESTEPS,dataset_size,BATCH_SIZE)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Cross Entropy)')\n",
    "plt.plot(loss_array)\n",
    "plt.title(information)\n",
    "plt.savefig(plot_path + information + \".eps\")\n",
    "plt.clf()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.plot(learning_rate_array)\n",
    "plt.title(information)\n",
    "plt.savefig(plot_path + information + \"LR.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(test_net.state_dict(), '/mnt/DATA/HIWI/IBT/saved_models/cnn_fixed_timesteps/cnn_10_steps_fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
