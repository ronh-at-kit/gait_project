{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. cnn_flows_prtrain\n",
      "Hyperparameters defined\n",
      "Bug with learning rate when sequence length is not the same for each element of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. cnn_flows_prtrain\")\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "\n",
    "VALIDATION_SPLIT = 0.5 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "TRAINING_PREPARATION = False\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "TIME_STEPS = 40\n",
    "\n",
    "SLICE_FROM_TIME_STEP = 0 #slices from timestep SLICE_FROM_TIMESTEP to the last one\n",
    "\n",
    "NR_EPOCHS = 100\n",
    "\n",
    "LR = 0.0001\n",
    "print(\"Hyperparameters defined\")\n",
    "print(\"Bug with learning rate when sequence length is not the same for each element of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gait_analysis.Models.ConvLSTMFlow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0545f2b77f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgait_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvLSTMFlow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvLSTMFlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gait_analysis.Models.ConvLSTMFlow'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "from gait_analysis import WeightWatcher\n",
    "from gait_analysis import AccuracyTrackerTrainTest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gait_analysis.Models.ConvLSTMFlow import ConvLSTMFlow\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: doesn't x = x.view(BATCH_SIZE,TIME_STEPS*LSTM_HIDDEN_FEATURES) mix up batch size order?\n",
      "TODO: BATCH_SIZE is currently fixed and one\n",
      "TODO: Is x_arr in device?\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "net = ConvLSTMFlow()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "# monitor = WeightWatcher(net,['conv5','fc1'])\n",
    "tt_acc_tracker = AccuracyTrackerTrainTest([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  cnn_flows_crop\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "# c.config['transformers']['DimensionResize']['dimension'] = TIME_STEPS\n",
    "# #c.config['indexing']['people selection'] = [1]\n",
    "# #c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "# c.config['pose']['load'] = False\n",
    "# c.config['flow']['load'] = True\n",
    "# c.config['heatmaps']['load'] = False\n",
    "# #c.config['scenes']['sequences'] = ['nm']\n",
    "# #c.config['scenes']['angles'] = ['108']\n",
    "# c.config['dataset_output'] = {\n",
    "# #         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "#         'data': ['flows'],\n",
    "#         'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 595\n",
      "Indices size: 595\n",
      "Split: 119\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PREPARATION FOR TRAINING\n",
    "# loss_array = []\n",
    "# learning_rate_array = []\n",
    "\n",
    "# print('Start training...')\n",
    "# print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "# running_loss = 0.0\n",
    "# #print(\"Data set length:\", len((train_loader)), \"Validation length:\", len(test_loader))\n",
    "# print(\"Batch size:\", BATCH_SIZE)\n",
    "# print(\"Evaluating first element...\")\n",
    "# start_time = time.time()\n",
    "# i, batch = next(iter(enumerate(train_loader)))\n",
    "# inputs, labels = batch\n",
    "# data_in = [s.to(device) for s in inputs['flows']]\n",
    "# # print(\"Data in original\", data_in)\n",
    "\n",
    "\n",
    "# # print(\"Proof for normalized data:\", data_in[0][0])\n",
    "# labels = labels.to(device)\n",
    "# print(\"Time steps:{}, input sequence length:{}\".format(TIME_STEPS,len(data_in)))\n",
    "# #print(\"NN input: \",len(flows),len(flows[0]),len(flows[0][0]),len(flows[0][0][0]),len(flows[0][0][0][0]))\n",
    "# optimizer.zero_grad() \n",
    "# outputs = net(data_in)\n",
    "# print(\"Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\")\n",
    "# # print(\"Output format:\", len(outputs), outputs.size())\n",
    "# print(\"Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\")\n",
    "# # print(\"Labels:\", len(labels), labels.size())\n",
    "# # print(\"Slicing loss. Using loss from:\",SLICE_FROM_TIME_STEP,\"to\",TIME_STEPS)\n",
    "# print(\"Final label: \",labels[:,SLICE_FROM_TIME_STEP:TIME_STEPS].size())\n",
    "# print(\"Final output:\", outputs[:,:,SLICE_FROM_TIME_STEP:TIME_STEPS].size())\n",
    "# # print(\"Labels content:\", labels)\n",
    "# labels = labels.squeeze(0)\n",
    "# outputs = outputs.squeeze(0)\n",
    "# print(\"Dimensions:\",\"Labels\",labels.size(),\"Pred\",outputs.size())\n",
    "# # before: loss = criterion(outputs[:,:,SLICE_FROM_TIME_STEP:TIME_STEPS].float(),labels[:,SLICE_FROM_TIME_STEP:TIME_STEPS].long())\n",
    "# loss = criterion(outputs[SLICE_FROM_TIME_STEP:TIME_STEPS,:].float(),labels[SLICE_FROM_TIME_STEP:TIME_STEPS].long())\n",
    "# loss.backward() \n",
    "# optimizer.step()\n",
    "\n",
    "# running_loss += loss.data.item()\n",
    "# elapsed_time = time.time() - start_time;\n",
    "# loss_array.append(running_loss)\n",
    "# learning_rate_array.append(LR)\n",
    "# print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "# print(\"Time needed:{}s\".format(elapsed_time))\n",
    "# print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "# print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)/60))\n",
    "# print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*NR_EPOCHS/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "-----------------------------\n",
      "Epoch 0: Loss [0.05680645208027092,0.06935679309164025], Acc [0.982,0.972] took 121.17s\n",
      "Accuracy by class [[0.986, 0.983, 0.976],[0.979, 0.987, 0.951]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 1: Loss [0.05508002482683343,0.06919843264847618], Acc [0.983,0.972] took 121.42s\n",
      "Accuracy by class [[0.987, 0.983, 0.979],[0.976, 0.991, 0.952]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 2: Loss [0.09811886025890078,0.1008537443307768], Acc [0.968,0.957] took 122.01s\n",
      "Accuracy by class [[0.972, 0.97, 0.963],[0.982, 0.963, 0.931]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 3: Loss [0.15207881521934463,0.13855161592990412], Acc [0.947,0.947] took 120.78s\n",
      "Accuracy by class [[0.955, 0.951, 0.937],[0.96, 0.962, 0.92]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 4: Loss [0.08899418673548512,0.10378783126557325], Acc [0.965,0.958] took 121.46s\n",
      "Accuracy by class [[0.974, 0.97, 0.954],[0.971, 0.976, 0.93]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 5: Loss [0.07167191485709996,0.09283904441702766], Acc [0.972,0.962] took 121.56s\n",
      "Accuracy by class [[0.978, 0.976, 0.964],[0.973, 0.98, 0.935]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 6: Loss [0.06444604979718437,0.08534007848968272], Acc [0.977,0.965] took 121.09s\n",
      "Accuracy by class [[0.983, 0.98, 0.971],[0.975, 0.982, 0.94]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 7: Loss [0.05944879088128186,0.07856592691714774], Acc [0.98,0.969] took 121.16s\n",
      "Accuracy by class [[0.985, 0.982, 0.974],[0.98, 0.982, 0.947]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 8: Loss [0.05571214018092187,0.07038273058142978], Acc [0.982,0.973] took 121.73s\n",
      "Accuracy by class [[0.986, 0.983, 0.977],[0.981, 0.986, 0.956]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 9: Loss [0.05267127789752637,0.06508966927424449], Acc [0.983,0.976] took 120.78s\n",
      "Accuracy by class [[0.988, 0.985, 0.979],[0.983, 0.988, 0.958]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 10: Loss [0.049702379739052824,0.06020906074520417], Acc [0.985,0.978] took 120.47s\n",
      "Accuracy by class [[0.989, 0.985, 0.981],[0.985, 0.987, 0.963]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 11: Loss [0.047209428431575794,0.05533927560885665], Acc [0.986,0.98] took 121.48s\n",
      "Accuracy by class [[0.99, 0.986, 0.983],[0.987, 0.989, 0.966]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 12: Loss [0.05513291645446837,0.0642577619096186], Acc [0.983,0.976] took 121.49s\n",
      "Accuracy by class [[0.986, 0.983, 0.979],[0.987, 0.985, 0.958]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 13: Loss [0.05170576857352051,0.05836711521064782], Acc [0.982,0.977] took 121.51s\n",
      "Accuracy by class [[0.987, 0.982, 0.979],[0.985, 0.988, 0.96]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 14: Loss [0.0447401781380568,0.05570649083296792], Acc [0.986,0.98] took 121.19s\n",
      "Accuracy by class [[0.99, 0.987, 0.983],[0.989, 0.99, 0.964]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 15: Loss [0.04194579356420444,0.053766183844697085], Acc [0.988,0.982] took 122.15s\n",
      "Accuracy by class [[0.991, 0.987, 0.986],[0.99, 0.991, 0.967]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 16: Loss [0.04359214147552848,0.05338001108783133], Acc [0.987,0.98] took 121.66s\n",
      "Accuracy by class [[0.99, 0.987, 0.983],[0.99, 0.988, 0.964]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 17: Loss [0.04073032846443451,0.05353315958377568], Acc [0.988,0.98] took 121.09s\n",
      "Accuracy by class [[0.992, 0.988, 0.986],[0.986, 0.991, 0.965]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 18: Loss [0.0371814772903434,0.05351332726129211], Acc [0.989,0.982] took 121.33s\n",
      "Accuracy by class [[0.992, 0.989, 0.988],[0.989, 0.992, 0.966]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 19: Loss [0.035716379456566416,0.05427838197885817], Acc [0.99,0.979] took 121.66s\n",
      "Accuracy by class [[0.993, 0.989, 0.989],[0.99, 0.989, 0.96]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 20: Loss [0.03548386070619049,0.05448921448981563], Acc [0.99,0.981] took 121.45s\n",
      "Accuracy by class [[0.993, 0.99, 0.988],[0.988, 0.992, 0.964]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 21: Loss [0.03664485490127635,0.05507843653444484], Acc [0.989,0.98] took 121.94s\n",
      "Accuracy by class [[0.991, 0.99, 0.987],[0.99, 0.989, 0.964]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 22: Loss [0.04892715518930224,0.07534280669006731], Acc [0.984,0.972] took 121.53s\n",
      "Accuracy by class [[0.987, 0.984, 0.98],[0.986, 0.983, 0.95]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 23: Loss [0.04188844786581247,0.055009982415608], Acc [0.987,0.979] took 120.71s\n",
      "Accuracy by class [[0.99, 0.989, 0.983],[0.986, 0.988, 0.963]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 24: Loss [0.035059809750298196,0.05129671160976926], Acc [0.99,0.98] took 120.98s\n",
      "Accuracy by class [[0.992, 0.99, 0.988],[0.986, 0.993, 0.963]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 25: Loss [0.03221732877515757,0.05717239692071529], Acc [0.991,0.978] took 121.87s\n",
      "Accuracy by class [[0.993, 0.992, 0.988],[0.987, 0.987, 0.962]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 26: Loss [0.03130103269114526,0.057023819046830675], Acc [0.991,0.978] took 122.27s\n",
      "Accuracy by class [[0.993, 0.992, 0.989],[0.985, 0.989, 0.962]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 27: Loss [0.029475475130250516,0.06041298396181033], Acc [0.993,0.979] took 121.97s\n",
      "Accuracy by class [[0.993, 0.992, 0.992],[0.987, 0.987, 0.963]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 28: Loss [0.026439732403842703,0.05586882998399873], Acc [0.994,0.979] took 121.9s\n",
      "Accuracy by class [[0.994, 0.994, 0.992],[0.986, 0.991, 0.961]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 29: Loss [0.025895391093437883,0.05901577696204184], Acc [0.994,0.978] took 121.7s\n",
      "Accuracy by class [[0.994, 0.994, 0.994],[0.986, 0.99, 0.961]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 30: Loss [0.02377142267915058,0.051226700429155045], Acc [0.995,0.981] took 121.78s\n",
      "Accuracy by class [[0.996, 0.996, 0.995],[0.988, 0.994, 0.962]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 31: Loss [0.06177436943217808,0.12724042007652656], Acc [0.982,0.96] took 121.42s\n",
      "Accuracy by class [[0.984, 0.984, 0.979],[0.989, 0.957, 0.937]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 32: Loss [0.08181060490955636,0.0979172884087477], Acc [0.972,0.963] took 121.41s\n",
      "Accuracy by class [[0.975, 0.978, 0.965],[0.973, 0.979, 0.941]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 33: Loss [0.04437615896649505,0.0722446373249052], Acc [0.984,0.974] took 121.85s\n",
      "Accuracy by class [[0.987, 0.988, 0.979],[0.978, 0.987, 0.957]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 34: Loss [0.033204238141295986,0.06958524292499264], Acc [0.991,0.975] took 121.62s\n",
      "Accuracy by class [[0.991, 0.992, 0.988],[0.977, 0.988, 0.961]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 35: Loss [0.02866980420697496,0.06282278376498393], Acc [0.992,0.978] took 121.5s\n",
      "Accuracy by class [[0.993, 0.994, 0.991],[0.982, 0.988, 0.965]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 36: Loss [0.025045508322180752,0.05596590037222858], Acc [0.995,0.981] took 120.8s\n",
      "Accuracy by class [[0.995, 0.997, 0.994],[0.984, 0.991, 0.971]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 37: Loss [0.022571501906857482,0.0520850164236642], Acc [0.996,0.983] took 121.27s\n",
      "Accuracy by class [[0.996, 0.997, 0.995],[0.984, 0.991, 0.974]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 38: Loss [0.020393071656175033,0.04771459784883097], Acc [0.997,0.984] took 121.48s\n",
      "Accuracy by class [[0.997, 0.998, 0.996],[0.985, 0.992, 0.976]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 39: Loss [0.018820988454933234,0.04643658071388529], Acc [0.997,0.985] took 121.28s\n",
      "Accuracy by class [[0.998, 0.998, 0.996],[0.986, 0.993, 0.978]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 40: Loss [0.02187234214570629,0.06700728991178469], Acc [0.995,0.979] took 121.97s\n",
      "Accuracy by class [[0.996, 0.995, 0.993],[0.975, 0.987, 0.974]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 41: Loss [0.07236673231916546,0.10886143744696211], Acc [0.982,0.959] took 121.44s\n",
      "Accuracy by class [[0.982, 0.984, 0.98],[0.966, 0.978, 0.936]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 42: Loss [0.05689117569327676,0.058873508969799246], Acc [0.98,0.979] took 120.78s\n",
      "Accuracy by class [[0.985, 0.982, 0.974],[0.984, 0.987, 0.968]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 43: Loss [0.03115352047186639,0.05634900684538883], Acc [0.991,0.98] took 121.09s\n",
      "Accuracy by class [[0.993, 0.993, 0.988],[0.983, 0.989, 0.969]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 44: Loss [0.024225236011078143,0.05370146457982415], Acc [0.994,0.981] took 121.31s\n",
      "Accuracy by class [[0.996, 0.995, 0.992],[0.984, 0.989, 0.97]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 45: Loss [0.021226402523999776,0.04944603134752759], Acc [0.996,0.982] took 120.87s\n",
      "Accuracy by class [[0.997, 0.996, 0.994],[0.985, 0.991, 0.97]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 46: Loss [0.018684945247422688,0.04510161965525858], Acc [0.997,0.986] took 121.34s\n",
      "Accuracy by class [[0.998, 0.997, 0.996],[0.985, 0.993, 0.98]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 47: Loss [0.0170944095356879,0.04304084803384705], Acc [0.998,0.985] took 121.45s\n",
      "Accuracy by class [[0.998, 0.997, 0.997],[0.985, 0.994, 0.978]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 48: Loss [0.0158960576858391,0.034266825406342544], Acc [0.998,0.99] took 121.38s\n",
      "Accuracy by class [[0.999, 0.998, 0.998],[0.991, 0.994, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 49: Loss [0.014988576012620495,0.03858847356550081], Acc [0.998,0.989] took 121.88s\n",
      "Accuracy by class [[0.999, 0.997, 0.998],[0.986, 0.997, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 50: Loss [0.014500613697129986,0.03994993904816817], Acc [0.998,0.989] took 121.47s\n",
      "Accuracy by class [[0.999, 0.998, 0.997],[0.986, 0.995, 0.986]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 51: Loss [0.018905603452199233,0.05207974041671251], Acc [0.996,0.983] took 121.51s\n",
      "Accuracy by class [[0.996, 0.997, 0.994],[0.984, 0.996, 0.972]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 52: Loss [0.020579958558407864,0.049260858131353495], Acc [0.995,0.981] took 121.83s\n",
      "Accuracy by class [[0.995, 0.996, 0.994],[0.976, 0.987, 0.98]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 53: Loss [0.16776744108828093,0.10321737665823907], Acc [0.959,0.968] took 120.99s\n",
      "Accuracy by class [[0.96, 0.959, 0.957],[0.975, 0.981, 0.951]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 54: Loss [0.11689332737406409,0.14447672571204287], Acc [0.965,0.953] took 121.0s\n",
      "Accuracy by class [[0.968, 0.973, 0.954],[0.974, 0.958, 0.93]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 55: Loss [0.07694866373707503,0.06333246876235209], Acc [0.972,0.975] took 121.73s\n",
      "Accuracy by class [[0.977, 0.977, 0.962],[0.98, 0.981, 0.965]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 56: Loss [0.044124684154208615,0.05376320946452935], Acc [0.984,0.98] took 121.4s\n",
      "Accuracy by class [[0.986, 0.987, 0.978],[0.984, 0.981, 0.975]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 57: Loss [0.03222728064973413,0.05124779961447567], Acc [0.989,0.981] took 121.68s\n",
      "Accuracy by class [[0.99, 0.991, 0.986],[0.983, 0.984, 0.976]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 58: Loss [0.026426914697948303,0.04647580754724059], Acc [0.992,0.983] took 122.24s\n",
      "Accuracy by class [[0.994, 0.994, 0.99],[0.984, 0.988, 0.977]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 59: Loss [0.021755155621408565,0.04210280517053581], Acc [0.994,0.986] took 121.69s\n",
      "Accuracy by class [[0.996, 0.995, 0.993],[0.986, 0.99, 0.981]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 60: Loss [0.01831250931987413,0.03985499988893096], Acc [0.997,0.986] took 121.12s\n",
      "Accuracy by class [[0.997, 0.997, 0.995],[0.985, 0.991, 0.982]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 61: Loss [0.01588900951045457,0.03849416412021808], Acc [0.998,0.987] took 121.73s\n",
      "Accuracy by class [[0.998, 0.998, 0.997],[0.986, 0.992, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 62: Loss [0.013968686681735318,0.03586845503759985], Acc [0.998,0.989] took 121.36s\n",
      "Accuracy by class [[0.999, 0.998, 0.998],[0.986, 0.994, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 63: Loss [0.012906692460856291,0.03659395679651502], Acc [0.999,0.989] took 121.71s\n",
      "Accuracy by class [[0.999, 0.999, 0.998],[0.986, 0.994, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 64: Loss [0.013517720316401737,0.02846143104208383], Acc [0.998,0.991] took 121.47s\n",
      "Accuracy by class [[0.999, 0.998, 0.997],[0.987, 0.994, 0.991]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 65: Loss [0.02173449566993547,0.09252609715096556], Acc [0.994,0.966] took 121.02s\n",
      "Accuracy by class [[0.996, 0.996, 0.992],[0.977, 0.975, 0.947]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 66: Loss [0.027588232691414277,0.05965809536483452], Acc [0.991,0.98] took 121.82s\n",
      "Accuracy by class [[0.993, 0.992, 0.988],[0.985, 0.982, 0.974]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 67: Loss [0.03190308213403925,0.046308091939466084], Acc [0.991,0.986] took 121.44s\n",
      "Accuracy by class [[0.992, 0.992, 0.989],[0.984, 0.996, 0.978]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 68: Loss [0.024084165290684304,0.04308834722844744], Acc [0.993,0.987] took 121.84s\n",
      "Accuracy by class [[0.994, 0.994, 0.991],[0.985, 0.995, 0.98]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 69: Loss [0.01586422883518316,0.03214690692139742], Acc [0.997,0.991] took 121.66s\n",
      "Accuracy by class [[0.997, 0.997, 0.996],[0.988, 0.996, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 70: Loss [0.012680169466959738,0.03482388377389205], Acc [0.998,0.99] took 121.36s\n",
      "Accuracy by class [[0.998, 0.998, 0.997],[0.988, 0.995, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 71: Loss [0.010740605679728534,0.036086260791116646], Acc [0.999,0.988] took 121.14s\n",
      "Accuracy by class [[1.0, 0.999, 0.999],[0.987, 0.994, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 72: Loss [0.009354949068484006,0.03257755296717655], Acc [0.999,0.991] took 121.6s\n",
      "Accuracy by class [[1.0, 0.999, 0.999],[0.989, 0.996, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 73: Loss [0.014412033245001203,0.03815188712123469], Acc [0.997,0.987] took 120.93s\n",
      "Accuracy by class [[0.997, 0.997, 0.997],[0.99, 0.994, 0.978]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 74: Loss [0.02272648826872972,0.02546508228523704], Acc [0.993,0.994] took 121.49s\n",
      "Accuracy by class [[0.994, 0.994, 0.991],[0.994, 0.996, 0.992]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 75: Loss [0.013497688381383882,0.03964440184136648], Acc [0.996,0.987] took 121.66s\n",
      "Accuracy by class [[0.998, 0.996, 0.995],[0.986, 0.991, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 76: Loss [0.009139053080440798,0.03671501386086966], Acc [0.999,0.988] took 121.58s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.986, 0.994, 0.983]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 77: Loss [0.007839853098003854,0.033919984106944884], Acc [0.999,0.991] took 120.92s\n",
      "Accuracy by class [[1.0, 0.999, 0.999],[0.987, 0.996, 0.989]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 78: Loss [0.007008220599032453,0.02719464628299808], Acc [1.0,0.991] took 121.33s\n",
      "Accuracy by class [[1.0, 0.999, 0.999],[0.989, 0.997, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 79: Loss [0.04014347337160415,0.04896498255665459], Acc [0.988,0.982] took 121.63s\n",
      "Accuracy by class [[0.99, 0.989, 0.985],[0.983, 0.993, 0.97]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 80: Loss [0.03230020023843015,0.05508015581363609], Acc [0.99,0.983] took 121.32s\n",
      "Accuracy by class [[0.991, 0.992, 0.986],[0.984, 0.986, 0.98]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 81: Loss [0.016812994199579465,0.0396570541857773], Acc [0.996,0.989] took 120.54s\n",
      "Accuracy by class [[0.995, 0.996, 0.995],[0.988, 0.995, 0.984]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 82: Loss [0.01100983715927015,0.04045435495991572], Acc [0.998,0.986] took 121.05s\n",
      "Accuracy by class [[0.999, 0.998, 0.997],[0.985, 0.988, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 83: Loss [0.00906412374706251,0.038032662086267065], Acc [0.999,0.988] took 121.6s\n",
      "Accuracy by class [[0.999, 0.999, 0.998],[0.987, 0.992, 0.986]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 84: Loss [0.008027567618434555,0.03373932263260208], Acc [0.999,0.988] took 117.67s\n",
      "Accuracy by class [[1.0, 0.999, 0.999],[0.984, 0.992, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 85: Loss [0.008192162002359057,0.03234567376442582], Acc [0.999,0.989] took 114.06s\n",
      "Accuracy by class [[0.999, 0.999, 0.999],[0.99, 0.99, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 86: Loss [0.018683439704836324,0.026429878733905437], Acc [0.994,0.991] took 113.42s\n",
      "Accuracy by class [[0.995, 0.995, 0.992],[0.991, 0.991, 0.99]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 87: Loss [0.013856738951442883,0.02861934377344232], Acc [0.996,0.991] took 113.74s\n",
      "Accuracy by class [[0.996, 0.997, 0.996],[0.992, 0.994, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 88: Loss [0.00844123272045869,0.03067746725199479], Acc [0.999,0.99] took 113.63s\n",
      "Accuracy by class [[0.999, 0.999, 0.998],[0.99, 0.995, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 89: Loss [0.008506827887793089,0.04406471043422098], Acc [0.999,0.986] took 114.73s\n",
      "Accuracy by class [[0.999, 0.999, 0.998],[0.986, 0.983, 0.987]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 90: Loss [0.01259398559729461,0.026256409926171714], Acc [0.997,0.993] took 114.06s\n",
      "Accuracy by class [[0.998, 0.996, 0.997],[0.992, 0.998, 0.988]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 91: Loss [0.013266152205030544,0.024782319239168547], Acc [0.997,0.992] took 114.42s\n",
      "Accuracy by class [[0.998, 0.997, 0.996],[0.99, 0.997, 0.99]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 92: Loss [0.01665980738382605,0.035860216583540085], Acc [0.995,0.988] took 114.39s\n",
      "Accuracy by class [[0.996, 0.995, 0.994],[0.988, 0.989, 0.985]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 93: Loss [0.010520671701318344,0.029051742562437182], Acc [0.997,0.992] took 113.91s\n",
      "Accuracy by class [[0.998, 0.998, 0.996],[0.991, 0.992, 0.994]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 94: Loss [0.010861946639002366,0.045804104622954325], Acc [0.997,0.989] took 114.5s\n",
      "Accuracy by class [[0.997, 0.998, 0.996],[0.983, 0.997, 0.986]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 95: Loss [0.008718697214991317,0.03385380095510721], Acc [0.998,0.992] took 113.92s\n",
      "Accuracy by class [[0.998, 0.999, 0.998],[0.988, 0.993, 0.995]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 96: Loss [0.008501816280695797,0.017809100678491896], Acc [0.998,0.994] took 114.08s\n",
      "Accuracy by class [[0.999, 0.998, 0.998],[0.99, 0.995, 0.996]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 97: Loss [0.00834159347509778,0.03195281257973239], Acc [0.998,0.991] took 113.45s\n",
      "Accuracy by class [[0.998, 0.998, 0.998],[0.988, 0.993, 0.992]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 98: Loss [0.05788834862522427,0.051355580603625585], Acc [0.983,0.982] took 114.05s\n",
      "Accuracy by class [[0.985, 0.985, 0.981],[0.984, 0.984, 0.977]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "-----------------------------\n",
      "Epoch 99: Loss [0.03781565963888604,0.07789305167491822], Acc [0.988,0.976] took 114.11s\n",
      "Accuracy by class [[0.99, 0.989, 0.985],[0.972, 0.972, 0.982]] Total elements: [[6066, 6090, 6884],[1460, 1591, 1709]]\n",
      "...Training finished\n"
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    start_time = time.time()\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_test = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "#         print(\"Time steps:{}, input sequence length:{}\".format(TIME_STEPS,len(data_in)))\n",
    "        #print(\"NN input: \",len(flows),len(flows[0]),len(flows[0][0]),len(flows[0][0][0]),len(flows[0][0][0][0]))\n",
    "        optimizer.zero_grad() \n",
    "        outputs = net(data_in)\n",
    "        labels = labels.squeeze(0).long()\n",
    "        outputs = outputs.squeeze(0).float()\n",
    "        \n",
    "#         print(\"Size output\",outputs.size(),\"and label\", labels.size())\n",
    "#         print(outputs)\n",
    "        \n",
    "        loss = criterion(outputs[SLICE_FROM_TIME_STEP:TIME_STEPS,:].float(),labels[SLICE_FROM_TIME_STEP:TIME_STEPS].long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_train += loss.data.item()/((1-VALIDATION_SPLIT)*len(dataset))\n",
    "        \n",
    "#         acc_tracker.update_lr(LR)\n",
    "\n",
    "        prediction = torch.max(outputs,1)[1]\n",
    "        label_element = labels\n",
    "    \n",
    "        tt_acc_tracker.update_acc(prediction,label_element,\"TRAIN\")\n",
    "        #         print(\"Prediction\", labels)\n",
    "#         acc_tracker.update_acc(prediction,labels)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "            data_in = [s.to(device) for s in inputs['flows']]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(data_in)\n",
    "            labels = labels.squeeze(0).long()\n",
    "            outputs = outputs.squeeze(0).float()\n",
    "            loss = criterion(outputs[:,SLICE_FROM_TIME_STEP:TIME_STEPS],labels[SLICE_FROM_TIME_STEP:TIME_STEPS])\n",
    "            running_loss_test += loss.data.item()/(VALIDATION_SPLIT*len(dataset))\n",
    "\n",
    "            prediction = torch.max(outputs,1)[1]\n",
    "            label_element = labels\n",
    "\n",
    "            tt_acc_tracker.update_acc(prediction,label_element,\"TEST\")\n",
    "   \n",
    "    elapsed_time = time.time() - start_time;      \n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Epoch {}: Loss [{},{}], Acc [{},{}] took {}s\".format(epoch, running_loss_train,running_loss_test,tt_acc_tracker.get_acc_tot(\"TRAIN\"),tt_acc_tracker.get_acc_tot(\"TEST\"), np.around(elapsed_time,decimals=2)))\n",
    "    print(\"Accuracy by class [{},{}] Total elements: [{},{}]\".format(tt_acc_tracker.get_acc(\"TRAIN\"),tt_acc_tracker.get_acc(\"TEST\"), tt_acc_tracker.get_labels_distribution(\"TRAIN\"),tt_acc_tracker.get_labels_distribution(\"TEST\")))\n",
    "\n",
    "    tt_acc_tracker.update_loss(running_loss_train,\"TRAIN\")\n",
    "    tt_acc_tracker.update_loss(running_loss_test,\"TEST\")\n",
    "    tt_acc_tracker.update_graph()\n",
    "    tt_acc_tracker.reset_acc_both()\n",
    "    tt_acc_tracker.update_lr(LR)\n",
    "\n",
    "print('...Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
