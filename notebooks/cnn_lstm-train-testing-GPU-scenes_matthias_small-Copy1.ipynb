{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  scenes\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "\n",
    "crop_im_size = [186,250]\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "c.config['indexing']['people selection'] = [1]\n",
    "#c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "c.config['pose']['load'] = False\n",
    "c.config['flow']['load'] = False\n",
    "c.config['heatmaps']['load'] = False\n",
    "#c.config['scenes']['sequences'] = ['nm']\n",
    "#c.config['scenes']['angles'] = ['108']\n",
    "c.config['dataset_output'] = {\n",
    "#         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "        'data': [\"scenes\"],\n",
    "        'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NETWORK AND DATASET PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters defined\n"
     ]
    }
   ],
   "source": [
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2 \n",
    "IMAGE_INPUT_SIZE_W = 640\n",
    "IMAGE_INPUT_SIZE_H = 480\n",
    "\n",
    "IMAGE_AFTER_CONV_SIZE_W = 18\n",
    "IMAGE_AFTER_CONV_SIZE_H = 13\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_IO_SIZE = 18*13\n",
    "LSTM_HIDDEN_SIZE = 18*13\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 40 # size videos\n",
    "BATCH_SIZE = 1 #until now just batch_size = 1\n",
    "\n",
    "NR_EPOCHS = 50\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOMIZED_SEED = 10\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "learning_rate = 0.01 # reduce factos of 10 .. some epoch later.\n",
    "momentum = 0.9\n",
    "print(\"Hyperparameters defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings \n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5\n",
      "Indices size: 5\n",
      "Split: 1\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#USE RANDOM IMAGES TO SET UP WORKING EXAMPLE\n",
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.avialable_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #input 640x480\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #input 638x478 output 319x239\n",
    "        self.conv2 = nn.Conv2d(6,16,3) # input 319x239 output 317x237\n",
    "        self.pool2 = nn.MaxPool2d(2,2) # input 317x237 output 158x118\n",
    "        self.conv3 = nn.Conv2d(16,6,3) # input 158x118 output 156x116\n",
    "        self.pool3 = nn.MaxPool2d(2,2) # input 156x116 output 78x58\n",
    "        self.conv4 = nn.Conv2d(6,3,3)  # input 78x58 output 76x56\n",
    "        self.pool4 = nn.MaxPool2d(2,2) # input 76x56 output 39x29\n",
    "        self.conv5 = nn.Conv2d(3,1,3)  # input 39x29 output 37x27\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #output 37x27 output 18x13\n",
    "        self.lstm1 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.lstm2 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.fc1 = nn.Linear(LSTM_IO_SIZE,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of LSTM\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        #print(\"Hidden:\", _hidden)\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device), \n",
    "                torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device))\n",
    "    def forward(self,x):\n",
    "#         print(\"Input list len:\",len(x))\n",
    "#         print(\"Input elemens size:\", x[0].size())\n",
    "#         batch_size = x[0].size()[0]\n",
    "\n",
    "        x_arr = torch.zeros(TIMESTEPS,BATCH_SIZE,1,IMAGE_AFTER_CONV_SIZE_H,IMAGE_AFTER_CONV_SIZE_W).to(self.avialable_device)\n",
    "        ## print(\"X arr size\", x_arr.size())\n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i].float())))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = x_tmp_c5 # torch.squeeze(x_tmp_c5)\n",
    "        \n",
    "        x, hidden = self.lstm1(x_arr.view(TIMESTEPS,BATCH_SIZE,-1), self.hidden)\n",
    "        x, hidden = self.lstm2(x, self.hidden)\n",
    "        # the reshaping was taken from the documentation... and makes scense\n",
    "        x = x.view(TIMESTEPS,BATCH_SIZE,LSTM_HIDDEN_SIZE) #output.view(seq_len, batch, num_dir*hidden_size)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        x = x.permute(1,2,0)\n",
    "        return x\n",
    "print(\"Class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOS:\n",
    "# Look for overfitting..\n",
    "# use GPU: 300 samples. many epochs. learning rate maybe is too high.\n",
    "# Tweak learning rate\n",
    "# Increase the RNN size\n",
    "# Maybe change the CNN\n",
    "# Use patches. or OF\n",
    "# Increase the dataset: 10 images 10peoplo+++ not too much too much variance. \n",
    "# FEEDBACK on friday..:) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define and execute testing function\n",
    "def test_all_preds(model):\n",
    "    n_batches_test = len(test_loader)\n",
    "\n",
    "    #Time for printing\n",
    "    testing_start_time = time.time()\n",
    "\n",
    "    print('Start testing...')\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            scenes = [s.to(device) for s in inputs['scenes']]\n",
    "            labels = labels.to(device)\n",
    "            if not labels.size()[0] == BATCH_SIZE:\n",
    "                # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "                continue\n",
    "            outputs = model(scenes)\n",
    "#             print(\"Out:\", len(outputs), outputs.size())\n",
    "#             print(\"Labels:\", len(labels), labels.size())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "            n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "            total += predicted.numel()\n",
    "            # print('predicted',predicted)\n",
    "            correct += predicted.numel() - n_errors\n",
    "            # print('labels',labels)\n",
    "    print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "    print('...testing finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training over all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ron/anaconda3/envs/gait_37/lib/python3.7/site-packages/pandas/core/computation/expressions.py:180: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  .format(op=op_str, alt_op=unsupported[op_str]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss epoch 0: 4.5122010707855225, took 3.152700901031494s\n",
      "Epoch: 1\n",
      "Loss epoch 1: 4.4953895807266235, took 3.031104803085327s\n",
      "Epoch: 2\n",
      "Loss epoch 2: 4.469879984855652, took 2.8708813190460205s\n",
      "Epoch: 3\n",
      "Loss epoch 3: 4.442201375961304, took 2.927917718887329s\n",
      "Epoch: 4\n",
      "Loss epoch 4: 4.416372179985046, took 3.1973021030426025s\n",
      "Epoch: 5\n",
      "Loss epoch 5: 4.394070386886597, took 2.8572919368743896s\n",
      "Epoch: 6\n",
      "Loss epoch 6: 4.375882029533386, took 2.8608944416046143s\n",
      "Epoch: 7\n",
      "Loss epoch 7: 4.361982822418213, took 2.9415712356567383s\n",
      "Epoch: 8\n",
      "Loss epoch 8: 4.351172566413879, took 2.9368441104888916s\n",
      "Epoch: 9\n",
      "Loss epoch 9: 4.342569470405579, took 3.14585280418396s\n",
      "Epoch: 10\n",
      "Loss epoch 10: 4.336242318153381, took 3.1707701683044434s\n",
      "Epoch: 11\n",
      "Loss epoch 11: 4.331258654594421, took 3.084019660949707s\n",
      "Epoch: 12\n",
      "Loss epoch 12: 4.3269122838974, took 3.204972267150879s\n",
      "Epoch: 13\n",
      "Loss epoch 13: 4.322893023490906, took 3.084568738937378s\n",
      "Epoch: 14\n",
      "Loss epoch 14: 4.31899106502533, took 2.7815818786621094s\n",
      "Epoch: 15\n",
      "Loss epoch 15: 4.315061450004578, took 2.99242901802063s\n",
      "Epoch: 16\n",
      "Loss epoch 16: 4.311006546020508, took 2.927687406539917s\n",
      "Epoch: 17\n",
      "Loss epoch 17: 4.3067967891693115, took 3.064059019088745s\n",
      "Epoch: 18\n",
      "Loss epoch 18: 4.302248120307922, took 3.1234400272369385s\n",
      "Epoch: 19\n",
      "Loss epoch 19: 4.297313332557678, took 3.021721363067627s\n",
      "Epoch: 20\n",
      "Loss epoch 20: 4.291918516159058, took 3.041076421737671s\n",
      "Epoch: 21\n",
      "Loss epoch 21: 4.285990834236145, took 3.072270154953003s\n",
      "Epoch: 22\n",
      "Loss epoch 22: 4.2794588804244995, took 2.7087349891662598s\n",
      "Epoch: 23\n",
      "Loss epoch 23: 4.272274732589722, took 3.119521379470825s\n",
      "Epoch: 24\n",
      "Loss epoch 24: 4.2643303871154785, took 3.068228244781494s\n",
      "Epoch: 25\n",
      "Loss epoch 25: 4.25551962852478, took 2.9162404537200928s\n",
      "Epoch: 26\n",
      "Loss epoch 26: 4.245736718177795, took 2.8571770191192627s\n",
      "Epoch: 27\n",
      "Loss epoch 27: 4.234842896461487, took 2.7451870441436768s\n",
      "Epoch: 28\n",
      "Loss epoch 28: 4.222771406173706, took 2.857851505279541s\n",
      "Epoch: 29\n",
      "Loss epoch 29: 4.2095195055007935, took 2.9894649982452393s\n",
      "Epoch: 30\n",
      "Loss epoch 30: 4.194989323616028, took 2.8267431259155273s\n",
      "Epoch: 31\n",
      "Loss epoch 31: 4.1791733503341675, took 2.866313934326172s\n",
      "Epoch: 32\n",
      "Loss epoch 32: 4.161984324455261, took 2.88295841217041s\n",
      "Epoch: 33\n",
      "Loss epoch 33: 4.143446326255798, took 3.0647826194763184s\n",
      "Epoch: 34\n",
      "Loss epoch 34: 4.123706817626953, took 2.8580756187438965s\n",
      "Epoch: 35\n",
      "Loss epoch 35: 4.10288393497467, took 2.9099323749542236s\n",
      "Epoch: 36\n",
      "Loss epoch 36: 4.0811744928359985, took 2.968147039413452s\n",
      "Epoch: 37\n",
      "Loss epoch 37: 4.058809995651245, took 2.8400654792785645s\n",
      "Epoch: 38\n",
      "Loss epoch 38: 4.036079049110413, took 2.900007963180542s\n",
      "Epoch: 39\n",
      "Loss epoch 39: 4.0132155418396, took 2.895012378692627s\n",
      "Epoch: 40\n",
      "Loss epoch 40: 3.990487575531006, took 2.7855169773101807s\n",
      "Epoch: 41\n",
      "Loss epoch 41: 3.968134105205536, took 2.8359615802764893s\n",
      "Epoch: 42\n",
      "Loss epoch 42: 3.9463399052619934, took 2.8544912338256836s\n",
      "Epoch: 43\n",
      "Loss epoch 43: 3.9257365465164185, took 2.8073301315307617s\n",
      "Epoch: 44\n",
      "Loss epoch 44: 3.906275451183319, took 2.8550498485565186s\n",
      "Epoch: 45\n",
      "Loss epoch 45: 3.887884020805359, took 3.0609164237976074s\n",
      "Epoch: 46\n",
      "Loss epoch 46: 3.8705608248710632, took 2.929865598678589s\n",
      "Epoch: 47\n",
      "Loss epoch 47: 3.8543724417686462, took 2.835726499557495s\n",
      "Epoch: 48\n",
      "Loss epoch 48: 3.8393037915229797, took 2.836664915084839s\n",
      "Epoch: 49\n",
      "Loss epoch 49: 3.8253930807113647, took 2.8505959510803223s\n",
      "Start testing...\n",
      "Accuracy 53.75%\n",
      "...testing finished\n",
      "...Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7130b8beb8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "\n",
    "#Time for printing\n",
    "training_start_time = time.time()\n",
    "\n",
    "loss_array = np.zeros(NR_EPOCHS)\n",
    "\n",
    "print('Start training...')\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "#     if (epoch == 80):\n",
    "#         learning_rate = 0.001\n",
    "#         optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#         print (\"Learning rate changed\", learning_rate)\n",
    "#     if (epoch == 80):\n",
    "#         learning_rate = 0.001\n",
    "#         optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#         print (\"Learning rate changed\", learning_rate)\n",
    "    \n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader): #loads all angles\n",
    "        inputs, labels = batch\n",
    "        scenes = [s.to(device) for s in inputs['scenes']]\n",
    "        labels = labels.unsqueeze(-1).to(device)\n",
    "        if not labels.size()[0] == BATCH_SIZE:\n",
    "            # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "            continue\n",
    "        optimizer.zero_grad() \n",
    "        outputs = test_net(scenes)\n",
    "        outputs = outputs.unsqueeze(-1)\n",
    "        #print(\"Size before permutation:\", outputs.size())\n",
    "        outputs = outputs.permute(0,2,1,3).squeeze(3).squeeze(0)\n",
    "        labels = labels.squeeze(2).squeeze(0)\n",
    "        #print(\"Out:\", len(outputs), outputs.size())\n",
    "        #print(\"Labels:\", len(labels), labels.size())\n",
    "        #print(\"Here\")\n",
    "        #print(scenes[0].squeeze(0).permute(2,1,0).size())\n",
    "        #for j in range(10):\n",
    "        #    scene_print = scenes[j].squeeze(0).permute(1,2,0)\n",
    "        #    plt.imshow(scene_print.numpy())\n",
    "        #    plt.show()\n",
    "        #print(labels)\n",
    "        loss = criterion(outputs,labels.long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print statistics\n",
    "        # print(loss.data.item())\n",
    "        running_loss += loss.data.item()\n",
    "        #total_train_loss += loss.data.item()\n",
    "        \n",
    "        #Print every 10th batch of an epoch\n",
    "#         if (i + 1) % (print_every + 1) == 0:\n",
    "#             print(\"Epoch {}, {:d}% \\t train_loss(mean): {:.2f} took: {:.2f}s\".format(\n",
    "#                     epoch+1, int(100 * (i+1) / n_batches), running_loss/print_every, time.time() - start_time))\n",
    "#             #Reset running loss and time\n",
    "#             running_loss = 0.0\n",
    "#             start_time = time.time()\n",
    "    # test after each epoch\n",
    "    print(\"Loss epoch {}: {}, took {}s\".format(epoch, running_loss,time.time()-start_time))\n",
    "    loss_array[epoch] = running_loss\n",
    "\n",
    "test_all_preds(test_net)\n",
    "#print('total training loss for epoch {}: {:.6f}'.format(epoch+1, total_train_loss))    \n",
    "print('...Training finished')\n",
    "plt.plot(loss_array)\n",
    "plt.plot(loss_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels.size())\n",
    "# print(outputs.size())\n",
    "# print(labels)\n",
    "# print(outputs)\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(scenes[0].squeeze(0).size())\n",
    "# plt.imshow(scenes[9].squeeze(0).permute(1,2,0))\n",
    "# plt.figure(figsize=(30,40))\n",
    "# print('left foot trans from air to ground')\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(scenes[4])\n",
    "# plt.title('frame 44')\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(scenes[5])\n",
    "# plt.title('frame 45')\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(scenes[6])\n",
    "# plt.title('frame 46')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_37",
   "language": "python",
   "name": "gait_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
