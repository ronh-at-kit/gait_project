{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. flows_1\n",
      "Hyperparameters defined\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. flows_1\")\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2 \n",
    "IMAGE_INPUT_SIZE_W = 640\n",
    "IMAGE_INPUT_SIZE_H = 480\n",
    "\n",
    "IMAGE_AFTER_CONV_SIZE_W = 18\n",
    "IMAGE_AFTER_CONV_SIZE_H = 13\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_IO_SIZE = 18*13\n",
    "LSTM_HIDDEN_SIZE = 18*13\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 30 # size videos\n",
    "BATCH_SIZE = 1 #until now just batch_size = 1\n",
    "SLICE_FROM_TIMESTEP = 1 #slices from timestep SLICE_FROM_TIMESTEP to the last one\n",
    "\n",
    "NR_EPOCHS = 20\n",
    "\n",
    "VALIDATION_SPLIT = 0.0 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "USE_EXISTING_NORMALIZATION = True\n",
    "\n",
    "learning_rate = 0.01 # reduce factos of 10 .. some epoch later.\n",
    "momentum = 0.9\n",
    "print(\"Hyperparameters defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change configuration in settings.py\n",
    "crop_im_size = [186,250]\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "c.config['transformers']['DimensionResize']['dimension'] = TIMESTEPS\n",
    "#c.config['indexing']['people_selection'] = [3]\n",
    "#c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "c.config['pose']['load'] = False\n",
    "c.config['flow']['load'] = True\n",
    "c.config['heatmaps']['load'] = False\n",
    "#c.config['annotations']['preprocess']=True\n",
    "#c.config['scenes']['sequences'] = ['nm']\n",
    "#c.config['scenes']['angles'] = ['108']\n",
    "c.config['dataset_output'] = {\n",
    "#         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "        'data': ['flows'],\n",
    "        'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "a,b = dataset[1]\n",
    "\n",
    "print(len(a[\"flows\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['flows'])\n"
     ]
    }
   ],
   "source": [
    "print(a.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 135\n",
      "Indices size: 135\n",
      "Split: 0\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "\n",
    "mean = [126.1972, 127.50477, 1.3415707]\n",
    "std_dev = [6.436958,  1.7359118, 5.022056 ]\n",
    "\n",
    "#mean = [126.04878374565972, 127.50835754123264, 1.5171344184027777]\n",
    "#std_dev = [7.03813454, 2.04901181, 5.66295848]\n",
    "if not(USE_EXISTING_NORMALIZATION):\n",
    "    print('Normalizing')\n",
    "    image_list_ch_0 = []\n",
    "    image_list_ch_1 = []\n",
    "    image_list_ch_2 = []\n",
    "    for data in dataset:\n",
    "        print('here')\n",
    "        for image in data[0]['flows']:\n",
    "            image_list_ch_0.append(image[0].numpy())\n",
    "            image_list_ch_1.append(image[1].numpy())\n",
    "            image_list_ch_2.append(image[2].numpy())\n",
    "    print(\"List of channels created\")\n",
    "    mean = [np.mean(image_list_ch_0),np.mean(image_list_ch_1),np.mean(image_list_ch_2)]\n",
    "    std_dev = np.sqrt([np.var(image_list_ch_0),np.var(image_list_ch_1),np.var(image_list_ch_2)])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# print(\"Data in original\", len(data_in))\n",
    "# data_in_norm = data_in\n",
    "# for i,data in enumerate(data_in):\n",
    "#     for image in data:\n",
    "#         ch_0 = (image[0][0] - mean[0]) / std_dev[0]\n",
    "#         ch_1 = (image[0][1] - mean[1]) / std_dev[1]\n",
    "#         ch_2 = (image[0][2] - mean[2]) / std_dev[2]\n",
    "#         print(ch_3)\n",
    "# #         channel_list = [ch_0, ch_1, ch_2]\n",
    "# #         data_in_norm.append(channel_list)\n",
    "  \n",
    "# print(\"New\",data_in_norm)\n",
    "# print(\"Original\",data_in)\n",
    "# dataset_new = dataset\n",
    "# print(\"Normalizing images... DOES NOT WORK LIKE THIS\")\n",
    "# for i, data in enumerate(dataset):\n",
    "#     print(\"Dataset item nr \", i)\n",
    "#     print(\"Iterating through\", len(data[0]['flows']), \"images\")\n",
    "#     for j,image in enumerate(data[0]['flows']):\n",
    "#         image[0] = (image[0] - mean[0]) / std_dev[0]\n",
    "#         image[1] = (image[1] - mean[1]) / std_dev[1]\n",
    "#         image[2] = (image[2] - mean[2]) / std_dev[2]\n",
    "#         print(\"Before\", dataset_new[i][0]['flows'][j])\n",
    "#         dataset_new[i][0]['flows'][j] = image\n",
    "#         print(\"Image\",image)\n",
    "#         print(\"From dataset\",dataset_new[i][0]['flows'][j])\n",
    "    \n",
    "#         print(\"Dataset image\",dataset_new[i][0]['flows'][0])\n",
    "# print(\"Images normalized with mean and std dev\",mean, std_dev)\n",
    "# print(\"Display normalized image as array:\")\n",
    "# print(image[1])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined\n"
     ]
    }
   ],
   "source": [
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.avialable_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #input 640x480\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #input 638x478 output 319x239\n",
    "        self.conv2 = nn.Conv2d(6,16,3) # input 319x239 output 317x237\n",
    "        self.pool2 = nn.MaxPool2d(2,2) # input 317x237 output 158x118\n",
    "        self.conv3 = nn.Conv2d(16,6,3) # input 158x118 output 156x116\n",
    "        self.pool3 = nn.MaxPool2d(2,2) # input 156x116 output 78x58\n",
    "        self.conv4 = nn.Conv2d(6,3,3)  # input 78x58 output 76x56\n",
    "        self.pool4 = nn.MaxPool2d(2,2) # input 76x56 output 39x29\n",
    "        self.conv5 = nn.Conv2d(3,1,3)  # input 39x29 output 37x27\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #output 37x27 output 18x13\n",
    "        self.lstm1 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# before: TIMESTEPS. But TIMESTEPS should be wrong\n",
    "#         self.lstm2 = nn.LSTM(LSTM_IO_SIZE,\n",
    "#                             LSTM_HIDDEN_SIZE,\n",
    "#                             TIMESTEPS)# before: TIMESTEPS. But TIMESTEPS should be wrong\n",
    "        self.fc1 = nn.Linear(LSTM_IO_SIZE,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of LSTM\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        #print(\"Hidden:\", _hidden)\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device),#before: TIMESTEPS \n",
    "                torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device))#before: TIMESTEPS\n",
    "    def forward(self,x):\n",
    "#         print(\"Input list len:\",len(x))\n",
    "#         print(\"Input elemens size:\", x[0].size())\n",
    "#         batch_size = x[0].size()[0]\n",
    "\n",
    "        x_arr = torch.zeros(TIMESTEPS,BATCH_SIZE,1,IMAGE_AFTER_CONV_SIZE_H,IMAGE_AFTER_CONV_SIZE_W).to(self.avialable_device)\n",
    "        #print(\"X arr size\", x_arr.size())\n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i])))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = x_tmp_c5\n",
    "        \n",
    "        x = torch.cat((x_arr[0],\n",
    "              x_arr[1],\n",
    "              x_arr[2],\n",
    "              x_arr[3],\n",
    "              x_arr[4],\n",
    "              x_arr[5],\n",
    "              x_arr[6],\n",
    "              x_arr[7],\n",
    "              x_arr[8],\n",
    "              x_arr[9]),0)\n",
    "        x, hidden = self.lstm1(x_arr.view(TIMESTEPS,BATCH_SIZE,-1), self.hidden)#before: TIMESTEPS\n",
    "#         x, hidden = self.lstm2(x, self.hidden)\n",
    "        # the reshaping was taken from the documentation... and makes scense\n",
    "        x = x.view(TIMESTEPS,BATCH_SIZE,LSTM_HIDDEN_SIZE) #output.view(seq_len, batch, num_dir*hidden_size)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        #x = x.squeeze(1)\n",
    "        x = x.permute(1,2,0)\n",
    "        #print (\"Size network output\", x.shape)\n",
    "        return x\n",
    "print(\"Class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition done\n"
     ]
    }
   ],
   "source": [
    "### define and execute testing function\n",
    "def test_all_preds(model):\n",
    "    n_batches_test = len(test_loader)\n",
    "\n",
    "    #Time for printing\n",
    "    testing_start_time = time.time()\n",
    "\n",
    "    print('Start testing...')\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            data_in = [s.to(device) for s in inputs['flows']]\n",
    "            labels = labels.to(device)\n",
    "            if not labels.size()[0] == BATCH_SIZE:\n",
    "                # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "                continue\n",
    "            outputs = model(data_in)\n",
    "#             print(\"Out:\", len(outputs), outputs.size())\n",
    "#             print(\"Labels:\", len(labels), labels.size())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "            n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "            total += predicted.numel()\n",
    "            # print('predicted',predicted)\n",
    "            correct += predicted.numel() - n_errors\n",
    "            # print('labels',labels)\n",
    "    print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "    print('...testing finished')\n",
    "print(\"Definition done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set and evaluate computing time etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Expected loss with 3 different classes and 30 data elements: 32.95836866004329\n",
      "Batch size: 1\n",
      "Evaluating first element...\n",
      "Proof for normalized data: tensor([[[ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.1352,  0.2772,  0.2772],\n",
      "         ...,\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772]],\n",
      "\n",
      "        [[-0.2481,  0.2399,  0.2399,  ...,  0.2399,  0.2399,  0.2399],\n",
      "         [-0.2481,  0.2399,  0.2399,  ...,  0.2399,  0.2399,  0.2399],\n",
      "         [-0.2481,  0.2399, -0.2481,  ...,  0.2399,  0.2399,  0.2399],\n",
      "         ...,\n",
      "         [ 0.2399,  0.2399, -0.2481,  ..., -0.2481, -0.2481, -0.2481],\n",
      "         [-0.2481,  0.2399,  0.2399,  ..., -0.2481, -0.2481, -0.2481],\n",
      "         [ 0.2399,  0.2399,  0.2399,  ..., -0.2481, -0.2481, -0.2481]],\n",
      "\n",
      "        [[-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         ...,\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679]]],\n",
      "       device='cuda:0')\n",
      "Time steps:10, input sequence length:10\n",
      "Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\n",
      "Output format: 1 torch.Size([1, 3, 10])\n",
      "Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\n",
      "Labels: 1 torch.Size([1, 10])\n",
      "Slicing loss. Using loss from: 1 to 10\n",
      "Loss:1.1128796339035034, expected loss:1.0986122886681098\n",
      "Time needed:1.1913647651672363s\n",
      "Expected loss for total training data:  32.95836866004329\n",
      "Expected training time per epoch:0.5956823825836182 min\n",
      "Estimated total training time:0.1985607941945394 hours\n"
     ]
    }
   ],
   "source": [
    "#PREPARATION FOR TRAINING\n",
    "loss_array = []\n",
    "learning_rate_array = []\n",
    "\n",
    "print('Start training...')\n",
    "print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "running_loss = 0.0\n",
    "#print(\"Data set length:\", len((train_loader)), \"Validation length:\", len(test_loader))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Evaluating first element...\")\n",
    "start_time = time.time()\n",
    "i, batch = next(iter(enumerate(train_loader)))\n",
    "inputs, labels = batch\n",
    "data_in = [s.to(device) for s in inputs['flows']]\n",
    "# print(\"Data in original\", data_in)\n",
    "# data_in_norm = data_in\n",
    "# for i,image in enumerate(data_in):\n",
    "#     print(\"Normalizing:\",i)\n",
    "# #     data_in_norm[i][1] = data[i][1]\n",
    "#     print()\n",
    "#     data_in_norm[0] = (image[0] - mean[0]) / std_dev[0]\n",
    "#     data_in_norm[1] = (image[1] - mean[1]) / std_dev[1]\n",
    "#     data_in_norm[2] = (image[2] - mean[2]) / std_dev[2]\n",
    "# print(data_in_norm[0])\n",
    "# print(\"Normalization done\")\n",
    "\n",
    "print(\"Proof for normalized data:\", data_in[0][0])\n",
    "labels = labels.to(device)\n",
    "print(\"Time steps:{}, input sequence length:{}\".format(TIMESTEPS,len(data_in)))\n",
    "#print(\"NN input: \",len(flows),len(flows[0]),len(flows[0][0]),len(flows[0][0][0]),len(flows[0][0][0][0]))\n",
    "optimizer.zero_grad() \n",
    "outputs = test_net(data_in)\n",
    "print(\"Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\")\n",
    "print(\"Output format:\", len(outputs), outputs.size())\n",
    "print(\"Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\")\n",
    "print(\"Labels:\", len(labels), labels.size())\n",
    "print(\"Slicing loss. Using loss from:\",SLICE_FROM_TIMESTEP,\"to\",TIMESTEPS)\n",
    "#print(\"Labels content:\", labels)\n",
    "#original: loss = criterion(outputs.float(),labels.long())\n",
    "loss = criterion(outputs[:,:,SLICE_FROM_TIMESTEP:TIMESTEPS].float(),labels[:,SLICE_FROM_TIMESTEP:TIMESTEPS].long())\n",
    "loss.backward() \n",
    "optimizer.step()\n",
    "\n",
    "running_loss += loss.data.item()\n",
    "elapsed_time = time.time() - start_time;\n",
    "loss_array.append(running_loss)\n",
    "learning_rate_array.append(learning_rate)\n",
    "print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "print(\"Time needed:{}s\".format(elapsed_time))\n",
    "print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)/60))\n",
    "print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*NR_EPOCHS/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss epoch 0: 4.815722219645977, took 33.41770076751709s\n",
      "Epoch: 1\n",
      "Loss epoch 1: 4.760320007801056, took 33.43797540664673s\n",
      "Epoch: 2\n",
      "Loss epoch 2: 4.814400762319565, took 33.32983875274658s\n",
      "Epoch: 3\n",
      "Loss epoch 3: 4.826962321996689, took 33.5497944355011s\n",
      "Epoch: 4\n",
      "Loss epoch 4: 4.7932266145944595, took 33.561710357666016s\n",
      "Epoch: 5\n",
      "Loss epoch 5: 4.8098878636956215, took 33.350581884384155s\n",
      "Epoch: 6\n",
      "Loss epoch 6: 4.801881864666939, took 33.707473039627075s\n",
      "Epoch: 7\n",
      "Loss epoch 7: 4.806560091674328, took 33.43438959121704s\n",
      "Epoch: 8\n",
      "Loss epoch 8: 4.807913966476917, took 33.41324996948242s\n",
      "Epoch: 9\n",
      "Loss epoch 9: 4.794204398989677, took 33.36453413963318s\n",
      "Epoch: 10\n",
      "Loss epoch 10: 4.828056283295155, took 33.69233965873718s\n",
      "Epoch: 11\n",
      "Loss epoch 11: 4.808852426707745, took 33.36387228965759s\n",
      "Epoch: 12\n",
      "Loss epoch 12: 4.793481923639774, took 33.50886011123657s\n",
      "Epoch: 13\n",
      "Loss epoch 13: 4.825078494846821, took 33.56784152984619s\n",
      "Epoch: 14\n",
      "Loss epoch 14: 4.783532977104187, took 33.467684268951416s\n",
      "Epoch: 15\n",
      "Loss epoch 15: 4.800808660686016, took 33.35645294189453s\n",
      "Epoch: 16\n",
      "Loss epoch 16: 4.802496522665024, took 33.35033631324768s\n",
      "Epoch: 17\n",
      "Loss epoch 17: 4.79634266346693, took 33.511518716812134s\n",
      "Epoch: 18\n",
      "Loss epoch 18: 4.79426196962595, took 33.35241889953613s\n",
      "Epoch: 19\n",
      "Loss epoch 19: 4.794772990047932, took 33.492714405059814s\n",
      "Epoch: 20\n",
      "Loss epoch 20: 4.816589370369911, took 33.38615083694458s\n",
      "Epoch: 21\n",
      "Loss epoch 21: 4.7845097705721855, took 33.62457346916199s\n",
      "Epoch: 22\n",
      "Loss epoch 22: 4.795794494450092, took 33.67045474052429s\n",
      "Epoch: 23\n",
      "Loss epoch 23: 4.790766268968582, took 33.37505102157593s\n",
      "Epoch: 24\n",
      "Loss epoch 24: 4.816877499222755, took 33.49303698539734s\n",
      "Epoch: 25\n",
      "Loss epoch 25: 4.78494007140398, took 33.40695118904114s\n",
      "Epoch: 26\n",
      "Loss epoch 26: 4.816715240478516, took 33.58631348609924s\n",
      "Epoch: 27\n",
      "Loss epoch 27: 4.781040549278259, took 33.506957054138184s\n",
      "Epoch: 28\n",
      "Loss epoch 28: 4.811777874827385, took 33.521520137786865s\n",
      "Epoch: 29\n",
      "Loss epoch 29: 4.804732449352741, took 33.51124334335327s\n",
      "Epoch: 30\n",
      "Loss epoch 30: 4.771151192486286, took 33.552847385406494s\n",
      "Epoch: 31\n",
      "Loss epoch 31: 4.788126155734062, took 33.55895638465881s\n",
      "Epoch: 32\n",
      "Loss epoch 32: 4.793333113193512, took 33.681193828582764s\n",
      "Epoch: 33\n",
      "Loss epoch 33: 4.811734452843666, took 33.53533411026001s\n",
      "Epoch: 34\n",
      "Loss epoch 34: 4.772535756230354, took 33.401981592178345s\n",
      "Epoch: 35\n",
      "Loss epoch 35: 4.7807193621993065, took 33.627788066864014s\n",
      "Epoch: 36\n",
      "Loss epoch 36: 4.799085460603237, took 33.434706926345825s\n",
      "Epoch: 37\n",
      "Loss epoch 37: 4.7702941223979, took 33.45529890060425s\n",
      "Epoch: 38\n",
      "Loss epoch 38: 4.784399338066578, took 33.5619010925293s\n",
      "Epoch: 39\n",
      "Loss epoch 39: 4.801249802112579, took 33.54660773277283s\n",
      "Epoch: 40\n",
      "Loss epoch 40: 4.7823630049824715, took 33.51699924468994s\n",
      "Epoch: 41\n",
      "Loss epoch 41: 4.79029393196106, took 33.42579746246338s\n",
      "Epoch: 42\n",
      "Loss epoch 42: 4.769947990775108, took 33.65422511100769s\n",
      "Epoch: 43\n",
      "Loss epoch 43: 4.785345360636711, took 33.50857162475586s\n",
      "Epoch: 44\n",
      "Loss epoch 44: 4.795317612588406, took 33.51260232925415s\n",
      "Epoch: 45\n",
      "Loss epoch 45: 4.765336096286774, took 33.43772888183594s\n",
      "Epoch: 46\n",
      "Loss epoch 46: 4.808115452528, took 33.53950047492981s\n",
      "Epoch: 47\n",
      "Loss epoch 47: 4.759688191115856, took 33.516199827194214s\n",
      "Epoch: 48\n",
      "Loss epoch 48: 4.771660268306732, took 33.398881673812866s\n",
      "Epoch: 49\n",
      "Loss epoch 49: 4.792136810719967, took 33.58439874649048s\n",
      "Epoch: 50\n",
      "Loss epoch 50: 4.786190964281559, took 33.40211272239685s\n",
      "Epoch: 51\n",
      "Loss epoch 51: 4.770986132323742, took 33.47175621986389s\n",
      "Epoch: 52\n",
      "Loss epoch 52: 4.784706629812717, took 33.389025926589966s\n",
      "Epoch: 53\n",
      "Loss epoch 53: 4.766118437051773, took 33.49951910972595s\n",
      "Epoch: 54\n",
      "Loss epoch 54: 4.773177765309811, took 33.5191490650177s\n",
      "Epoch: 55\n",
      "Loss epoch 55: 4.784263424575329, took 33.46036386489868s\n",
      "Epoch: 56\n",
      "Loss epoch 56: 4.778958670794964, took 33.5952513217926s\n",
      "Epoch: 57\n",
      "Loss epoch 57: 4.780018351972103, took 33.54375457763672s\n",
      "Epoch: 58\n",
      "Loss epoch 58: 4.758224368095398, took 33.57199573516846s\n",
      "Epoch: 59\n",
      "Loss epoch 59: 4.767731599509716, took 33.38180947303772s\n",
      "Epoch: 60\n",
      "Loss epoch 60: 4.784030206501484, took 33.52803921699524s\n",
      "Epoch: 61\n",
      "Loss epoch 61: 4.754073575139046, took 33.47856616973877s\n",
      "Epoch: 62\n",
      "Loss epoch 62: 4.7731276005506516, took 33.4872727394104s\n",
      "Epoch: 63\n",
      "Loss epoch 63: 4.78802514821291, took 33.37408781051636s\n",
      "Epoch: 64\n",
      "Loss epoch 64: 4.749264784157276, took 33.608983755111694s\n",
      "Epoch: 65\n",
      "Loss epoch 65: 4.756907865405083, took 33.445611000061035s\n",
      "Epoch: 66\n",
      "Loss epoch 66: 4.7584923058748245, took 33.47885513305664s\n",
      "Epoch: 67\n",
      "Loss epoch 67: 4.848067782819271, took 33.57160472869873s\n",
      "Epoch: 68\n",
      "Loss epoch 68: 4.848970599472523, took 33.6333646774292s\n",
      "Epoch: 69\n",
      "Loss epoch 69: 4.756114810705185, took 33.492998123168945s\n",
      "Epoch: 70\n",
      "Loss epoch 70: 4.770858883857727, took 33.38160991668701s\n",
      "Epoch: 71\n",
      "Loss epoch 71: 4.7822108045220375, took 33.485187292099s\n",
      "Epoch: 72\n",
      "Loss epoch 72: 4.751070335507393, took 33.48683834075928s\n",
      "Epoch: 73\n",
      "Loss epoch 73: 4.7669311463832855, took 33.50957274436951s\n",
      "Epoch: 74\n",
      "Loss epoch 74: 4.7541955560445786, took 33.595852851867676s\n",
      "Epoch: 75\n",
      "Loss epoch 75: 4.764433652162552, took 33.363325119018555s\n",
      "Epoch: 76\n",
      "Loss epoch 76: 4.749331593513489, took 33.53978872299194s\n",
      "Epoch: 77\n",
      "Loss epoch 77: 4.765665724873543, took 33.411553144454956s\n",
      "Epoch: 78\n",
      "Loss epoch 78: 4.752252221107483, took 33.699700355529785s\n",
      "Epoch: 79\n",
      "Loss epoch 79: 4.780263401567936, took 33.61940121650696s\n",
      "Epoch: 80\n",
      "Loss epoch 80: 4.750472642481327, took 33.434688568115234s\n",
      "Epoch: 81\n",
      "Loss epoch 81: 4.781006142497063, took 33.52189016342163s\n",
      "Epoch: 82\n",
      "Loss epoch 82: 4.739910759031773, took 33.369049072265625s\n",
      "Epoch: 83\n",
      "Loss epoch 83: 4.75444883108139, took 33.54137659072876s\n",
      "Epoch: 84\n",
      "Loss epoch 84: 4.778853006660938, took 33.45301079750061s\n",
      "Epoch: 85\n",
      "Loss epoch 85: 4.737280867993832, took 33.62132143974304s\n",
      "Epoch: 86\n",
      "Loss epoch 86: 4.753370299935341, took 33.52799344062805s\n",
      "Epoch: 87\n",
      "Loss epoch 87: 4.743564426898956, took 33.51826333999634s\n",
      "Epoch: 88\n",
      "Loss epoch 88: 4.7508305311203, took 33.428706407547s\n",
      "Epoch: 89\n",
      "Loss epoch 89: 4.749180696904659, took 33.673871994018555s\n",
      "Epoch: 90\n",
      "Loss epoch 90: 4.770734526216984, took 33.588584184646606s\n",
      "Epoch: 91\n",
      "Loss epoch 91: 4.74054516851902, took 33.3723087310791s\n",
      "Epoch: 92\n",
      "Loss epoch 92: 4.745981402695179, took 33.687450885772705s\n",
      "Epoch: 93\n",
      "Loss epoch 93: 4.762801557779312, took 33.30684781074524s\n",
      "Epoch: 94\n",
      "Loss epoch 94: 4.735338039696217, took 33.539546251297s\n",
      "Epoch: 95\n",
      "Loss epoch 95: 4.767249196767807, took 33.43415021896362s\n",
      "Epoch: 96\n",
      "Loss epoch 96: 4.731078647077084, took 33.53321313858032s\n",
      "Epoch: 97\n",
      "Loss epoch 97: 4.742077261209488, took 33.45522379875183s\n",
      "Epoch: 98\n",
      "Loss epoch 98: 4.736015982925892, took 33.477720499038696s\n",
      "Epoch: 99\n",
      "Loss epoch 99: 4.740633487701416, took 33.549623250961304s\n",
      "Epoch: 100\n",
      "Loss epoch 100: 4.750870630145073, took 33.474050998687744s\n",
      "Epoch: 101\n",
      "Loss epoch 101: 4.7395922020077705, took 33.597827196121216s\n",
      "Epoch: 102\n",
      "Loss epoch 102: 4.745619982481003, took 33.392757177352905s\n",
      "Epoch: 103\n",
      "Loss epoch 103: 4.740570455789566, took 33.51736927032471s\n",
      "Epoch: 104\n",
      "Loss epoch 104: 4.766254998743534, took 33.46402645111084s\n",
      "Epoch: 105\n",
      "Loss epoch 105: 4.722311653196812, took 33.49915647506714s\n",
      "Epoch: 106\n",
      "Loss epoch 106: 4.730530686676502, took 33.487754344940186s\n",
      "Epoch: 107\n",
      "Loss epoch 107: 4.747696399688721, took 33.51201391220093s\n",
      "Epoch: 108\n",
      "Loss epoch 108: 4.731251239776611, took 33.48636174201965s\n",
      "Epoch: 109\n",
      "Loss epoch 109: 4.740604221820831, took 33.42632269859314s\n",
      "Epoch: 110\n",
      "Loss epoch 110: 4.7347521930933, took 33.5245418548584s\n",
      "Epoch: 111\n",
      "Loss epoch 111: 4.758633352816105, took 33.52245831489563s\n",
      "Epoch: 112\n",
      "Loss epoch 112: 4.725497528910637, took 33.4593780040741s\n",
      "Epoch: 113\n",
      "Loss epoch 113: 4.733647987246513, took 33.510287046432495s\n",
      "Epoch: 114\n",
      "Loss epoch 114: 4.7592932134866714, took 33.569666385650635s\n",
      "Epoch: 115\n",
      "Loss epoch 115: 4.718411706387997, took 33.518474102020264s\n",
      "Epoch: 116\n",
      "Loss epoch 116: 4.730684779584408, took 33.48426151275635s\n",
      "Epoch: 117\n",
      "Loss epoch 117: 4.715396769344807, took 33.618160009384155s\n",
      "Epoch: 118\n",
      "Loss epoch 118: 4.7657720521092415, took 33.73567342758179s\n",
      "Epoch: 119\n",
      "Loss epoch 119: 4.846991173923016, took 33.60606622695923s\n",
      "Epoch: 120\n",
      "Loss epoch 120: 4.7249152064323425, took 33.41884803771973s\n",
      "Epoch: 121\n",
      "Loss epoch 121: 4.739430248737335, took 33.58068537712097s\n",
      "Epoch: 122\n",
      "Loss epoch 122: 4.737559154629707, took 33.51595735549927s\n",
      "Epoch: 123\n",
      "Loss epoch 123: 4.727921903133392, took 33.48330116271973s\n",
      "Epoch: 124\n",
      "Loss epoch 124: 4.735067933797836, took 33.677734375s\n",
      "Epoch: 125\n",
      "Loss epoch 125: 4.7230508625507355, took 33.478633880615234s\n",
      "Epoch: 126\n",
      "Loss epoch 126: 4.731244549155235, took 33.548489570617676s\n",
      "Epoch: 127\n",
      "Loss epoch 127: 4.734827138483524, took 33.46822428703308s\n",
      "Epoch: 128\n",
      "Loss epoch 128: 4.72274111956358, took 33.570335388183594s\n",
      "Epoch: 129\n",
      "Loss epoch 129: 4.723969720304012, took 33.521881341934204s\n",
      "Epoch: 130\n",
      "Loss epoch 130: 4.758372075855732, took 33.56198263168335s\n",
      "Epoch: 131\n",
      "Loss epoch 131: 4.714054569602013, took 33.507936239242554s\n",
      "Epoch: 132\n",
      "Loss epoch 132: 4.74252762645483, took 33.58551025390625s\n",
      "Epoch: 133\n",
      "Loss epoch 133: 4.714660905301571, took 33.515056848526s\n",
      "Epoch: 134\n",
      "Loss epoch 134: 4.728630475699902, took 33.470168590545654s\n",
      "Epoch: 135\n",
      "Loss epoch 135: 4.756528042256832, took 33.63242316246033s\n",
      "Epoch: 136\n",
      "Loss epoch 136: 4.707796029746532, took 33.453614473342896s\n",
      "Epoch: 137\n",
      "Loss epoch 137: 4.7165218740701675, took 33.5564329624176s\n",
      "Epoch: 138\n",
      "Loss epoch 138: 4.72351148724556, took 33.4892897605896s\n",
      "Epoch: 139\n",
      "Loss epoch 139: 4.745928384363651, took 33.63458204269409s\n",
      "Epoch: 140\n",
      "Loss epoch 140: 4.705432057380676, took 33.45793294906616s\n",
      "Epoch: 141\n",
      "Loss epoch 141: 4.712445177137852, took 33.527838706970215s\n",
      "Epoch: 142\n",
      "Loss epoch 142: 4.720905661582947, took 33.61978769302368s\n",
      "Epoch: 143\n",
      "Loss epoch 143: 4.723068960011005, took 33.52980637550354s\n",
      "Epoch: 144\n",
      "Loss epoch 144: 4.711408697068691, took 33.61545014381409s\n",
      "Epoch: 145\n",
      "Loss epoch 145: 4.720520384609699, took 33.425296783447266s\n",
      "Epoch: 146\n",
      "Loss epoch 146: 4.704818852245808, took 33.60104203224182s\n",
      "Epoch: 147\n",
      "Loss epoch 147: 4.724051482975483, took 33.48644161224365s\n",
      "Epoch: 148\n",
      "Loss epoch 148: 4.708375781774521, took 33.56788396835327s\n",
      "Epoch: 149\n",
      "Loss epoch 149: 4.712379939854145, took 33.67069888114929s\n",
      "Epoch: 150\n",
      "Loss epoch 150: 4.711905166506767, took 33.249496936798096s\n",
      "Epoch: 151\n",
      "Loss epoch 151: 4.747696071863174, took 33.67473888397217s\n",
      "Epoch: 152\n",
      "Loss epoch 152: 4.6992471888661385, took 33.428467988967896s\n",
      "Epoch: 153\n",
      "Loss epoch 153: 4.707259848713875, took 33.69509935379028s\n",
      "Epoch: 154\n",
      "Loss epoch 154: 4.71315910667181, took 33.59558963775635s\n",
      "Epoch: 155\n",
      "Loss epoch 155: 4.707882426679134, took 33.526832580566406s\n",
      "Epoch: 156\n",
      "Loss epoch 156: 4.73753760010004, took 33.48609519004822s\n",
      "Epoch: 157\n",
      "Loss epoch 157: 4.696556098759174, took 33.74330115318298s\n",
      "Epoch: 158\n",
      "Loss epoch 158: 4.698697246611118, took 33.57238459587097s\n",
      "Epoch: 159\n",
      "Loss epoch 159: 4.7299840450286865, took 33.50389266014099s\n",
      "Epoch: 160\n",
      "Loss epoch 160: 4.702997013926506, took 33.79055738449097s\n",
      "Epoch: 161\n",
      "Loss epoch 161: 4.715326368808746, took 33.46420860290527s\n",
      "Epoch: 162\n",
      "Loss epoch 162: 4.722483314573765, took 33.493475914001465s\n",
      "Epoch: 163\n",
      "Loss epoch 163: 4.6969631388783455, took 33.47977638244629s\n",
      "Epoch: 164\n",
      "Loss epoch 164: 4.705956779420376, took 33.617544174194336s\n",
      "Epoch: 165\n",
      "Loss epoch 165: 4.715522423386574, took 33.48277139663696s\n",
      "Epoch: 166\n",
      "Loss epoch 166: 4.699845105409622, took 33.70181894302368s\n",
      "Epoch: 167\n",
      "Loss epoch 167: 4.699843995273113, took 33.61977529525757s\n",
      "Epoch: 168\n",
      "Loss epoch 168: 4.7313098311424255, took 33.601420879364014s\n",
      "Epoch: 169\n",
      "Loss epoch 169: 4.7216066643595695, took 33.77592396736145s\n",
      "Epoch: 170\n",
      "Loss epoch 170: 4.692295528948307, took 33.688767433166504s\n",
      "Epoch: 171\n",
      "Loss epoch 171: 4.699281774461269, took 33.51831841468811s\n",
      "Epoch: 172\n",
      "Loss epoch 172: 4.732251882553101, took 33.50039744377136s\n",
      "Epoch: 173\n",
      "Loss epoch 173: 4.691703140735626, took 33.61245107650757s\n",
      "Epoch: 174\n",
      "Loss epoch 174: 4.696405239403248, took 33.531450033187866s\n",
      "Epoch: 175\n",
      "Loss epoch 175: 4.7079815194010735, took 33.68458127975464s\n",
      "Epoch: 176\n",
      "Loss epoch 176: 4.692983567714691, took 33.70335340499878s\n",
      "Epoch: 177\n",
      "Loss epoch 177: 4.703884772956371, took 33.444881439208984s\n",
      "Epoch: 178\n",
      "Loss epoch 178: 4.686189845204353, took 33.768653869628906s\n",
      "Epoch: 179\n",
      "Loss epoch 179: 4.7102808728814125, took 33.362443685531616s\n",
      "Epoch: 180\n",
      "Loss epoch 180: 4.691771455109119, took 33.595375061035156s\n",
      "Epoch: 181\n",
      "Loss epoch 181: 4.708666682243347, took 33.43438649177551s\n",
      "Epoch: 182\n",
      "Loss epoch 182: 4.7005845457315445, took 33.71945667266846s\n",
      "Epoch: 183\n",
      "Loss epoch 183: 4.695892877876759, took 33.63261818885803s\n",
      "Epoch: 184\n",
      "Loss epoch 184: 4.692400336265564, took 33.50656795501709s\n",
      "Epoch: 185\n",
      "Loss epoch 185: 4.698489472270012, took 33.76167893409729s\n",
      "Epoch: 186\n",
      "Loss epoch 186: 4.6888628378510475, took 33.54099631309509s\n",
      "Epoch: 187\n",
      "Loss epoch 187: 4.700514815747738, took 33.56407356262207s\n",
      "Epoch: 188\n",
      "Loss epoch 188: 4.694854363799095, took 33.388251543045044s\n",
      "Epoch: 189\n",
      "Loss epoch 189: 4.690715290606022, took 33.77385377883911s\n",
      "Epoch: 190\n",
      "Loss epoch 190: 4.691211700439453, took 33.44521164894104s\n",
      "Epoch: 191\n",
      "Loss epoch 191: 4.702269949018955, took 33.69791007041931s\n",
      "Epoch: 192\n",
      "Loss epoch 192: 4.6871581226587296, took 33.56969380378723s\n",
      "Epoch: 193\n",
      "Loss epoch 193: 4.724266454577446, took 33.53791856765747s\n",
      "Epoch: 194\n",
      "Loss epoch 194: 4.677906006574631, took 33.681068420410156s\n",
      "Epoch: 195\n",
      "Loss epoch 195: 4.695515923202038, took 33.53050899505615s\n",
      "Epoch: 196\n",
      "Loss epoch 196: 4.6831074655056, took 33.73931550979614s\n",
      "Epoch: 197\n",
      "Loss epoch 197: 4.727127179503441, took 33.424415826797485s\n",
      "Epoch: 198\n",
      "Loss epoch 198: 4.679226025938988, took 33.62944316864014s\n",
      "Epoch: 199\n",
      "Loss epoch 199: 4.684451937675476, took 33.65105628967285s\n",
      "Epoch: 200\n",
      "Loss epoch 200: 4.683624669909477, took 33.67579245567322s\n",
      "Epoch: 201\n",
      "Loss epoch 201: 4.720521196722984, took 33.6942834854126s\n",
      "Epoch: 202\n",
      "Loss epoch 202: 4.677464358508587, took 33.47525382041931s\n",
      "Epoch: 203\n",
      "Loss epoch 203: 4.689237035810947, took 33.73979353904724s\n",
      "Epoch: 204\n",
      "Loss epoch 204: 4.704226158559322, took 33.506205558776855s\n",
      "Epoch: 205\n",
      "Loss epoch 205: 4.677794821560383, took 33.5298957824707s\n",
      "Epoch: 206\n",
      "Loss epoch 206: 4.68803258985281, took 33.46730971336365s\n",
      "Epoch: 207\n",
      "Loss epoch 207: 4.683273017406464, took 33.78379249572754s\n",
      "Epoch: 208\n",
      "Loss epoch 208: 4.686040975153446, took 33.55468034744263s\n",
      "Epoch: 209\n",
      "Loss epoch 209: 4.681524120271206, took 33.48646903038025s\n",
      "Epoch: 210\n",
      "Loss epoch 210: 4.691256709396839, took 33.63715124130249s\n",
      "Epoch: 211\n",
      "Loss epoch 211: 4.678638249635696, took 33.4949996471405s\n",
      "Epoch: 212\n",
      "Loss epoch 212: 4.709779679775238, took 33.677196979522705s\n",
      "Epoch: 213\n",
      "Loss epoch 213: 4.684374682605267, took 33.517754554748535s\n",
      "Epoch: 214\n",
      "Loss epoch 214: 4.677696123719215, took 33.767937660217285s\n",
      "Epoch: 215\n",
      "Loss epoch 215: 4.709752947092056, took 33.6479287147522s\n",
      "Epoch: 216\n",
      "Loss epoch 216: 4.673065587878227, took 34.50129199028015s\n",
      "Epoch: 217\n",
      "Loss epoch 217: 4.675559386610985, took 33.7091920375824s\n",
      "Epoch: 218\n",
      "Loss epoch 218: 4.678851947188377, took 33.70872378349304s\n",
      "Epoch: 219\n",
      "Loss epoch 219: 4.675933942198753, took 33.73560881614685s\n",
      "Epoch: 220\n",
      "Loss epoch 220: 4.707978017628193, took 33.570317029953s\n",
      "Epoch: 221\n",
      "Loss epoch 221: 4.671313591301441, took 33.81902623176575s\n",
      "Epoch: 222\n",
      "Loss epoch 222: 4.675369203090668, took 33.684558391571045s\n",
      "Epoch: 223\n",
      "Loss epoch 223: 4.707818642258644, took 33.57743549346924s\n",
      "Epoch: 224\n",
      "Loss epoch 224: 4.6692086681723595, took 33.807058572769165s\n",
      "Epoch: 225\n",
      "Loss epoch 225: 4.699845403432846, took 33.81243968009949s\n",
      "Epoch: 226\n",
      "Loss epoch 226: 4.659550115466118, took 33.69618034362793s\n",
      "Epoch: 227\n",
      "Loss epoch 227: 4.833786528557539, took 33.64990568161011s\n",
      "Epoch: 228\n",
      "Loss epoch 228: 4.826843366026878, took 33.94008159637451s\n",
      "Epoch: 229\n",
      "Loss epoch 229: 4.68083792924881, took 33.4955677986145s\n",
      "Epoch: 230\n",
      "Loss epoch 230: 4.686620317399502, took 33.68132019042969s\n",
      "Epoch: 231\n",
      "Loss epoch 231: 4.683975212275982, took 33.562851667404175s\n",
      "Epoch: 232\n",
      "Loss epoch 232: 4.681187696754932, took 33.84278869628906s\n",
      "Epoch: 233\n",
      "Loss epoch 233: 4.6799046993255615, took 33.60624599456787s\n",
      "Epoch: 234\n",
      "Loss epoch 234: 4.683307610452175, took 33.55557370185852s\n",
      "Epoch: 235\n",
      "Loss epoch 235: 4.677972927689552, took 33.69893217086792s\n",
      "Epoch: 236\n",
      "Loss epoch 236: 4.674767851829529, took 33.44642448425293s\n",
      "Epoch: 237\n",
      "Loss epoch 237: 4.700284473598003, took 33.599225997924805s\n",
      "Epoch: 238\n",
      "Loss epoch 238: 4.671237766742706, took 33.551703691482544s\n",
      "Epoch: 239\n",
      "Loss epoch 239: 4.7003356367349625, took 33.88763403892517s\n",
      "Epoch: 240\n",
      "Loss epoch 240: 4.668371930718422, took 33.526540756225586s\n",
      "Epoch: 241\n",
      "Loss epoch 241: 4.675186984241009, took 33.61853384971619s\n",
      "Epoch: 242\n",
      "Loss epoch 242: 4.674590289592743, took 33.81714367866516s\n",
      "Epoch: 243\n",
      "Loss epoch 243: 4.673089362680912, took 33.63719463348389s\n",
      "Epoch: 244\n",
      "Loss epoch 244: 4.681434318423271, took 33.625346183776855s\n",
      "Epoch: 245\n",
      "Loss epoch 245: 4.671157367527485, took 33.55234432220459s\n",
      "Epoch: 246\n",
      "Loss epoch 246: 4.7026578187942505, took 33.624070167541504s\n",
      "Epoch: 247\n",
      "Loss epoch 247: 4.667082756757736, took 33.589128732681274s\n",
      "Epoch: 248\n",
      "Loss epoch 248: 4.668227843940258, took 33.5520122051239s\n",
      "Epoch: 249\n",
      "Loss epoch 249: 4.682187333703041, took 33.54252648353577s\n",
      "Epoch: 250\n",
      "Loss epoch 250: 4.666469909250736, took 33.780465841293335s\n",
      "Epoch: 251\n",
      "Loss epoch 251: 4.680775433778763, took 33.6092414855957s\n",
      "Epoch: 252\n",
      "Loss epoch 252: 4.667946450412273, took 33.569968938827515s\n",
      "Epoch: 253\n",
      "Loss epoch 253: 4.671168312430382, took 33.883105516433716s\n",
      "Epoch: 254\n",
      "Loss epoch 254: 4.670406818389893, took 33.604620933532715s\n",
      "Epoch: 255\n",
      "Loss epoch 255: 4.699241757392883, took 33.6003623008728s\n",
      "Epoch: 256\n",
      "Loss epoch 256: 4.661245249211788, took 33.608779191970825s\n",
      "Epoch: 257\n",
      "Loss epoch 257: 4.6685773357748985, took 33.75703167915344s\n",
      "Epoch: 258\n",
      "Loss epoch 258: 4.664998732507229, took 33.70544123649597s\n",
      "Epoch: 259\n",
      "Loss epoch 259: 4.669696651399136, took 33.691229581832886s\n",
      "Epoch: 260\n",
      "Loss epoch 260: 4.667837768793106, took 33.7584125995636s\n",
      "Epoch: 261\n",
      "Loss epoch 261: 4.680152118206024, took 33.52854108810425s\n",
      "Epoch: 262\n",
      "Loss epoch 262: 4.665865927934647, took 33.698798179626465s\n",
      "Epoch: 263\n",
      "Loss epoch 263: 4.695599906146526, took 33.568702936172485s\n",
      "Epoch: 264\n",
      "Loss epoch 264: 4.656579650938511, took 33.659374952316284s\n",
      "Epoch: 265\n",
      "Loss epoch 265: 4.665394529700279, took 33.73216509819031s\n",
      "Epoch: 266\n",
      "Loss epoch 266: 4.664321236312389, took 33.59251093864441s\n",
      "Epoch: 267\n",
      "Loss epoch 267: 4.664147891104221, took 33.77882218360901s\n",
      "Epoch: 268\n",
      "Loss epoch 268: 4.668503597378731, took 33.56835460662842s\n",
      "Epoch: 269\n",
      "Loss epoch 269: 4.660321362316608, took 33.66831135749817s\n",
      "Epoch: 270\n",
      "Loss epoch 270: 4.657737269997597, took 33.53071641921997s\n",
      "Epoch: 271\n",
      "Loss epoch 271: 4.688812464475632, took 33.61231231689453s\n",
      "Epoch: 272\n",
      "Loss epoch 272: 4.658391289412975, took 33.623695611953735s\n",
      "Epoch: 273\n",
      "Loss epoch 273: 4.66066736727953, took 33.706968784332275s\n",
      "Epoch: 274\n",
      "Loss epoch 274: 4.66060995310545, took 33.59317135810852s\n",
      "Epoch: 275\n",
      "Loss epoch 275: 4.65874545276165, took 33.66884088516235s\n",
      "Epoch: 276\n",
      "Loss epoch 276: 4.683727473020554, took 33.674840688705444s\n",
      "Epoch: 277\n",
      "Loss epoch 277: 4.654806822538376, took 33.49771547317505s\n",
      "Epoch: 278\n",
      "Loss epoch 278: 4.654993616044521, took 33.81446862220764s\n",
      "Epoch: 279\n",
      "Loss epoch 279: 4.662960812449455, took 33.57237982749939s\n",
      "Epoch: 280\n",
      "Loss epoch 280: 4.656978830695152, took 33.62160587310791s\n",
      "Epoch: 281\n",
      "Loss epoch 281: 4.664295069873333, took 33.52878260612488s\n",
      "Epoch: 282\n",
      "Loss epoch 282: 4.659650966525078, took 33.89653754234314s\n",
      "Epoch: 283\n",
      "Loss epoch 283: 4.653210513293743, took 33.71429514884949s\n",
      "Epoch: 284\n",
      "Loss epoch 284: 4.684244811534882, took 33.50519275665283s\n",
      "Epoch: 285\n",
      "Loss epoch 285: 4.65471027046442, took 33.85440158843994s\n",
      "Epoch: 286\n",
      "Loss epoch 286: 4.657684318721294, took 33.533820390701294s\n",
      "Epoch: 287\n",
      "Loss epoch 287: 4.6565659791231155, took 33.69527840614319s\n",
      "Epoch: 288\n",
      "Loss epoch 288: 4.6642040610313416, took 33.560490131378174s\n",
      "Epoch: 289\n",
      "Loss epoch 289: 4.651343390345573, took 33.758912563323975s\n",
      "Epoch: 290\n",
      "Loss epoch 290: 4.654030591249466, took 33.576215982437134s\n",
      "Epoch: 291\n",
      "Loss epoch 291: 4.660855375230312, took 33.67146277427673s\n",
      "Epoch: 292\n",
      "Loss epoch 292: 4.654181405901909, took 33.76415801048279s\n",
      "Epoch: 293\n",
      "Loss epoch 293: 4.6596571654081345, took 33.53520202636719s\n",
      "Epoch: 294\n",
      "Loss epoch 294: 4.655420050024986, took 33.60647916793823s\n",
      "Epoch: 295\n",
      "Loss epoch 295: 4.648597456514835, took 33.64694547653198s\n",
      "Epoch: 296\n",
      "Loss epoch 296: 4.655042864382267, took 33.64655351638794s\n",
      "Epoch: 297\n",
      "Loss epoch 297: 4.652552604675293, took 33.55908799171448s\n",
      "Epoch: 298\n",
      "Loss epoch 298: 4.658973477780819, took 33.60131549835205s\n",
      "Epoch: 299\n",
      "Loss epoch 299: 4.648307606577873, took 33.629369258880615s\n",
      "Epoch: 300\n",
      "Loss epoch 300: 4.653866298496723, took 33.616679668426514s\n",
      "Epoch: 301\n",
      "Loss epoch 301: 4.671577945351601, took 33.59071373939514s\n",
      "Epoch: 302\n",
      "Loss epoch 302: 4.644528850913048, took 33.580974102020264s\n",
      "Epoch: 303\n",
      "Loss epoch 303: 4.655747912824154, took 33.655869245529175s\n",
      "Epoch: 304\n",
      "Loss epoch 304: 4.6488854587078094, took 33.797178745269775s\n",
      "Epoch: 305\n",
      "Loss epoch 305: 4.650526136159897, took 33.758899211883545s\n",
      "Epoch: 306\n",
      "Loss epoch 306: 4.653109408915043, took 33.52118420600891s\n",
      "Epoch: 307\n",
      "Loss epoch 307: 4.6557507291436195, took 33.69227623939514s\n",
      "Epoch: 308\n",
      "Loss epoch 308: 4.65155291557312, took 35.01193904876709s\n",
      "Epoch: 309\n",
      "Loss epoch 309: 4.649840414524078, took 33.61218619346619s\n",
      "Epoch: 310\n",
      "Loss epoch 310: 4.646841451525688, took 33.84067249298096s\n",
      "Epoch: 311\n",
      "Loss epoch 311: 4.653858534991741, took 33.5002977848053s\n",
      "Epoch: 312\n",
      "Loss epoch 312: 4.646946452558041, took 33.66262745857239s\n",
      "Epoch: 313\n",
      "Loss epoch 313: 4.652165897190571, took 33.567787408828735s\n",
      "Epoch: 314\n",
      "Loss epoch 314: 4.642742767930031, took 33.76156187057495s\n",
      "Epoch: 315\n",
      "Loss epoch 315: 4.66834606975317, took 33.59087872505188s\n",
      "Epoch: 316\n",
      "Loss epoch 316: 4.644105538725853, took 33.6849467754364s\n",
      "Epoch: 317\n",
      "Loss epoch 317: 4.643748439848423, took 33.73294019699097s\n",
      "Epoch: 318\n",
      "Loss epoch 318: 4.645048044621944, took 33.722697734832764s\n",
      "Epoch: 319\n",
      "Loss epoch 319: 4.642678573727608, took 33.65796208381653s\n",
      "Epoch: 320\n",
      "Loss epoch 320: 4.643120534718037, took 33.5457866191864s\n",
      "Epoch: 321\n",
      "Loss epoch 321: 4.6742696687579155, took 33.722010374069214s\n",
      "Epoch: 322\n",
      "Loss epoch 322: 4.648242689669132, took 33.5868775844574s\n",
      "Epoch: 323\n",
      "Loss epoch 323: 4.6454169526696205, took 33.76197123527527s\n",
      "Epoch: 324\n",
      "Loss epoch 324: 4.6564838364720345, took 33.591965675354004s\n",
      "Epoch: 325\n",
      "Loss epoch 325: 4.645324237644672, took 33.60480499267578s\n",
      "Epoch: 326\n",
      "Loss epoch 326: 4.646355390548706, took 33.65853238105774s\n",
      "Epoch: 327\n",
      "Loss epoch 327: 4.645772606134415, took 33.625014543533325s\n",
      "Epoch: 328\n",
      "Loss epoch 328: 4.686512343585491, took 33.65923833847046s\n",
      "Epoch: 329\n",
      "Loss epoch 329: 4.6396230310201645, took 33.617563247680664s\n",
      "Epoch: 330\n",
      "Loss epoch 330: 4.643830716609955, took 33.654600620269775s\n",
      "Epoch: 331\n",
      "Loss epoch 331: 4.642059072852135, took 33.51901865005493s\n",
      "Epoch: 332\n",
      "Loss epoch 332: 4.648763872683048, took 33.664085388183594s\n",
      "Epoch: 333\n",
      "Loss epoch 333: 4.643285825848579, took 33.64903211593628s\n",
      "Epoch: 334\n",
      "Loss epoch 334: 4.653979271650314, took 33.55128526687622s\n",
      "Epoch: 335\n",
      "Loss epoch 335: 4.639508038759232, took 33.768104791641235s\n",
      "Epoch: 336\n",
      "Loss epoch 336: 4.638541288673878, took 33.58353567123413s\n",
      "Epoch: 337\n",
      "Loss epoch 337: 4.652358368039131, took 33.70191216468811s\n",
      "Epoch: 338\n",
      "Loss epoch 338: 4.655867867171764, took 33.58600640296936s\n",
      "Epoch: 339\n",
      "Loss epoch 339: 4.65160359442234, took 33.61426496505737s\n",
      "Epoch: 340\n",
      "Loss epoch 340: 4.649305298924446, took 33.69754958152771s\n",
      "Epoch: 341\n",
      "Loss epoch 341: 4.647458381950855, took 33.57907176017761s\n",
      "Epoch: 342\n",
      "Loss epoch 342: 4.6459000036120415, took 33.752506494522095s\n",
      "Epoch: 343\n",
      "Loss epoch 343: 4.644448101520538, took 33.569730281829834s\n",
      "Epoch: 344\n",
      "Loss epoch 344: 4.646613955497742, took 33.61563539505005s\n",
      "Epoch: 345\n",
      "Loss epoch 345: 4.641496479511261, took 33.67362141609192s\n",
      "Epoch: 346\n",
      "Loss epoch 346: 4.640941724181175, took 33.685566663742065s\n",
      "Epoch: 347\n",
      "Loss epoch 347: 4.6666538417339325, took 33.61209321022034s\n",
      "Epoch: 348\n",
      "Loss epoch 348: 4.63638037443161, took 33.62998151779175s\n",
      "Epoch: 349\n",
      "Loss epoch 349: 4.637342236936092, took 33.85180854797363s\n",
      "Epoch: 350\n",
      "Loss epoch 350: 4.635181620717049, took 33.63210940361023s\n",
      "Epoch: 351\n",
      "Loss epoch 351: 4.642433591187, took 33.7652850151062s\n",
      "Epoch: 352\n",
      "Loss epoch 352: 4.638105891644955, took 33.60665488243103s\n",
      "Epoch: 353\n",
      "Loss epoch 353: 4.662107437849045, took 33.85501313209534s\n",
      "Epoch: 354\n",
      "Loss epoch 354: 4.634361281991005, took 33.69426941871643s\n",
      "Epoch: 355\n",
      "Loss epoch 355: 4.635766394436359, took 33.54280471801758s\n",
      "Epoch: 356\n",
      "Loss epoch 356: 4.64038610458374, took 33.54702043533325s\n",
      "Epoch: 357\n",
      "Loss epoch 357: 4.641268916428089, took 33.549352169036865s\n",
      "Epoch: 358\n",
      "Loss epoch 358: 4.645796798169613, took 33.699339628219604s\n",
      "Epoch: 359\n",
      "Loss epoch 359: 4.63453234732151, took 33.572829723358154s\n",
      "Epoch: 360\n",
      "Loss epoch 360: 4.632419593632221, took 33.86880970001221s\n",
      "Epoch: 361\n",
      "Loss epoch 361: 4.641785517334938, took 33.57134413719177s\n",
      "Epoch: 362\n",
      "Loss epoch 362: 4.6358044520020485, took 33.51716876029968s\n",
      "Epoch: 363\n",
      "Loss epoch 363: 4.636331722140312, took 33.596620082855225s\n",
      "Epoch: 364\n",
      "Loss epoch 364: 4.6364877969026566, took 33.754433155059814s\n",
      "Epoch: 365\n",
      "Loss epoch 365: 4.6326310858130455, took 33.789570808410645s\n",
      "Epoch: 366\n",
      "Loss epoch 366: 4.637057572603226, took 33.52766489982605s\n",
      "Epoch: 367\n",
      "Loss epoch 367: 4.634970985352993, took 33.65663528442383s\n",
      "Epoch: 368\n",
      "Loss epoch 368: 4.6392766535282135, took 33.47282004356384s\n",
      "Epoch: 369\n",
      "Loss epoch 369: 4.634402714669704, took 33.56670689582825s\n",
      "Epoch: 370\n",
      "Loss epoch 370: 4.644213438034058, took 33.57733130455017s\n",
      "Epoch: 371\n",
      "Loss epoch 371: 4.632235452532768, took 33.649925231933594s\n",
      "Epoch: 372\n",
      "Loss epoch 372: 4.634740598499775, took 33.559406042099s\n",
      "Epoch: 373\n",
      "Loss epoch 373: 4.636976346373558, took 33.72977638244629s\n",
      "Epoch: 374\n",
      "Loss epoch 374: 4.632616125047207, took 33.758774757385254s\n",
      "Epoch: 375\n",
      "Loss epoch 375: 4.638088203966618, took 33.59500765800476s\n",
      "Epoch: 376\n",
      "Loss epoch 376: 4.632773116230965, took 33.81821918487549s\n",
      "Epoch: 377\n",
      "Loss epoch 377: 4.644089929759502, took 33.66321611404419s\n",
      "Epoch: 378\n",
      "Loss epoch 378: 4.6331853196024895, took 33.65272402763367s\n",
      "Epoch: 379\n",
      "Loss epoch 379: 4.631659425795078, took 33.55111241340637s\n",
      "Epoch: 380\n",
      "Loss epoch 380: 4.634058207273483, took 33.695406913757324s\n",
      "Epoch: 381\n",
      "Loss epoch 381: 4.632076069712639, took 33.67850041389465s\n",
      "Epoch: 382\n",
      "Loss epoch 382: 4.629437245428562, took 33.71774411201477s\n",
      "Epoch: 383\n",
      "Loss epoch 383: 4.631348975002766, took 33.744181394577026s\n",
      "Epoch: 384\n",
      "Loss epoch 384: 4.634019821882248, took 33.58818984031677s\n",
      "Epoch: 385\n",
      "Loss epoch 385: 4.631340675055981, took 33.721060037612915s\n",
      "Epoch: 386\n",
      "Loss epoch 386: 4.598707400262356, took 33.48789191246033s\n",
      "Epoch: 387\n",
      "Loss epoch 387: 5.782747428864241, took 33.66774320602417s\n",
      "Epoch: 388\n",
      "Loss epoch 388: 4.833122558891773, took 33.62877535820007s\n",
      "Epoch: 389\n",
      "Loss epoch 389: 4.671580500900745, took 33.71114802360535s\n",
      "Epoch: 390\n",
      "Loss epoch 390: 4.632758788764477, took 33.726319789886475s\n",
      "Epoch: 391\n",
      "Loss epoch 391: 4.638869732618332, took 33.47123408317566s\n",
      "Epoch: 392\n",
      "Loss epoch 392: 4.634374342858791, took 33.87622570991516s\n",
      "Epoch: 393\n",
      "Loss epoch 393: 4.634806752204895, took 33.59457087516785s\n",
      "Epoch: 394\n",
      "Loss epoch 394: 4.633163183927536, took 33.588568449020386s\n",
      "Epoch: 395\n",
      "Loss epoch 395: 4.636307664215565, took 33.56378483772278s\n",
      "Epoch: 396\n",
      "Loss epoch 396: 4.63594976067543, took 33.64906907081604s\n",
      "Epoch: 397\n",
      "Loss epoch 397: 4.635366164147854, took 33.602824687957764s\n",
      "Epoch: 398\n",
      "Loss epoch 398: 4.634559042751789, took 33.61870217323303s\n",
      "Epoch: 399\n",
      "Loss epoch 399: 4.634404890239239, took 33.57813858985901s\n",
      "Epoch: 400\n",
      "Loss epoch 400: 4.631562270224094, took 33.57627487182617s\n",
      "Epoch: 401\n",
      "Loss epoch 401: 4.631221242249012, took 33.649659633636475s\n",
      "Epoch: 402\n",
      "Loss epoch 402: 4.6303506270051, took 33.594597816467285s\n",
      "Epoch: 403\n",
      "Loss epoch 403: 4.632780246436596, took 33.74022054672241s\n",
      "Epoch: 404\n",
      "Loss epoch 404: 4.631246529519558, took 33.585270166397095s\n",
      "Epoch: 405\n",
      "Loss epoch 405: 4.634752728044987, took 33.66355586051941s\n",
      "Epoch: 406\n",
      "Loss epoch 406: 4.629463821649551, took 33.67624545097351s\n",
      "Epoch: 407\n",
      "Loss epoch 407: 4.62837478518486, took 33.52289891242981s\n",
      "Epoch: 408\n",
      "Loss epoch 408: 4.630391970276833, took 33.68197536468506s\n",
      "Epoch: 409\n",
      "Loss epoch 409: 4.6289918422698975, took 33.54359269142151s\n",
      "Epoch: 410\n",
      "Loss epoch 410: 4.630362935364246, took 33.69700217247009s\n",
      "Epoch: 411\n",
      "Loss epoch 411: 4.626599475741386, took 33.53784251213074s\n",
      "Epoch: 412\n",
      "Loss epoch 412: 4.627299956977367, took 33.62504863739014s\n",
      "Epoch: 413\n",
      "Loss epoch 413: 4.626554526388645, took 33.5516881942749s\n",
      "Epoch: 414\n",
      "Loss epoch 414: 4.625524163246155, took 33.77015662193298s\n",
      "Epoch: 415\n",
      "Loss epoch 415: 4.628764875233173, took 33.72367548942566s\n",
      "Epoch: 416\n",
      "Loss epoch 416: 4.64809063822031, took 33.5790650844574s\n",
      "Epoch: 417\n",
      "Loss epoch 417: 4.622823290526867, took 33.88540720939636s\n",
      "Epoch: 418\n",
      "Loss epoch 418: 4.626022256910801, took 33.646785736083984s\n",
      "Epoch: 419\n",
      "Loss epoch 419: 4.628642998635769, took 33.59997844696045s\n",
      "Epoch: 420\n",
      "Loss epoch 420: 4.62581741809845, took 33.548455238342285s\n",
      "Epoch: 421\n",
      "Loss epoch 421: 4.627754561603069, took 33.832191944122314s\n",
      "Epoch: 422\n",
      "Loss epoch 422: 4.626437582075596, took 33.595664501190186s\n",
      "Epoch: 423\n",
      "Loss epoch 423: 4.626596704125404, took 33.532002687454224s\n",
      "Epoch: 424\n",
      "Loss epoch 424: 4.619952075183392, took 33.83411359786987s\n",
      "Epoch: 425\n",
      "Loss epoch 425: 4.627180494368076, took 33.563393115997314s\n",
      "Epoch: 426\n",
      "Loss epoch 426: 4.626251958310604, took 33.63836598396301s\n",
      "Epoch: 427\n",
      "Loss epoch 427: 4.625408045947552, took 33.632163524627686s\n",
      "Epoch: 428\n",
      "Loss epoch 428: 4.624273017048836, took 33.728190183639526s\n",
      "Epoch: 429\n",
      "Loss epoch 429: 4.624686650931835, took 33.55492806434631s\n",
      "Epoch: 430\n",
      "Loss epoch 430: 4.624126702547073, took 33.6535861492157s\n",
      "Epoch: 431\n",
      "Loss epoch 431: 4.622690342366695, took 33.768617391586304s\n",
      "Epoch: 432\n",
      "Loss epoch 432: 4.624805316329002, took 33.63618278503418s\n",
      "Epoch: 433\n",
      "Loss epoch 433: 4.623673342168331, took 33.80332589149475s\n",
      "Epoch: 434\n",
      "Loss epoch 434: 4.623982839286327, took 33.60979986190796s\n",
      "Epoch: 435\n",
      "Loss epoch 435: 4.624232344329357, took 33.743096590042114s\n",
      "Epoch: 436\n",
      "Loss epoch 436: 4.621891923248768, took 33.593156814575195s\n",
      "Epoch: 437\n",
      "Loss epoch 437: 4.6465988382697105, took 33.617201805114746s\n",
      "Epoch: 438\n",
      "Loss epoch 438: 4.6204561442136765, took 33.67343497276306s\n",
      "Epoch: 439\n",
      "Loss epoch 439: 4.6212621703743935, took 33.72163677215576s\n",
      "Epoch: 440\n",
      "Loss epoch 440: 4.621532030403614, took 33.863202810287476s\n",
      "Epoch: 441\n",
      "Loss epoch 441: 4.624901212751865, took 33.67343735694885s\n",
      "Epoch: 442\n",
      "Loss epoch 442: 4.620163917541504, took 33.729588985443115s\n",
      "Epoch: 443\n",
      "Loss epoch 443: 4.6227149069309235, took 33.55018997192383s\n",
      "Epoch: 444\n",
      "Loss epoch 444: 4.618947364389896, took 33.71492671966553s\n",
      "Epoch: 445\n",
      "Loss epoch 445: 4.622284486889839, took 33.57171297073364s\n",
      "Epoch: 446\n",
      "Loss epoch 446: 4.617913611233234, took 33.69066500663757s\n",
      "Epoch: 447\n",
      "Loss epoch 447: 4.617703482508659, took 33.67535352706909s\n",
      "Epoch: 448\n",
      "Loss epoch 448: 4.618592642247677, took 33.5280065536499s\n",
      "Epoch: 449\n",
      "Loss epoch 449: 4.615832053124905, took 33.84984874725342s\n",
      "Epoch: 450\n",
      "Loss epoch 450: 4.620756536722183, took 33.63199472427368s\n",
      "Epoch: 451\n",
      "Loss epoch 451: 4.615156203508377, took 33.83275890350342s\n",
      "Epoch: 452\n",
      "Loss epoch 452: 4.6195776760578156, took 33.6277220249176s\n",
      "Epoch: 453\n",
      "Loss epoch 453: 4.617791190743446, took 33.746365547180176s\n",
      "Epoch: 454\n",
      "Loss epoch 454: 4.614201627671719, took 33.654786348342896s\n",
      "Epoch: 455\n",
      "Loss epoch 455: 4.659289568662643, took 33.618494749069214s\n",
      "Epoch: 456\n",
      "Loss epoch 456: 4.614810429513454, took 33.66705656051636s\n",
      "Epoch: 457\n",
      "Loss epoch 457: 4.617342092096806, took 33.51460862159729s\n",
      "Epoch: 458\n",
      "Loss epoch 458: 4.6185427233576775, took 33.71251821517944s\n",
      "Epoch: 459\n",
      "Loss epoch 459: 4.6177820935845375, took 33.63046216964722s\n",
      "Epoch: 460\n",
      "Loss epoch 460: 4.614560015499592, took 33.666919231414795s\n",
      "Epoch: 461\n",
      "Loss epoch 461: 4.618316002190113, took 33.62991666793823s\n",
      "Epoch: 462\n",
      "Loss epoch 462: 4.615532614290714, took 33.84376621246338s\n",
      "Epoch: 463\n",
      "Loss epoch 463: 4.61556101590395, took 33.63060784339905s\n",
      "Epoch: 464\n",
      "Loss epoch 464: 4.616034746170044, took 33.72004961967468s\n",
      "Epoch: 465\n",
      "Loss epoch 465: 4.614077977836132, took 33.68648290634155s\n",
      "Epoch: 466\n",
      "Loss epoch 466: 4.616242855787277, took 33.57943391799927s\n",
      "Epoch: 467\n",
      "Loss epoch 467: 4.61450932174921, took 33.88251543045044s\n",
      "Epoch: 468\n",
      "Loss epoch 468: 4.615258999168873, took 33.53395438194275s\n",
      "Epoch: 469\n",
      "Loss epoch 469: 4.6314392760396, took 33.66846299171448s\n",
      "Epoch: 470\n",
      "Loss epoch 470: 4.6120109632611275, took 33.64322328567505s\n",
      "Epoch: 471\n",
      "Loss epoch 471: 4.613682344555855, took 33.755449533462524s\n",
      "Epoch: 472\n",
      "Loss epoch 472: 4.612987078726292, took 33.66458868980408s\n",
      "Epoch: 473\n",
      "Loss epoch 473: 4.614600352942944, took 33.520222187042236s\n",
      "Epoch: 474\n",
      "Loss epoch 474: 4.610751010477543, took 33.73071312904358s\n",
      "Epoch: 475\n",
      "Loss epoch 475: 4.613860011100769, took 33.54916596412659s\n",
      "Epoch: 476\n",
      "Loss epoch 476: 4.611663065850735, took 33.651597023010254s\n",
      "Epoch: 477\n",
      "Loss epoch 477: 4.617483898997307, took 33.57854628562927s\n",
      "Epoch: 478\n",
      "Loss epoch 478: 4.611548580229282, took 33.73802161216736s\n",
      "Epoch: 479\n",
      "Loss epoch 479: 4.615184508264065, took 33.68775749206543s\n",
      "Epoch: 480\n",
      "Loss epoch 480: 4.612458847463131, took 33.5931601524353s\n",
      "Epoch: 481\n",
      "Loss epoch 481: 4.6186569184064865, took 33.66230010986328s\n",
      "Epoch: 482\n",
      "Loss epoch 482: 4.609035484492779, took 33.676443338394165s\n",
      "Epoch: 483\n",
      "Loss epoch 483: 4.61631153523922, took 33.679218769073486s\n",
      "Epoch: 484\n",
      "Loss epoch 484: 4.611289322376251, took 33.47129249572754s\n",
      "Epoch: 485\n",
      "Loss epoch 485: 4.611377574503422, took 33.70220494270325s\n",
      "Epoch: 486\n",
      "Loss epoch 486: 4.606621779501438, took 33.56925415992737s\n",
      "Epoch: 487\n",
      "Loss epoch 487: 4.615750156342983, took 33.709338426589966s\n",
      "Epoch: 488\n",
      "Loss epoch 488: 4.610784813761711, took 33.81855010986328s\n",
      "Epoch: 489\n",
      "Loss epoch 489: 4.611656531691551, took 33.65633225440979s\n",
      "Epoch: 490\n",
      "Loss epoch 490: 4.6121944934129715, took 33.74818992614746s\n",
      "Epoch: 491\n",
      "Loss epoch 491: 4.611159108579159, took 33.55164933204651s\n",
      "Epoch: 492\n",
      "Loss epoch 492: 4.613598085939884, took 33.7111291885376s\n",
      "Epoch: 493\n",
      "Loss epoch 493: 4.611364372074604, took 33.54238200187683s\n",
      "Epoch: 494\n",
      "Loss epoch 494: 4.611396498978138, took 33.548492431640625s\n",
      "Epoch: 495\n",
      "Loss epoch 495: 4.638075985014439, took 33.54156231880188s\n",
      "Epoch: 496\n",
      "Loss epoch 496: 4.608668386936188, took 33.911574602127075s\n",
      "Epoch: 497\n",
      "Loss epoch 497: 4.609351523220539, took 33.713622093200684s\n",
      "Epoch: 498\n",
      "Loss epoch 498: 4.607557445764542, took 33.481969594955444s\n",
      "Epoch: 499\n",
      "Loss epoch 499: 4.613430559635162, took 33.70493268966675s\n",
      "Epoch: 500\n",
      "Loss epoch 500: 4.60757040977478, took 33.65073204040527s\n",
      "Epoch: 501\n",
      "Loss epoch 501: 4.609551452100277, took 33.63568329811096s\n",
      "Epoch: 502\n",
      "Loss epoch 502: 4.608546681702137, took 33.52477669715881s\n",
      "Epoch: 503\n",
      "Loss epoch 503: 4.606258355081081, took 33.733845472335815s\n",
      "Epoch: 504\n",
      "Loss epoch 504: 4.606074646115303, took 33.59762787818909s\n",
      "Epoch: 505\n",
      "Loss epoch 505: 4.608112245798111, took 33.533262491226196s\n",
      "Epoch: 506\n",
      "Loss epoch 506: 4.607880011200905, took 33.62160897254944s\n",
      "Epoch: 507\n",
      "Loss epoch 507: 4.608268439769745, took 33.703709840774536s\n",
      "Epoch: 508\n",
      "Loss epoch 508: 4.610011242330074, took 33.6379919052124s\n",
      "Epoch: 509\n",
      "Loss epoch 509: 4.606601618230343, took 33.586305379867554s\n",
      "Epoch: 510\n",
      "Loss epoch 510: 4.608138367533684, took 33.82799530029297s\n",
      "Epoch: 511\n",
      "Loss epoch 511: 4.606605611741543, took 33.5333411693573s\n",
      "Epoch: 512\n",
      "Loss epoch 512: 4.608141183853149, took 33.55988693237305s\n",
      "Epoch: 513\n",
      "Loss epoch 513: 4.605314336717129, took 33.71652960777283s\n",
      "Epoch: 514\n",
      "Loss epoch 514: 4.616427384316921, took 33.604552268981934s\n",
      "Epoch: 515\n",
      "Loss epoch 515: 4.6059137135744095, took 33.779276847839355s\n",
      "Epoch: 516\n",
      "Loss epoch 516: 4.606696635484695, took 33.54981064796448s\n",
      "Epoch: 517\n",
      "Loss epoch 517: 4.605264976620674, took 33.79825568199158s\n",
      "Epoch: 518\n",
      "Loss epoch 518: 4.603672757744789, took 33.63191246986389s\n",
      "Epoch: 519\n",
      "Loss epoch 519: 4.647736668586731, took 33.64049506187439s\n",
      "Epoch: 520\n",
      "Loss epoch 520: 4.614636108279228, took 33.64761567115784s\n",
      "Epoch: 521\n",
      "Loss epoch 521: 4.616817861795425, took 33.64478254318237s\n",
      "Epoch: 522\n",
      "Loss epoch 522: 4.61190502345562, took 33.66924738883972s\n",
      "Epoch: 523\n",
      "Loss epoch 523: 4.610579080879688, took 33.65902900695801s\n",
      "Epoch: 524\n",
      "Loss epoch 524: 4.606560096144676, took 33.757118701934814s\n",
      "Epoch: 525\n",
      "Loss epoch 525: 4.6133973225951195, took 33.57302641868591s\n",
      "Epoch: 526\n",
      "Loss epoch 526: 4.60388358682394, took 33.65107083320618s\n",
      "Epoch: 527\n",
      "Loss epoch 527: 4.606312148272991, took 33.61151695251465s\n",
      "Epoch: 528\n",
      "Loss epoch 528: 4.60889245569706, took 33.876771211624146s\n",
      "Epoch: 529\n",
      "Loss epoch 529: 4.6179205402731895, took 33.71777582168579s\n",
      "Epoch: 530\n",
      "Loss epoch 530: 4.606981247663498, took 33.53758406639099s\n",
      "Epoch: 531\n",
      "Loss epoch 531: 4.606697201728821, took 33.82330918312073s\n",
      "Epoch: 532\n",
      "Loss epoch 532: 4.606680430471897, took 33.61719560623169s\n",
      "Epoch: 533\n",
      "Loss epoch 533: 4.606516994535923, took 33.71268367767334s\n",
      "Epoch: 534\n",
      "Loss epoch 534: 4.602626666426659, took 33.49137020111084s\n",
      "Epoch: 535\n",
      "Loss epoch 535: 4.6088961735367775, took 33.65535545349121s\n",
      "Epoch: 536\n",
      "Loss epoch 536: 4.600224919617176, took 33.61620473861694s\n",
      "Epoch: 537\n",
      "Loss epoch 537: 4.601562544703484, took 33.84400820732117s\n",
      "Epoch: 538\n",
      "Loss epoch 538: 4.605346404016018, took 33.68740391731262s\n",
      "Epoch: 539\n",
      "Loss epoch 539: 4.603132218122482, took 33.54417109489441s\n",
      "Epoch: 540\n",
      "Loss epoch 540: 4.627276577055454, took 33.61380577087402s\n",
      "Epoch: 541\n",
      "Loss epoch 541: 4.601995185017586, took 33.565813302993774s\n",
      "Epoch: 542\n",
      "Loss epoch 542: 4.601981081068516, took 33.721697092056274s\n",
      "Epoch: 543\n",
      "Loss epoch 543: 4.595992214977741, took 33.47598385810852s\n",
      "Epoch: 544\n",
      "Loss epoch 544: 4.613692931830883, took 33.581411361694336s\n",
      "Epoch: 545\n",
      "Loss epoch 545: 4.598721340298653, took 33.6324360370636s\n",
      "Epoch: 546\n",
      "Loss epoch 546: 4.599746912717819, took 33.759477853775024s\n",
      "Epoch: 547\n",
      "Loss epoch 547: 4.601258501410484, took 33.697999477386475s\n",
      "Epoch: 548\n",
      "Loss epoch 548: 4.600573129951954, took 33.58432340621948s\n",
      "Epoch: 549\n",
      "Loss epoch 549: 4.600090838968754, took 33.674153566360474s\n",
      "Epoch: 550\n",
      "Loss epoch 550: 4.610076196491718, took 33.62632179260254s\n",
      "Epoch: 551\n",
      "Loss epoch 551: 4.5996580719947815, took 33.56480598449707s\n",
      "Epoch: 552\n",
      "Loss epoch 552: 4.602394379675388, took 33.66870093345642s\n",
      "Epoch: 553\n",
      "Loss epoch 553: 4.601587630808353, took 33.73590111732483s\n",
      "Epoch: 554\n",
      "Loss epoch 554: 4.594435974955559, took 33.61882758140564s\n",
      "Epoch: 555\n",
      "Loss epoch 555: 4.60915470123291, took 33.491783618927s\n",
      "Epoch: 556\n",
      "Loss epoch 556: 4.605347514152527, took 33.65686917304993s\n",
      "Epoch: 557\n",
      "Loss epoch 557: 4.60333626717329, took 33.57966089248657s\n",
      "Epoch: 558\n",
      "Loss epoch 558: 4.604922644793987, took 33.65880012512207s\n",
      "Epoch: 559\n",
      "Loss epoch 559: 4.6058990359306335, took 33.62367510795593s\n",
      "Epoch: 560\n",
      "Loss epoch 560: 4.6027428060770035, took 33.70425224304199s\n",
      "Epoch: 561\n",
      "Loss epoch 561: 4.60682525485754, took 33.52583312988281s\n",
      "Epoch: 562\n",
      "Loss epoch 562: 4.5999419912695885, took 33.56084680557251s\n",
      "Epoch: 563\n",
      "Loss epoch 563: 4.600364789366722, took 33.58257985115051s\n",
      "Epoch: 564\n",
      "Loss epoch 564: 4.6014557629823685, took 33.53456926345825s\n",
      "Epoch: 565\n",
      "Loss epoch 565: 4.603726543486118, took 33.6533682346344s\n",
      "Epoch: 566\n",
      "Loss epoch 566: 4.599676303565502, took 33.53857207298279s\n",
      "Epoch: 567\n",
      "Loss epoch 567: 4.6023463904857635, took 33.77546310424805s\n",
      "Epoch: 568\n",
      "Loss epoch 568: 4.598838306963444, took 33.57238960266113s\n",
      "Epoch: 569\n",
      "Loss epoch 569: 4.599235504865646, took 33.800020694732666s\n",
      "Epoch: 570\n",
      "Loss epoch 570: 4.6006481647491455, took 33.686033725738525s\n",
      "Epoch: 571\n",
      "Loss epoch 571: 4.598510406911373, took 33.68161177635193s\n",
      "Epoch: 572\n",
      "Loss epoch 572: 4.59853682667017, took 33.615437746047974s\n",
      "Epoch: 573\n",
      "Loss epoch 573: 4.600526690483093, took 33.49921941757202s\n",
      "Epoch: 574\n",
      "Loss epoch 574: 4.597409054636955, took 33.78839564323425s\n",
      "Epoch: 575\n",
      "Loss epoch 575: 4.59770842641592, took 33.5449640750885s\n",
      "Epoch: 576\n",
      "Loss epoch 576: 4.6345518082380295, took 33.603694915771484s\n",
      "Epoch: 577\n",
      "Loss epoch 577: 4.594759248197079, took 33.59139943122864s\n",
      "Epoch: 578\n",
      "Loss epoch 578: 4.596542619168758, took 33.67395782470703s\n",
      "Epoch: 579\n",
      "Loss epoch 579: 4.597625456750393, took 33.607218980789185s\n",
      "Epoch: 580\n",
      "Loss epoch 580: 4.597788959741592, took 33.58384442329407s\n",
      "Epoch: 581\n",
      "Loss epoch 581: 4.597435534000397, took 33.75460457801819s\n",
      "Epoch: 582\n",
      "Loss epoch 582: 4.5970025435090065, took 33.80440306663513s\n",
      "Epoch: 583\n",
      "Loss epoch 583: 4.596577323973179, took 33.652310609817505s\n",
      "Epoch: 584\n",
      "Loss epoch 584: 4.595624044537544, took 33.51765251159668s\n",
      "Epoch: 585\n",
      "Loss epoch 585: 4.596701011061668, took 33.843055725097656s\n",
      "Epoch: 586\n",
      "Loss epoch 586: 4.5953285694122314, took 33.511712074279785s\n",
      "Epoch: 587\n",
      "Loss epoch 587: 4.5950184017419815, took 33.623788833618164s\n",
      "Epoch: 588\n",
      "Loss epoch 588: 4.594848029315472, took 33.64135551452637s\n",
      "Epoch: 589\n",
      "Loss epoch 589: 4.639347575604916, took 33.578208684921265s\n",
      "Epoch: 590\n",
      "Loss epoch 590: 4.591416165232658, took 33.82333827018738s\n",
      "Epoch: 591\n",
      "Loss epoch 591: 4.592367209494114, took 33.54987287521362s\n",
      "Epoch: 592\n",
      "Loss epoch 592: 4.594076335430145, took 33.87369155883789s\n",
      "Epoch: 593\n",
      "Loss epoch 593: 4.594399265944958, took 33.50835394859314s\n",
      "Epoch: 594\n",
      "Loss epoch 594: 4.5940821170806885, took 33.63939714431763s\n",
      "Epoch: 595\n",
      "Loss epoch 595: 4.593616999685764, took 33.766801834106445s\n",
      "Epoch: 596\n",
      "Loss epoch 596: 4.593372710049152, took 33.732722997665405s\n",
      "Epoch: 597\n",
      "Loss epoch 597: 4.592598035931587, took 33.763004779815674s\n",
      "Epoch: 598\n",
      "Loss epoch 598: 4.596112243831158, took 33.53443765640259s\n",
      "Epoch: 599\n",
      "Loss epoch 599: 4.591194197535515, took 33.547120332717896s\n",
      "...Training finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NR_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        outputs = test_net(data_in)\n",
    "        loss = criterion(outputs[:,:,SLICE_FROM_TIMESTEP:TIMESTEPS].float(),labels[:,SLICE_FROM_TIMESTEP:TIMESTEPS].long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "    print(\"Loss epoch {}: {}, took {}s\".format(epoch, running_loss,time.time()-start_time))\n",
    "    loss_array.append(running_loss)\n",
    "    learning_rate_array.append(learning_rate)\n",
    "\n",
    "#test_all_preds(test_net) \n",
    "print('...Training finished')\n",
    "# plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed\n"
     ]
    }
   ],
   "source": [
    "#DYNAMIC CHANGES:\n",
    "NR_EPOCHS = 600\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "print(\"LR changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-aa035d741967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#EVALUATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected loss:{}, last loss:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sequence length:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTIMESTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total epochs learnt:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_array' is not defined"
     ]
    }
   ],
   "source": [
    "#EVALUATION\n",
    "print(\"Expected loss:{}, last loss:{}\".format((len(dataset)-split)*np.log(3),loss_array[-1]))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Sequence length:\",TIMESTEPS)\n",
    "print(\"Total epochs learnt:\", len(loss_array))\n",
    "# plt.plot(learning_rate_array)\n",
    "# plt.plot(loss_array)\n",
    "test_all_preds(test_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5728752990c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m   3264\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3266\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m     \u001b[0;31m# This method is the one actually exporting the required methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0mmanagers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmanagers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mmanagers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected loss for untrained set with Cross Entropy:\n",
    "k = number of classes\n",
    "N = number of labeled data in dataset\n",
    "loss_per_prediction = -log(1/k) = log(k)\n",
    "total_loss = sum(log(k)) = N*log(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/matthias/Desktop/flows_L7.149_timesteps10_nrsequences30_batch1.eps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-22ae3e2f77d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".eps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, frameon, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2076\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36mprint_eps\u001b[0;34m(self, outfile, *args, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_ps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     def _print_ps(self, outfile, format, *args,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36m_print_ps\u001b[0;34m(self, outfile, format, papertype, dpi, facecolor, edgecolor, orientation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             self._print_figure(outfile, format, dpi, facecolor, edgecolor,\n\u001b[1;32m    949\u001b[0m                                \u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misLandscape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpapertype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m                                **kwargs)\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m     def _print_figure(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_ps.py\u001b[0m in \u001b[0;36m_print_figure\u001b[0;34m(self, outfile, format, dpi, facecolor, edgecolor, orientation, isLandscape, papertype, metadata, dryrun, bbox_inches_restore, **kwargs)\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mprint_figure_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m                     \u001b[0mprint_figure_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/matthias/Desktop/flows_L7.149_timesteps10_nrsequences30_batch1.eps'"
     ]
    }
   ],
   "source": [
    "plt.clf()\n",
    "plot_path = \"/home/matthias/Desktop/\"\n",
    "loss_str = \"{0:.3f}\".format(loss_array[-1])\n",
    "information = \"flows_L\" + loss_str + \"_timesteps{}_nrsequences{}_batch{}\".format(TIMESTEPS,dataset_size,BATCH_SIZE)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Cross Entropy)')\n",
    "plt.plot(loss_array)\n",
    "plt.title(information)\n",
    "plt.savefig(plot_path + information + \".eps\")\n",
    "plt.clf()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.plot(learning_rate_array)\n",
    "plt.title(information)\n",
    "plt.savefig(plot_path + information + \"LR.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.size())\n",
    "print(outputs[:,:,5:10])\n",
    "print(outputs.size()[2])\n",
    "print(\"Loss half:\", criterion(outputs[:,:,SLICE_FROM_TIMESTEP:TIMESTEPS].float(),labels[:,SLICE_FROM_TIMESTEP:TIMESTEPS].long()))\n",
    "print(\"Loss full:\", criterion(outputs.float(),labels.long()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "List of channels created\n"
     ]
    }
   ],
   "source": [
    "image_list_ch_0 = []\n",
    "image_list_ch_1 = []\n",
    "image_list_ch_2 = []\n",
    "for a,data in enumerate(dataset):\n",
    "    #print(a)\n",
    "    for image in data[0]['flows']:        \n",
    "        image_list_ch_0.append(image[0].numpy())\n",
    "        image_list_ch_1.append(image[1].numpy())\n",
    "        image_list_ch_2.append(image[2].numpy())\n",
    "print(\"List of channels created\")\n",
    "mean = [np.mean(image_list_ch_0),np.mean(image_list_ch_1),np.mean(image_list_ch_2)]\n",
    "std_dev = np.sqrt([np.var(image_list_ch_0),np.var(image_list_ch_1),np.var(image_list_ch_2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126.1972, 127.50477, 1.3415707]\n",
      "[6.436958  1.7359118 5.022056 ]\n",
      "[6.436958, 1.7359118, 5.022056]\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "print(std_dev)\n",
    "\n",
    "std_dev2 = [np.std(image_list_ch_0),np.std(image_list_ch_1),np.std(image_list_ch_2)]\n",
    "print(std_dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
