{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. flows_1\n",
      "Hyperparameters defined\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. flows_1\")\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2 \n",
    "IMAGE_INPUT_SIZE_W = 640\n",
    "IMAGE_INPUT_SIZE_H = 480\n",
    "\n",
    "IMAGE_AFTER_CONV_SIZE_W = 18\n",
    "IMAGE_AFTER_CONV_SIZE_H = 13\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_IO_SIZE = 18*13\n",
    "LSTM_HIDDEN_SIZE = 18*13\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 10 # size videos\n",
    "BATCH_SIZE = 1 #until now just batch_size = 1\n",
    "SLICE_FROM_TIMESTEP = 1 #slices from timestep SLICE_FROM_TIMESTEP to the last one\n",
    "\n",
    "NR_EPOCHS = 20\n",
    "\n",
    "VALIDATION_SPLIT = 0.0 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "USE_EXISTING_NORMALIZATION = True\n",
    "\n",
    "learning_rate = 0.01 # reduce factos of 10 .. some epoch later.\n",
    "momentum = 0.9\n",
    "print(\"Hyperparameters defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  flows_1\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "crop_im_size = [186,250]\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "c.config['transformers']['DimensionResize']['dimension'] = TIMESTEPS\n",
    "#c.config['indexing']['people selection'] = [1]\n",
    "#c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "c.config['pose']['load'] = False\n",
    "c.config['flow']['load'] = True\n",
    "c.config['heatmaps']['load'] = False\n",
    "#c.config['scenes']['sequences'] = ['nm']\n",
    "#c.config['scenes']['angles'] = ['108']\n",
    "c.config['dataset_output'] = {\n",
    "#         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "        'data': ['flows'],\n",
    "        'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings \n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 30\n",
      "Indices size: 30\n",
      "Split: 0\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "mean = [126.04878374565972, 127.50835754123264, 1.5171344184027777]\n",
    "std_dev = [7.03813454, 2.04901181, 5.66295848]\n",
    "if not(USE_EXISTING_NORMALIZATION):\n",
    "    image_list_ch_0 = []\n",
    "    image_list_ch_1 = []\n",
    "    image_list_ch_2 = []\n",
    "    for data in dataset:\n",
    "    #     print()\n",
    "        for image in data[0]['flows']:\n",
    "            image_list_ch_0.append(image[0].numpy())\n",
    "            image_list_ch_1.append(image[1].numpy())\n",
    "            image_list_ch_2.append(image[2].numpy())\n",
    "    print(\"List of channels created\")\n",
    "    mean = [np.mean(image_list_ch_0),np.mean(image_list_ch_1),np.mean(image_list_ch_2)]\n",
    "    std_dev = np.sqrt([np.var(image_list_ch_0),np.var(image_list_ch_1),np.var(image_list_ch_2)])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# print(\"Data in original\", len(data_in))\n",
    "# data_in_norm = data_in\n",
    "# for i,data in enumerate(data_in):\n",
    "#     for image in data:\n",
    "#         ch_0 = (image[0][0] - mean[0]) / std_dev[0]\n",
    "#         ch_1 = (image[0][1] - mean[1]) / std_dev[1]\n",
    "#         ch_2 = (image[0][2] - mean[2]) / std_dev[2]\n",
    "#         print(ch_3)\n",
    "# #         channel_list = [ch_0, ch_1, ch_2]\n",
    "# #         data_in_norm.append(channel_list)\n",
    "  \n",
    "# print(\"New\",data_in_norm)\n",
    "# print(\"Original\",data_in)\n",
    "# dataset_new = dataset\n",
    "# print(\"Normalizing images... DOES NOT WORK LIKE THIS\")\n",
    "# for i, data in enumerate(dataset):\n",
    "#     print(\"Dataset item nr \", i)\n",
    "#     print(\"Iterating through\", len(data[0]['flows']), \"images\")\n",
    "#     for j,image in enumerate(data[0]['flows']):\n",
    "#         image[0] = (image[0] - mean[0]) / std_dev[0]\n",
    "#         image[1] = (image[1] - mean[1]) / std_dev[1]\n",
    "#         image[2] = (image[2] - mean[2]) / std_dev[2]\n",
    "#         print(\"Before\", dataset_new[i][0]['flows'][j])\n",
    "#         dataset_new[i][0]['flows'][j] = image\n",
    "#         print(\"Image\",image)\n",
    "#         print(\"From dataset\",dataset_new[i][0]['flows'][j])\n",
    "    \n",
    "#         print(\"Dataset image\",dataset_new[i][0]['flows'][0])\n",
    "# print(\"Images normalized with mean and std dev\",mean, std_dev)\n",
    "# print(\"Display normalized image as array:\")\n",
    "# print(image[1])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined\n"
     ]
    }
   ],
   "source": [
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.avialable_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #input 640x480\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #input 638x478 output 319x239\n",
    "        self.conv2 = nn.Conv2d(6,16,3) # input 319x239 output 317x237\n",
    "        self.pool2 = nn.MaxPool2d(2,2) # input 317x237 output 158x118\n",
    "        self.conv3 = nn.Conv2d(16,6,3) # input 158x118 output 156x116\n",
    "        self.pool3 = nn.MaxPool2d(2,2) # input 156x116 output 78x58\n",
    "        self.conv4 = nn.Conv2d(6,3,3)  # input 78x58 output 76x56\n",
    "        self.pool4 = nn.MaxPool2d(2,2) # input 76x56 output 39x29\n",
    "        self.conv5 = nn.Conv2d(3,1,3)  # input 39x29 output 37x27\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #output 37x27 output 18x13\n",
    "        self.lstm1 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# before: TIMESTEPS. But TIMESTEPS should be wrong\n",
    "#         self.lstm2 = nn.LSTM(LSTM_IO_SIZE,\n",
    "#                             LSTM_HIDDEN_SIZE,\n",
    "#                             TIMESTEPS)# before: TIMESTEPS. But TIMESTEPS should be wrong\n",
    "        self.fc1 = nn.Linear(LSTM_IO_SIZE,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of LSTM\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        #print(\"Hidden:\", _hidden)\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device),#before: TIMESTEPS \n",
    "                torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device))#before: TIMESTEPS\n",
    "    def forward(self,x):\n",
    "#         print(\"Input list len:\",len(x))\n",
    "#         print(\"Input elemens size:\", x[0].size())\n",
    "#         batch_size = x[0].size()[0]\n",
    "\n",
    "        x_arr = torch.zeros(TIMESTEPS,BATCH_SIZE,1,IMAGE_AFTER_CONV_SIZE_H,IMAGE_AFTER_CONV_SIZE_W).to(self.avialable_device)\n",
    "        #print(\"X arr size\", x_arr.size())\n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i])))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = x_tmp_c5\n",
    "        \n",
    "        x = torch.cat((x_arr[0],\n",
    "              x_arr[1],\n",
    "              x_arr[2],\n",
    "              x_arr[3],\n",
    "              x_arr[4],\n",
    "              x_arr[5],\n",
    "              x_arr[6],\n",
    "              x_arr[7],\n",
    "              x_arr[8],\n",
    "              x_arr[9]),0)\n",
    "        x, hidden = self.lstm1(x_arr.view(TIMESTEPS,BATCH_SIZE,-1), self.hidden)#before: TIMESTEPS\n",
    "#         x, hidden = self.lstm2(x, self.hidden)\n",
    "        # the reshaping was taken from the documentation... and makes scense\n",
    "        x = x.view(TIMESTEPS,BATCH_SIZE,LSTM_HIDDEN_SIZE) #output.view(seq_len, batch, num_dir*hidden_size)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        #x = x.squeeze(1)\n",
    "        x = x.permute(1,2,0)\n",
    "        #print (\"Size network output\", x.shape)\n",
    "        return x\n",
    "print(\"Class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition done\n"
     ]
    }
   ],
   "source": [
    "### define and execute testing function\n",
    "def test_all_preds(model):\n",
    "    n_batches_test = len(test_loader)\n",
    "\n",
    "    #Time for printing\n",
    "    testing_start_time = time.time()\n",
    "\n",
    "    print('Start testing...')\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            data_in = [s.to(device) for s in inputs['flows']]\n",
    "            labels = labels.to(device)\n",
    "            if not labels.size()[0] == BATCH_SIZE:\n",
    "                # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "                continue\n",
    "            outputs = model(data_in)\n",
    "#             print(\"Out:\", len(outputs), outputs.size())\n",
    "#             print(\"Labels:\", len(labels), labels.size())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "            n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "            total += predicted.numel()\n",
    "            # print('predicted',predicted)\n",
    "            correct += predicted.numel() - n_errors\n",
    "            # print('labels',labels)\n",
    "    print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "    print('...testing finished')\n",
    "print(\"Definition done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set and evaluate computing time etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Expected loss with 3 different classes and 30 data elements: 32.95836866004329\n",
      "Batch size: 1\n",
      "Evaluating first element...\n",
      "Proof for normalized data: tensor([[[ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.1352,  0.2772,  0.2772],\n",
      "         ...,\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772],\n",
      "         [ 0.2772,  0.2772,  0.2772,  ...,  0.2772,  0.2772,  0.2772]],\n",
      "\n",
      "        [[-0.2481,  0.2399,  0.2399,  ...,  0.2399,  0.2399,  0.2399],\n",
      "         [-0.2481,  0.2399,  0.2399,  ...,  0.2399,  0.2399,  0.2399],\n",
      "         [-0.2481,  0.2399, -0.2481,  ...,  0.2399,  0.2399,  0.2399],\n",
      "         ...,\n",
      "         [ 0.2399,  0.2399, -0.2481,  ..., -0.2481, -0.2481, -0.2481],\n",
      "         [-0.2481,  0.2399,  0.2399,  ..., -0.2481, -0.2481, -0.2481],\n",
      "         [ 0.2399,  0.2399,  0.2399,  ..., -0.2481, -0.2481, -0.2481]],\n",
      "\n",
      "        [[-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         ...,\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679],\n",
      "         [-0.2679, -0.2679, -0.2679,  ..., -0.2679, -0.2679, -0.2679]]])\n",
      "Time steps:10, input sequence length:10\n",
      "Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\n",
      "Output format: 1 torch.Size([1, 3, 10])\n",
      "Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\n",
      "Labels: 1 torch.Size([1, 10])\n",
      "Slicing loss. Using loss from: 1 to 10\n",
      "Loss:1.0743273496627808, expected loss:1.0986122886681098\n",
      "Time needed:7.966966152191162s\n",
      "Expected loss for total training data:  32.95836866004329\n",
      "Expected training time per epoch:3.983483076095581 min\n",
      "Estimated total training time:1.3278276920318604 hours\n"
     ]
    }
   ],
   "source": [
    "#PREPARATION FOR TRAINING\n",
    "loss_array = []\n",
    "learning_rate_array = []\n",
    "\n",
    "print('Start training...')\n",
    "print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "running_loss = 0.0\n",
    "#print(\"Data set length:\", len((train_loader)), \"Validation length:\", len(test_loader))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Evaluating first element...\")\n",
    "start_time = time.time()\n",
    "i, batch = next(iter(enumerate(train_loader)))\n",
    "inputs, labels = batch\n",
    "data_in = [s.to(device) for s in inputs['flows']]\n",
    "# print(\"Data in original\", data_in)\n",
    "# data_in_norm = data_in\n",
    "# for i,image in enumerate(data_in):\n",
    "#     print(\"Normalizing:\",i)\n",
    "# #     data_in_norm[i][1] = data[i][1]\n",
    "#     print()\n",
    "#     data_in_norm[0] = (image[0] - mean[0]) / std_dev[0]\n",
    "#     data_in_norm[1] = (image[1] - mean[1]) / std_dev[1]\n",
    "#     data_in_norm[2] = (image[2] - mean[2]) / std_dev[2]\n",
    "# print(data_in_norm[0])\n",
    "# print(\"Normalization done\")\n",
    "\n",
    "print(\"Proof for normalized data:\", data_in[0][0])\n",
    "labels = labels.to(device)\n",
    "print(\"Time steps:{}, input sequence length:{}\".format(TIMESTEPS,len(data_in)))\n",
    "#print(\"NN input: \",len(flows),len(flows[0]),len(flows[0][0]),len(flows[0][0][0]),len(flows[0][0][0][0]))\n",
    "optimizer.zero_grad() \n",
    "outputs = test_net(data_in)\n",
    "print(\"Expected output format: [BATCH, NR_CLASSES, TIMESTEPS]\")\n",
    "print(\"Output format:\", len(outputs), outputs.size())\n",
    "print(\"Expected label format: [BATCH, TIMESTEPS] (with int-label as each element indicating the correct one)\")\n",
    "print(\"Labels:\", len(labels), labels.size())\n",
    "print(\"Slicing loss. Using loss from:\",SLICE_FROM_TIMESTEP,\"to\",TIMESTEPS)\n",
    "#print(\"Labels content:\", labels)\n",
    "#original: loss = criterion(outputs.float(),labels.long())\n",
    "loss = criterion(outputs[:,:,SLICE_FROM_TIMESTEP:TIMESTEPS].float(),labels[:,SLICE_FROM_TIMESTEP:TIMESTEPS].long())\n",
    "loss.backward() \n",
    "optimizer.step()\n",
    "\n",
    "running_loss += loss.data.item()\n",
    "elapsed_time = time.time() - start_time;\n",
    "loss_array.append(running_loss)\n",
    "learning_rate_array.append(learning_rate)\n",
    "print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "print(\"Time needed:{}s\".format(elapsed_time))\n",
    "print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)/60))\n",
    "print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*NR_EPOCHS/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss epoch 0: 27.582461774349213, took 197.53722286224365s\n",
      "Epoch: 1\n",
      "Loss epoch 1: 21.61893004179001, took 216.55900311470032s\n",
      "Epoch: 2\n",
      "Loss epoch 2: 18.67709046602249, took 207.80381608009338s\n",
      "Epoch: 3\n",
      "Loss epoch 3: 14.041937828063965, took 195.19779586791992s\n",
      "Epoch: 4\n",
      "Loss epoch 4: 9.068625509738922, took 211.29272270202637s\n",
      "Epoch: 5\n",
      "Loss epoch 5: 7.7634753957390785, took 215.84182214736938s\n",
      "Epoch: 6\n",
      "Loss epoch 6: 7.86685935407877, took 195.20355701446533s\n",
      "Epoch: 7\n",
      "Loss epoch 7: 10.797937907278538, took 195.1576509475708s\n",
      "Epoch: 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a9b691fb337d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdata_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flows'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gait_project/gait_analysis/DataSets/CasiaDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'flows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_indices'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0min_frame_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flows'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'heatmaps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_indices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_frame_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Loading scene images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36m_load_flow\u001b[0;34m(self, dataset_item)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m#im = im.astype('uint8')#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mscene_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscene_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m#im = im.astype('uint8')#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mscene_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscene_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gait_project/gait_analysis/DataSets/FlowsCasia.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(im_file)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m#im = cv2.astype('uint8')#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m#im = cv2.cvtColor(im.astype('uint8'), cv2.COLOR_BGR2RGB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;31m#im = im.astype('uint8')#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NR_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        outputs = test_net(data_in)\n",
    "        loss = criterion(outputs[:,:,SLICE_FROM_TIMESTEP:TIMESTEPS].float(),labels[:,SLICE_FROM_TIMESTEP:TIMESTEPS].long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "    print(\"Loss epoch {}: {}, took {}s\".format(epoch, running_loss,time.time()-start_time))\n",
    "    loss_array.append(running_loss)\n",
    "    learning_rate_array.append(learning_rate)\n",
    "\n",
    "#test_all_preds(test_net) \n",
    "print('...Training finished')\n",
    "# plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DYNAMIC CHANGES:\n",
    "NR_EPOCHS = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "print(\"LR changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "print(\"Expected loss:{}, last loss:{}\".format((len(dataset)-split)*np.log(3),loss_array[-1]))\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Sequence length:\",TIMESTEPS)\n",
    "print(\"Total epochs learnt:\", len(loss_array))\n",
    "# plt.plot(learning_rate_array)\n",
    "# plt.plot(loss_array)\n",
    "test_all_preds(test_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected loss for untrained set with Cross Entropy:\n",
    "k = number of classes\n",
    "N = number of labeled data in dataset\n",
    "loss_per_prediction = -log(1/k) = log(k)\n",
    "total_loss = sum(log(k)) = N*log(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plot_path = \"/home/matthias/Desktop/\"\n",
    "loss_str = \"{0:.3f}\".format(loss_array[-1])\n",
    "information = \"flows_L\" + loss_str + \"_timesteps{}_nrsequences{}_batch{}\".format(TIMESTEPS,dataset_size,BATCH_SIZE)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Cross Entropy)')\n",
    "plt.plot(loss_array)\n",
    "plt.title(information)\n",
    "plt.savefig(plot_path + information + \".eps\")\n",
    "plt.clf()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.plot(learning_rate_array)\n",
    "plt.title(information)\n",
    "plt.savefig(plot_path + information + \"LR.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.size())\n",
    "print(outputs[:,:,5:10])\n",
    "print(outputs.size()[2])\n",
    "print(\"Loss half:\", criterion(outputs[:,:,SLICE_FROM_TIMESTEP:TIMESTEPS].float(),labels[:,SLICE_FROM_TIMESTEP:TIMESTEPS].long()))\n",
    "print(\"Loss full:\", criterion(outputs.float(),labels.long()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
