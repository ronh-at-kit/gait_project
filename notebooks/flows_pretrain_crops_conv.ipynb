{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings.py should be e.g. cnn_flows_prtrain\n",
      "Hyperparameters defined\n",
      "Bug with learning rate when sequence length is not the same for each element of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings.py should be e.g. cnn_flows_prtrain\")\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 #indicated ratio of training to validation data: 0.2 -> 20% VALIDATION data\n",
    "RANDOMIZED_SEED = 20\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "TRAINING_PREPARATION = False\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "TIME_STEPS = 40\n",
    "\n",
    "NR_EPOCHS = 50\n",
    "\n",
    "LR = 0.001\n",
    "# MOMENTUM = 0.9\n",
    "print(\"Hyperparameters defined\")\n",
    "print(\"Bug with learning rate when sequence length is not the same for each element of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "from gait_analysis import WeightWatcher\n",
    "from gait_analysis import AccuracyTracker\n",
    "from gait_analysis import AccuracyTrackerTrainTest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gait_analysis.Models import PretrainConvFlow\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  cnn_flows_pretrain\n",
      "[OK]\n",
      "Dataset size: 5\n",
      "Indices size: 5\n",
      "Split: 1\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "net = PretrainConvFlow.PretrainConvFlow()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "learning_rate_array = []\n",
    "loss_array = []\n",
    "# monitor = WeightWatcher(net,['conv5','fc1'])\n",
    "tt_acc_tracker = AccuracyTrackerTrainTest([0,1,2])\n",
    "# test_acc_tracker = AccuracyTracker([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAINING_PREPARATION:\n",
    "#     print('Start training...')\n",
    "#     print(\"Expected loss with {} different classes and {} data elements: {}\".format(3, len(dataset)-split, (len(dataset)-split)*np.log(3)))\n",
    "#     running_loss = 0.0\n",
    "#     print(\"Batch size:\", BATCH_SIZE)\n",
    "#     print(\"Evaluating first element...\")\n",
    "#     start_time = time.time()\n",
    "#     i, batch = next(iter(enumerate(train_loader)))\n",
    "#     inputs, labels = batch\n",
    "#     #TODO make list disappear\n",
    "\n",
    "#     data_in_list = [s.to(device) for s in inputs['flows']]\n",
    "#     labels = labels.to(device)\n",
    "\n",
    "#     print(\"--------------------------\\nOriginal input:\")\n",
    "#     print(\"Data in: List length\", len(data_in_list), \"List element:\", data_in_list[0].size())\n",
    "#     print(\"Labels:\", labels.size())\n",
    "#     print(\"--------------------------\\nSingle element:\")\n",
    "#     data_element = data_in_list[0]\n",
    "#     label_element = labels[:,0].long()\n",
    "#     print(\"Data element:\", data_element.size())\n",
    "#     print(\"Label element:\", label_element.size())\n",
    "\n",
    "#     optimizer.zero_grad() \n",
    "#     output_element = net(data_element)\n",
    "\n",
    "#     loss = criterion(output_element,label_element)\n",
    "#     loss.backward() \n",
    "#     optimizer.step()\n",
    "\n",
    "#     running_loss += loss.data.item()\n",
    "#     elapsed_time = time.time() - start_time;\n",
    "#     print(\"Loss:{}, expected loss:{}\".format(running_loss, np.log(3)))\n",
    "#     print(\"Time needed:{}s\".format(elapsed_time))\n",
    "#     print(\"Expected loss for total training data: \", (len(dataset)-split)*np.log(3))\n",
    "#     print(\"Expected training time per epoch:{} min\".format(elapsed_time* len(train_loader)*TIME_STEPS/BATCH_SIZE/60))\n",
    "#     print(\"Estimated total training time:{} hours\".format(elapsed_time* len(train_loader)*TIME_STEPS/BATCH_SIZE*NR_EPOCHS/3600))\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "-----------------------------\n",
      "Training set:\n",
      "Epoch 0: Loss [1.0717468768358234,1.35871434211731], Acc [0.422,0.45] took 8.68s\n",
      "Accuracy by class [[0.0, 1.0, 0.0],[0.0, 1.0, 0.0]] Total elements: [[72, 124, 98],[9, 18, 13]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a0b2b3804a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moutput_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#             print(\"Output element\",output_element)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#             print(\"Label element after\",label_element)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gait_project/gait_analysis/Models/PretrainConvFlow.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# for i in len(self.features):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#     x = F.MaxPool2d(F.relu(self.features[i](x)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# for i in len(self.classifier)-1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    start_time = time.time()\n",
    "#     print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    running_test_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in_list = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "#         print(\"--------------------------\\nOriginal input:\")\n",
    "#         print(\"Data in: List length\", len(data_in_list), \"List element:\", data_in_list[0].size())\n",
    "#         print(\"Labels:\", labels.size())\n",
    "\n",
    "        for j, data_element in enumerate(data_in_list):\n",
    "            if (labels.size(1) != len(data_in_list)):\n",
    "                print(\"WARNING, labels and data_element length do not match in Set\",i, \"element\", j)\n",
    "#                 print(\"Label element after\")\n",
    "#                 print(labels[0:len(data_in_list),j].size())\n",
    "#                 print(labels.size(),len(data_in_list))\n",
    "            label_element = labels[0:len(data_in_list),j].long()\n",
    "#             print(\"--------------------------\\nSingle element:\")\n",
    "#             print(\"Data element size:\", data_element.size())\n",
    "#             print(\"Label element size:\", label_element.size())\n",
    "#             print(\"Data element\", data_element)\n",
    "#             print(\"Label element\", label_element)\n",
    "#             print(\"Label element after\",label_element)\n",
    "\n",
    "#             print(\"Data element\", data_element[0][0])\n",
    "    \n",
    "            optimizer.zero_grad() \n",
    "            output_element = net(data_element)\n",
    "#             print(\"Output element\",output_element)\n",
    "#             print(\"Label element after\",label_element)\n",
    "            loss = criterion(output_element,label_element)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.data.item()/(len(dataset)*(1-VALIDATION_SPLIT))/TIME_STEPS*BATCH_SIZE\n",
    "#             print(\"losses:\",loss.data.item(),len(dataset),TIMESTEPS)\n",
    "#             print(\"Prediction:\", torch.max(output_element,1)[1])\n",
    "#             print(\"Labels:\", label_element.long())\n",
    "            prediction = torch.max(output_element,1)[1]\n",
    "            tt_acc_tracker.update_acc(prediction,label_element,\"TRAIN\")\n",
    "        \n",
    "    #TEST SET\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        inputs, labels = batch\n",
    "        data_in_list = [s.to(device) for s in inputs['flows']]\n",
    "        labels = labels.to(device)\n",
    "        for j, data_element in enumerate(data_in_list):\n",
    "            if (labels.size(1) != len(data_in_list)):\n",
    "                print(\"WARNING, labels and data_element length do not match in TEST Set\",i, \"element\", j)\n",
    "#                 print(labels[0:len(data_in_list),j].size())\n",
    "            label_element = labels[0:len(data_in_list),j].long()\n",
    "\n",
    "            output_element = net(data_element)\n",
    "            test_loss = criterion(output_element,label_element)\n",
    "            running_test_loss += loss.data.item()/(len(dataset)*VALIDATION_SPLIT)/TIME_STEPS\n",
    "\n",
    "            prediction = torch.max(output_element,1)[1]\n",
    "            tt_acc_tracker.update_acc(prediction,label_element,\"TEST\")\n",
    "        \n",
    "            \n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Training set:\")\n",
    "    print(\"Epoch {}: Loss [{},{}], Acc [{},{}] took {}s\".format(epoch, running_loss,running_test_loss,tt_acc_tracker.get_acc_tot(\"TRAIN\"),tt_acc_tracker.get_acc_tot(\"TEST\"), np.around(time.time()-start_time,decimals=2)))\n",
    "    print(\"Accuracy by class [{},{}] Total elements: [{},{}]\".format(tt_acc_tracker.get_acc(\"TRAIN\"),tt_acc_tracker.get_acc(\"TEST\"), tt_acc_tracker.get_labels_distribution(\"TRAIN\"),tt_acc_tracker.get_labels_distribution(\"TEST\")))\n",
    "\n",
    "    tt_acc_tracker.update_graph()\n",
    "    tt_acc_tracker.reset_acc_both()\n",
    "    tt_acc_tracker.update_loss(running_loss)\n",
    "    tt_acc_tracker.update_lr(LR)\n",
    "    \n",
    "\n",
    "    \n",
    "#     monitor.update_weights(net,['conv5','fc1'])\n",
    "\n",
    "#test_all_preds(test_net) \n",
    "print('...Training finished')\n",
    "# plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_EPOCHS = 10\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(39,40):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), '/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/001_all_seq_ts40_test.pt')\n",
    "acc_tracker.write_to_csv('/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/001_all_seq_ts40_test/')\n",
    "# Later:\n",
    "# loaded_model = PretrainConvFlow.PretrainConvFlow()\n",
    "# loaded_model.load_state_dict(torch.load('/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/first_draft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# path = '/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/001_all_seq_ts40/'\n",
    "# with open(path + \"acc_tot.csv\", 'w') as file:\n",
    "#     wr = csv.writer(file)\n",
    "#     wr.writerow(acc_tracker.get_acc_tot_graph())\n",
    "# with open(path + \"loss.csv\", 'w') as file:\n",
    "#     wr = csv.writer(file)\n",
    "#     wr.writerow(acc_tracker.get_loss_graph())\n",
    "# with open(path + \"lr.csv\", 'w') as file:\n",
    "#     wr = csv.writer(file)\n",
    "#     wr.writerow(acc_tracker.get_lr_graph())\n",
    "# acc = acc_tracker.get_acc_graph()\n",
    "# for i in range(len(acc)):\n",
    "#     with open(path + \"acc\" + str(i) + \".csv\", 'w') as file:\n",
    "#         wr = csv.writer(file)\n",
    "#         wr.writerow(acc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later:\n",
    "# loaded_model = PretrainConvFlow.PretrainConvFlow()\n",
    "# loaded_model.load_state_dict(torch.load('/mnt/DATA/HIWI/IBT/saved_models/PretrainConvFlow/first_draft'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
