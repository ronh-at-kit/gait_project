{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  default\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "\n",
    "crop_im_size = [186,250]\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "c.config['indexing']['people selection'] = [1]\n",
    "#c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "c.config['pose']['load'] = False\n",
    "c.config['flow']['load'] = False\n",
    "c.config['heatmaps']['load'] = False\n",
    "#c.config['scenes']['sequences'] = ['nm']\n",
    "#c.config['scenes']['angles'] = ['108']\n",
    "c.config['dataset_output'] = {\n",
    "#         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "        'data': [\"scenes\"],\n",
    "        'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NETWORK AND DATASET PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters defined\n"
     ]
    }
   ],
   "source": [
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2 \n",
    "IMAGE_INPUT_SIZE_W = 640\n",
    "IMAGE_INPUT_SIZE_H = 480\n",
    "\n",
    "IMAGE_AFTER_CONV_SIZE_W = 18\n",
    "IMAGE_AFTER_CONV_SIZE_H = 13\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_IO_SIZE = 18*13\n",
    "LSTM_HIDDEN_SIZE = 18*13\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 10 # size videos\n",
    "BATCH_SIZE = 1 #until now just batch_size = 1\n",
    "\n",
    "NR_EPOCHS = 50\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOMIZED_SEED = 10\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "learning_rate = 0.01 # reduce factos of 10 .. some epoch later.\n",
    "momentum = 0.9\n",
    "print(\"Hyperparameters defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings \n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5\n",
      "Indices size: 5\n",
      "Split: 1\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#USE RANDOM IMAGES TO SET UP WORKING EXAMPLE\n",
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.avialable_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #input 640x480\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #input 638x478 output 319x239\n",
    "        self.conv2 = nn.Conv2d(6,16,3) # input 319x239 output 317x237\n",
    "        self.pool2 = nn.MaxPool2d(2,2) # input 317x237 output 158x118\n",
    "        self.conv3 = nn.Conv2d(16,6,3) # input 158x118 output 156x116\n",
    "        self.pool3 = nn.MaxPool2d(2,2) # input 156x116 output 78x58\n",
    "        self.conv4 = nn.Conv2d(6,3,3)  # input 78x58 output 76x56\n",
    "        self.pool4 = nn.MaxPool2d(2,2) # input 76x56 output 39x29\n",
    "        self.conv5 = nn.Conv2d(3,1,3)  # input 39x29 output 37x27\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #output 37x27 output 18x13\n",
    "        self.lstm1 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.lstm2 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.fc1 = nn.Linear(LSTM_IO_SIZE,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of LSTM\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        #print(\"Hidden:\", _hidden)\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device), \n",
    "                torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device))\n",
    "    def forward(self,x):\n",
    "#         print(\"Input list len:\",len(x))\n",
    "#         print(\"Input elemens size:\", x[0].size())\n",
    "#         batch_size = x[0].size()[0]\n",
    "\n",
    "        x_arr = torch.zeros(TIMESTEPS,BATCH_SIZE,1,IMAGE_AFTER_CONV_SIZE_H,IMAGE_AFTER_CONV_SIZE_W).to(self.avialable_device)\n",
    "        ## print(\"X arr size\", x_arr.size())\n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i].float())))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = x_tmp_c5 # torch.squeeze(x_tmp_c5)\n",
    "        \n",
    "        x, hidden = self.lstm1(x_arr.view(TIMESTEPS,BATCH_SIZE,-1), self.hidden)\n",
    "        x, hidden = self.lstm2(x, self.hidden)\n",
    "        # the reshaping was taken from the documentation... and makes scense\n",
    "        x = x.view(TIMESTEPS,BATCH_SIZE,LSTM_HIDDEN_SIZE) #output.view(seq_len, batch, num_dir*hidden_size)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        x = x.permute(1,2,0)\n",
    "        return x\n",
    "print(\"Class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOS:\n",
    "# Look for overfitting..\n",
    "# use GPU: 300 samples. many epochs. learning rate maybe is too high.\n",
    "# Tweak learning rate\n",
    "# Increase the RNN size\n",
    "# Maybe change the CNN\n",
    "# Use patches. or OF\n",
    "# Increase the dataset: 10 images 10peoplo+++ not too much too much variance. \n",
    "# FEEDBACK on friday..:) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define and execute testing function\n",
    "def test_all_preds(model):\n",
    "    n_batches_test = len(test_loader)\n",
    "\n",
    "    #Time for printing\n",
    "    testing_start_time = time.time()\n",
    "\n",
    "    print('Start testing...')\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            scenes = [s.to(device) for s in inputs['scenes']]\n",
    "            labels = labels.to(device)\n",
    "            if not labels.size()[0] == BATCH_SIZE:\n",
    "                # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "                continue\n",
    "            outputs = model(scenes)\n",
    "#             print(\"Out:\", len(outputs), outputs.size())\n",
    "#             print(\"Labels:\", len(labels), labels.size())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "            n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "            total += predicted.numel()\n",
    "            # print('predicted',predicted)\n",
    "            correct += predicted.numel() - n_errors\n",
    "            # print('labels',labels)\n",
    "    print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "    print('...testing finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training over all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.conda/envs/gait_37/lib/python3.7/site-packages/pandas/core/computation/expressions.py:183: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  .format(op=op_str, alt_op=unsupported[op_str]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss epoch 0: 4.1650155782699585, took 10.854314804077148s\n",
      "Epoch: 1\n",
      "Loss epoch 1: 4.102218747138977, took 9.13144588470459s\n",
      "Epoch: 2\n",
      "Loss epoch 2: 4.008196711540222, took 8.767223596572876s\n",
      "Epoch: 3\n",
      "Loss epoch 3: 3.9119938611984253, took 9.08334994316101s\n",
      "Epoch: 4\n",
      "Loss epoch 4: 3.8227299451828003, took 9.4818274974823s\n",
      "Epoch: 5\n",
      "Loss epoch 5: 3.742759048938751, took 8.998649835586548s\n",
      "Epoch: 6\n",
      "Loss epoch 6: 3.6747117042541504, took 8.649889707565308s\n",
      "Epoch: 7\n",
      "Loss epoch 7: 3.618113875389099, took 9.372781753540039s\n",
      "Epoch: 8\n",
      "Loss epoch 8: 3.570672392845154, took 12.605225086212158s\n",
      "Epoch: 9\n",
      "Loss epoch 9: 3.53006374835968, took 10.258641481399536s\n",
      "Epoch: 10\n",
      "Loss epoch 10: 3.4937307834625244, took 11.254355669021606s\n",
      "Epoch: 11\n",
      "Loss epoch 11: 3.45981502532959, took 9.721545934677124s\n",
      "Epoch: 12\n",
      "Loss epoch 12: 3.4265136122703552, took 9.635929107666016s\n",
      "Epoch: 13\n",
      "Loss epoch 13: 3.392087399959564, took 10.202988862991333s\n",
      "Epoch: 14\n",
      "Loss epoch 14: 3.3550307154655457, took 11.160292387008667s\n",
      "Epoch: 15\n",
      "Loss epoch 15: 3.3134365677833557, took 11.727988719940186s\n",
      "Epoch: 16\n",
      "Loss epoch 16: 3.2644203305244446, took 9.308537006378174s\n",
      "Epoch: 17\n",
      "Loss epoch 17: 3.2062509655952454, took 10.31168246269226s\n",
      "Epoch: 18\n",
      "Loss epoch 18: 3.1371489763259888, took 9.745519399642944s\n",
      "Epoch: 19\n",
      "Loss epoch 19: 3.055304169654846, took 10.8978750705719s\n",
      "Epoch: 20\n",
      "Loss epoch 20: 2.959160804748535, took 9.245887756347656s\n",
      "Epoch: 21\n",
      "Loss epoch 21: 2.848247230052948, took 9.11722469329834s\n",
      "Epoch: 22\n",
      "Loss epoch 22: 2.724658250808716, took 9.029541969299316s\n",
      "Epoch: 23\n",
      "Loss epoch 23: 2.594398558139801, took 9.442051887512207s\n",
      "Epoch: 24\n",
      "Loss epoch 24: 2.4595890045166016, took 11.688318967819214s\n",
      "Epoch: 25\n",
      "Loss epoch 25: 2.3211646676063538, took 9.586875200271606s\n",
      "Epoch: 26\n",
      "Loss epoch 26: 2.182640850543976, took 9.249445915222168s\n",
      "Epoch: 27\n",
      "Loss epoch 27: 2.0517672300338745, took 10.504568576812744s\n",
      "Epoch: 28\n",
      "Loss epoch 28: 1.9288990795612335, took 9.3705735206604s\n",
      "Epoch: 29\n",
      "Loss epoch 29: 1.8166195452213287, took 8.873892784118652s\n",
      "Epoch: 30\n",
      "Loss epoch 30: 1.723500370979309, took 8.752981424331665s\n",
      "Epoch: 31\n",
      "Loss epoch 31: 1.6445975005626678, took 9.691883563995361s\n",
      "Epoch: 32\n",
      "Loss epoch 32: 1.5758247077465057, took 9.236284732818604s\n",
      "Epoch: 33\n",
      "Loss epoch 33: 1.5175607949495316, took 10.187545537948608s\n",
      "Epoch: 34\n",
      "Loss epoch 34: 1.4683048725128174, took 10.196648836135864s\n",
      "Epoch: 35\n",
      "Loss epoch 35: 1.4263632744550705, took 9.681501150131226s\n",
      "Epoch: 36\n",
      "Loss epoch 36: 1.3910185396671295, took 10.43569803237915s\n",
      "Epoch: 37\n",
      "Loss epoch 37: 1.360765501856804, took 9.595128297805786s\n",
      "Epoch: 38\n",
      "Loss epoch 38: 1.3341443240642548, took 8.868751287460327s\n",
      "Epoch: 39\n",
      "Loss epoch 39: 1.3103598207235336, took 8.966486930847168s\n",
      "Learning rate changed 0.01\n",
      "Epoch: 40\n",
      "Loss epoch 40: 1.3036011159420013, took 8.887276649475098s\n",
      "Epoch: 41\n",
      "Loss epoch 41: 1.289889544248581, took 9.45603060722351s\n",
      "Epoch: 42\n",
      "Loss epoch 42: 1.2728830575942993, took 8.9811532497406s\n",
      "Epoch: 43\n",
      "Loss epoch 43: 1.263603687286377, took 8.967230796813965s\n",
      "Epoch: 44\n",
      "Loss epoch 44: 1.2563772350549698, took 10.172816753387451s\n",
      "Epoch: 45\n",
      "Loss epoch 45: 1.2443717122077942, took 9.461336374282837s\n",
      "Epoch: 46\n",
      "Loss epoch 46: 1.228712022304535, took 9.367156505584717s\n",
      "Epoch: 47\n",
      "Loss epoch 47: 1.2126567363739014, took 9.320302486419678s\n",
      "Epoch: 48\n",
      "Loss epoch 48: 1.2004331797361374, took 9.229983568191528s\n",
      "Epoch: 49\n",
      "Loss epoch 49: 1.1915197521448135, took 10.125231981277466s\n",
      "Start testing...\n",
      "Accuracy 90.00%\n",
      "...testing finished\n",
      "...Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc18ecf2e8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "\n",
    "#Time for printing\n",
    "training_start_time = time.time()\n",
    "\n",
    "loss_array = np.zeros(NR_EPOCHS)\n",
    "\n",
    "print('Start training...')\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    if (epoch == 40):\n",
    "        learning_rate = 0.01\n",
    "        optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        print (\"Learning rate changed\", learning_rate)\n",
    "#     if (epoch == 80):\n",
    "#         learning_rate = 0.001\n",
    "#         optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#         print (\"Learning rate changed\", learning_rate)\n",
    "    \n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader): #loads all angles\n",
    "        inputs, labels = batch\n",
    "        scenes = [s.to(device) for s in inputs['scenes']]\n",
    "        labels = labels.unsqueeze(-1).to(device)\n",
    "        if not labels.size()[0] == BATCH_SIZE:\n",
    "            # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "            continue\n",
    "        optimizer.zero_grad() \n",
    "        outputs = test_net(scenes)\n",
    "        outputs = outputs.unsqueeze(-1)\n",
    "        #print(\"Size before permutation:\", outputs.size())\n",
    "        outputs = outputs.permute(0,2,1,3).squeeze(3).squeeze(0)\n",
    "        labels = labels.squeeze(2).squeeze(0)\n",
    "        #print(\"Out:\", len(outputs), outputs.size())\n",
    "        #print(\"Labels:\", len(labels), labels.size())\n",
    "        #print(\"Here\")\n",
    "        #print(scenes[0].squeeze(0).permute(2,1,0).size())\n",
    "        #for j in range(10):\n",
    "        #    scene_print = scenes[j].squeeze(0).permute(1,2,0)\n",
    "        #    plt.imshow(scene_print.numpy())\n",
    "        #    plt.show()\n",
    "        #print(labels)\n",
    "        loss = criterion(outputs,labels.long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print statistics\n",
    "        # print(loss.data.item())\n",
    "        running_loss += loss.data.item()\n",
    "        #total_train_loss += loss.data.item()\n",
    "        \n",
    "        #Print every 10th batch of an epoch\n",
    "#         if (i + 1) % (print_every + 1) == 0:\n",
    "#             print(\"Epoch {}, {:d}% \\t train_loss(mean): {:.2f} took: {:.2f}s\".format(\n",
    "#                     epoch+1, int(100 * (i+1) / n_batches), running_loss/print_every, time.time() - start_time))\n",
    "#             #Reset running loss and time\n",
    "#             running_loss = 0.0\n",
    "#             start_time = time.time()\n",
    "    # test after each epoch\n",
    "    print(\"Loss epoch {}: {}, took {}s\".format(epoch, running_loss,time.time()-start_time))\n",
    "    loss_array[epoch] = running_loss\n",
    "\n",
    "test_all_preds(test_net)\n",
    "#print('total training loss for epoch {}: {:.6f}'.format(epoch+1, total_train_loss))    \n",
    "print('...Training finished')\n",
    "plt.plot(loss_array)\n",
    "plt.plot(loss_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels.size())\n",
    "# print(outputs.size())\n",
    "# print(labels)\n",
    "# print(outputs)\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(scenes[0].squeeze(0).size())\n",
    "# plt.imshow(scenes[9].squeeze(0).permute(1,2,0))\n",
    "# plt.figure(figsize=(30,40))\n",
    "# print('left foot trans from air to ground')\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(scenes[4])\n",
    "# plt.title('frame 44')\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(scenes[5])\n",
    "# plt.title('frame 45')\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(scenes[6])\n",
    "# plt.title('frame 46')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
