{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use this file\n",
    "This file shows the vanishing gradient problem. \n",
    "Execute it with LR 0.1 and LR 0.01 and have a look at the final plots when the conv-layers change.\n",
    "Note that the network uses the original loop approach (TIMESTEPS, RGB_CHANNELS, HEIGHT, WIDTH) and the gradients do change (if the learning rate is sufficiently high).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configuration needed\n"
     ]
    }
   ],
   "source": [
    "print(\"No configuration needed\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 1\n",
    "IMAGE_INPUT_SIZE = 550\n",
    "# Similar input size to \"normal\" data: 130x130 -> 15x15 with 3 conv layers\n",
    "# image input of 54 yields output after conv layers of 5\n",
    "IMAGE_AFTER_CONV_SIZE = 15\n",
    "\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_INPUT_SIZE = IMAGE_AFTER_CONV_SIZE*IMAGE_AFTER_CONV_SIZE\n",
    "LSTM_HIDDEN_SIZE = 100\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 10\n",
    "BATCH_SIZE = 1 #until now just batch_size = 1\n",
    "NR_EPOCHS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "FORMAT_TIMESTEPS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "if TIMESTEPS == 10:\n",
    "    lab = [\n",
    "        [0,0,0,1,1,2,2,2,1,1],\n",
    "        [0,1,1,2,2,2,1,1,0,0],\n",
    "        [1,2,2,2,1,1,0,0,0,1],\n",
    "        [2,2,1,1,0,0,0,1,1,2],\n",
    "        [1,1,0,0,0,1,1,2,2,2],\n",
    "        [0,0,1,1,2,2,2,1,1,0],\n",
    "        [1,1,2,2,2,1,1,0,0,1],\n",
    "        [2,2,2,1,1,0,0,0,1,1],\n",
    "        [2,1,1,0,0,0,1,1,2,2],\n",
    "        [1,0,0,0,1,1,2,2,2,1]\n",
    "    ]\n",
    "elif TIMESTEPS == 20:\n",
    "        lab = [\n",
    "        [0,0,0,1,1,2,2,2,1,1,0,0,0,1,1,2,2,2,1,1],\n",
    "        [0,1,1,2,2,2,1,1,0,0,0,1,1,2,2,2,1,1,0,0],\n",
    "        [1,2,2,2,1,1,0,0,0,1,1,2,2,2,1,1,0,0,0,1],\n",
    "        [2,2,1,1,0,0,0,1,1,2,2,2,1,1,0,0,0,1,1,2],\n",
    "        [1,1,0,0,0,1,1,2,2,2,1,1,0,0,0,1,1,2,2,2],\n",
    "        [0,0,1,1,2,2,2,1,1,0,0,0,1,1,2,2,2,1,1,0],\n",
    "        [1,1,2,2,2,1,1,0,0,1,1,1,2,2,2,1,1,0,0,1],\n",
    "        [2,2,2,1,1,0,0,0,1,1,2,2,2,1,1,0,0,0,1,1],\n",
    "        [2,1,1,0,0,0,1,1,2,2,2,1,1,0,0,0,1,1,2,2],\n",
    "        [1,0,0,0,1,1,2,2,2,1,1,0,0,0,1,1,2,2,2,1]\n",
    "        ]\n",
    "std_dev = 1\n",
    "training_set_size = len(lab)\n",
    "arr = np.full((training_set_size,TIMESTEPS,BATCH_SIZE,RGB_CHANNELS,IMAGE_INPUT_SIZE,IMAGE_INPUT_SIZE),0)\n",
    "noise_arr = np.random.normal(0,std_dev,arr.shape)\n",
    "# print(noise_arr.shape)\n",
    "\n",
    "sequences = lab\n",
    "for i,ll in enumerate(lab):\n",
    "    for j,l in enumerate(ll):\n",
    "        sequences[i][j] = (l-1)*0.5\n",
    "# print(sequences)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for j, image in enumerate(sequence):\n",
    "        noise_arr[i][j] = noise_arr[i][j] + np.full(noise_arr[i][j].shape,image)\n",
    "# print(noise_arr)\n",
    "\n",
    "dataset = torch.from_numpy(noise_arr)\n",
    "labelset = torch.tensor(lab)\n",
    "# print(labels)\n",
    "# print(dataset)\n",
    "print(\"Dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model):\n",
    "    print(\"To be implemented\")\n",
    "\n",
    "#     n_batches_test = len(test_loader)\n",
    "\n",
    "#     #Time for printing\n",
    "#     testing_start_time = time.time()\n",
    "\n",
    "#     print('Start testing...')\n",
    "#     correct = 0 \n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(train_loader):\n",
    "#             inputs, labels = batch\n",
    "            \n",
    "#             data_in = [s.to(device) for s in inputs['flows']]\n",
    "#             labels = labels.to(device)\n",
    "#             if not labels.size()[0] == BATCH_SIZE:\n",
    "#                 # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "#                 continue\n",
    "#             outputs = model(data_in)\n",
    "# #             print(\"Out:\", len(outputs), outputs.size())\n",
    "# #             print(\"Labels:\", len(labels), labels.size())\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "# #             print('predicted:',len(predicted),predicted.size())\n",
    "#             n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "#             total += predicted.numel()\n",
    "#             # print('predicted',predicted)\n",
    "#             correct += predicted.numel() - n_errors\n",
    "#             # print('labels',labels)\n",
    "#     print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "#     print('...testing finished')\n",
    "# print(\"Definition done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined\n",
      "Hidden: 2\n"
     ]
    }
   ],
   "source": [
    "#USE RANDOM IMAGES TO SET UP WORKING EXAMPLE\n",
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "                                        # in 54x54\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #out 52x52\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #out 26x26\n",
    "        self.conv2= nn.Conv2d(6,3,3) #out 24x24\n",
    "        self.pool2 = nn.MaxPool2d(2,2) #out 12x12\n",
    "        self.conv3 = nn.Conv2d(3,1,3) #out 10x10\n",
    "        self.pool3 = nn.MaxPool2d(2,2) #out 5x5\n",
    "        self.conv4= nn.Conv2d(1,1,3) #out 24x24\n",
    "        self.pool4 = nn.MaxPool2d(2,2) #out 12x12\n",
    "        self.conv5 = nn.Conv2d(1,1,3) #out 10x10\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #out 5x5     \n",
    "        \n",
    "#         self.lstm = nn.LSTM(LSTM_INPUT_SIZE,\n",
    "#                             LSTM_HIDDEN_SIZE,\n",
    "#                             NR_LSTM_UNITS)\n",
    "        self.fc1 = nn.Linear(LSTM_INPUT_SIZE,100)\n",
    "        self.fc2 = nn.Linear(100,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of normal LSTM\n",
    "        self._hidden = (torch.randn(NR_LSTM_UNITS, BATCH_SIZE, LSTM_HIDDEN_SIZE),\n",
    "                        torch.randn(NR_LSTM_UNITS, BATCH_SIZE, LSTM_HIDDEN_SIZE))\n",
    "\n",
    "        print(\"Hidden:\", len(self._hidden))\n",
    "    def forward(self,x):\n",
    "        #print(\"Input:\", x.size())\n",
    "        x = x.float()\n",
    "        \n",
    "#         print(\"X arr size\", x_arr.size())\n",
    "#         print(\"x shape\",x.shape)\n",
    "#         print(\"x[0]\",x[0].shape)\n",
    "        x_arr = torch.zeros(TIMESTEPS,BATCH_SIZE,1,IMAGE_AFTER_CONV_SIZE,IMAGE_AFTER_CONV_SIZE)\n",
    "    \n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i])))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = x_tmp_c5\n",
    "    \n",
    "        x = torch.cat(tuple(x for x in x_arr),0)\n",
    "        \n",
    "#         print(\"x before LSTM\",x.view(TIMESTEPS,BATCH_SIZE,-1).shape) \n",
    "#         x, _hidden = self.lstm(x.view(TIMESTEPS,BATCH_SIZE,-1), self._hidden)\n",
    "#         print(\"x after LSTM\",x.shape) \n",
    "        x = x.view(-1,LSTM_INPUT_SIZE)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "print(\"Class defined\")\n",
    "\n",
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1: Mean 0.00014501961 Variance 0.0014719596\n",
      "Saved initial weights and biases from net\n"
     ]
    }
   ],
   "source": [
    "parameters_conv1_init = list(test_net.conv1.parameters())\n",
    "bias_conv1_init = parameters_conv1_init[1].detach().numpy()\n",
    "weights_conv1_init = parameters_conv1_init[0].detach().numpy().flatten()\n",
    "\n",
    "parameters_fc1_init = list(test_net.fc1.parameters())\n",
    "bias_fc1_init = parameters_fc1_init[1].detach().numpy()\n",
    "weights_fc1_init = parameters_fc1_init[0].detach().numpy().flatten()\n",
    "\n",
    "print(\"FC1: Mean\",np.mean(weights_fc1_init),\"Variance\",np.var(weights_fc1_init))\n",
    "print(\"Saved initial weights and biases from net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 0 Loss 11.617700576782227\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 1 Loss 10.844573974609375\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 2 Loss 9.455743789672852\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 3 Loss 7.654208183288574\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 4 Loss 5.666996955871582\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 5 Loss 3.7423577308654785\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 6 Loss 2.071416139602661\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 7 Loss 0.8419156074523926\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 8 Loss 0.22957563400268555\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 9 Loss 0.04066944122314453\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 10 Loss 0.00484466552734375\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 11 Loss 0.0004100799560546875\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 12 Loss 2.86102294921875e-05\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 13 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 14 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 15 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 16 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 17 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 18 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 19 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 20 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 21 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n",
      "Optimizer step\n",
      "Epoch: 22 Loss 0.0\n",
      "Forward pass\n",
      "Loss defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7fa2be8adcc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss defined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimizer step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gait_37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_array = []\n",
    "conv_var_array = []\n",
    "fc_var_array = []\n",
    "print('Start training...')\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    loss = 0.0\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    for data_in, labels in zip(dataset, labelset):\n",
    "        outputs = test_net(data_in)\n",
    "        single_loss = criterion(outputs, labels.long())\n",
    "        loss += single_loss\n",
    "    \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Loss\",loss.data.item())\n",
    "    parameters_conv1 = list(test_net.conv1.parameters())\n",
    "    conv_weight_diff = parameters_conv1[0].detach().numpy().flatten()-weights_conv1_init\n",
    "    parameters_fc1 = list(test_net.fc1.parameters())\n",
    "    fc_weight_diff = parameters_fc1[0].detach().numpy().flatten()-weights_fc1_init\n",
    "    conv_var_array.append(np.var(conv_weight_diff))\n",
    "    fc_var_array.append(np.var(fc_weight_diff))\n",
    "    loss_array.append(loss.data.item())\n",
    "print('...Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_EPOCHS = 100\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_fc1 = list(test_net.fc1.parameters())\n",
    "bias_fc1 = parameters_fc1[1].detach().numpy()\n",
    "weights_fc1 = parameters_fc1[0].detach().numpy().flatten()\n",
    "\n",
    "parameters_conv1 = list(test_net.conv1.parameters())\n",
    "bias_conv1 = parameters_conv1[1].detach().numpy()\n",
    "weights_conv1 = parameters_conv1[0].detach().numpy().flatten()\n",
    "\n",
    "weights_fc1_m = np.mean(weights_fc1-weights_fc1_init)\n",
    "weights_fc1_v = np.var(weights_fc1-weights_fc1_init)\n",
    "\n",
    "print(\"Conv: Mean\",np.mean(weights_conv1_init),\"Variance\",np.var(weights_conv1_init))\n",
    "print(\"Conv: Mean\",np.mean(weights_conv1),\"Var\",np.var(weights_conv1))\n",
    "print(\"FC1 Diff: Var\",np.var(weights_fc1_init-weights_fc1))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Difference in FC-Layers\")\n",
    "# plt.plot(range(len(weights_fc1)),weights_fc1_init-weights_fc1)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Difference in Conv-Layers\")\n",
    "# plt.plot(range(len(weights_conv1)),weights_conv1_init-weights_conv1)#,range(len(weights_conv5)),weights_conv5)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_array)\n",
    "plt.figure()\n",
    "plt.title(\"Variance of difference of conv layer\")\n",
    "plt.plot(conv_var_array)\n",
    "plt.figure()\n",
    "plt.title(\"Variance of difference of fc layer\")\n",
    "plt.plot(fc_var_array)\n",
    "print(\"With LR 0.1 the network converges within 100 epochs\")\n",
    "print(\"With LR 0.01 the network does converge too, but only after around 500 epochs\")\n",
    "print(\"With LR 0.01 we experience a vanishing gradient problem, the conv layers do not change at all in the beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_36",
   "language": "python",
   "name": "gait_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
