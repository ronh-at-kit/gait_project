{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gait_analysis import CasiaDataset, settings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure a dataset aggregation of input/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the datasets and the indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  cnn_flows_pretrain\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "from gait_analysis.Config import Config\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence'\n",
    "c.config['pose']['load'] = True\n",
    "c.config['flow']['load'] = True\n",
    "c.config['heatmaps']['load'] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.config['dataset_output'] = {\n",
    "        'data': [\"scenes\",\"flows\",\"heatmaps\"],\n",
    "        'label': \"annotations\"}\n",
    "c.config['scenes']['load']=True\n",
    "c.config['flow']['load']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.config['transformers'] = {\n",
    "        # 'Crop':{'include list':['LAnkle','RAnkle'],'output_size':256,'target':'flows'}\n",
    "        'SpanImagesList': {'remove':True, 'names': [\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],'target': [\"heatmaps\"]},\n",
    "        'Rescale': {'output_size' : (640,480), 'target': [\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"]},\n",
    "        'AnnotationToLabel': {'target': [\"annotations\"]},\n",
    "        'Transpose': {'swapping': (2, 0, 1), 'target': [\"scenes\", \"flows\"]},\n",
    "#         'Transpose' : {'swapping': (2, 0, 1) , 'target': [\"scenes\"]},\n",
    "        'DimensionResize' : {'dimension': 40, 'target': [\"heatmaps_LAnkle\",\"heatmaps_RAnkle\",\"scenes\",\"flows\",\"annotations\"]},\n",
    "#         'DimensionResize' : {'dimension': 10, 'target': [\"scenes\",\"annotations\"]},\n",
    "        'ToTensor': {'target':[\"scenes\",\"annotations\"]}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plain and untransformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_plain = CasiaDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['scenes', 'flows', 'heatmaps'])\n"
     ]
    }
   ],
   "source": [
    "item, labels = dataset_plain[1]\n",
    "scenes = item['scenes']\n",
    "flows = item['flows']\n",
    "print(item.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the composer to construct reusable transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchvision.transforms.Compose object at 0x7fd864f98c88>\n"
     ]
    }
   ],
   "source": [
    "from gait_analysis import Composer\n",
    "\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person_sequence\n"
     ]
    }
   ],
   "source": [
    "print(c.get_indexing_grouping())\n",
    "test_item, labels_df = dataset_plain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just to test we include add a field annotations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> transformed item is:  dict_keys(['scenes', 'flows', 'annotations', 'heatmaps_LAnkle', 'heatmaps_RAnkle'])\n"
     ]
    }
   ],
   "source": [
    "test_item['annotations'] = labels_df\n",
    "transformed_item = transformer(test_item)\n",
    "print('==> transformed item is: ',test_item.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scenes', 'flows', 'heatmaps'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformed_item['scenes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': ['30',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '40',\n",
       "  '41',\n",
       "  '42',\n",
       "  '43',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '80',\n",
       "  '81'],\n",
       " 'frame_id': ['30',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '40',\n",
       "  '41',\n",
       "  '42',\n",
       "  '43',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '80',\n",
       "  '81'],\n",
       " 'left_foot': ['IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR'],\n",
       " 'right_foot': ['ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'IN_THE_AIR',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND',\n",
       "  'ON_GROUND']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the transformer and the dataset_output specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing some samples of the transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "0 dict_keys(['scenes', 'flows', 'heatmaps_LAnkle', 'heatmaps_RAnkle'])\n",
      "torch.Size([40])\n",
      "i:  1\n",
      "1 dict_keys(['scenes', 'flows', 'heatmaps_LAnkle', 'heatmaps_RAnkle'])\n",
      "torch.Size([40])\n",
      "i:  2\n",
      "2 dict_keys(['scenes', 'flows', 'heatmaps_LAnkle', 'heatmaps_RAnkle'])\n",
      "torch.Size([40])\n",
      "i:  3\n",
      "3 dict_keys(['scenes', 'flows', 'heatmaps_LAnkle', 'heatmaps_RAnkle'])\n",
      "torch.Size([40])\n",
      "i:  4\n",
      "4 dict_keys(['scenes', 'flows', 'heatmaps_LAnkle', 'heatmaps_RAnkle'])\n",
      "torch.Size([40])\n",
      "i:  5\n",
      "5 dict_keys(['scenes', 'flows', 'heatmaps_LAnkle', 'heatmaps_RAnkle'])\n",
      "torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "c.config['dataset_output'] = {\n",
    "        'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "        'label': \"annotations\"}\n",
    "transformed_dataset = CasiaDataset(transform=transformer)\n",
    "\n",
    "for i in range(len(transformed_dataset)):\n",
    "    print('i: ', i)\n",
    "    sample, labels = transformed_dataset[i]\n",
    "\n",
    "    print(i, sample.keys())\n",
    "    print(labels.size())\n",
    "\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader: using the transformed dataset with the transformer to create a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "#                         shuffle=True, num_workers=4)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEV: The problem is that the dimensions of the label are different for some of the videos. In this case the aggregation works for all the angles in the video but no arbitrary batch_size will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flows(inputs):\n",
    "    flows = inputs['flows']\n",
    "    print('plotting sample flow chunck')\n",
    "    flow = flows[4]\n",
    "    batch_size = flow.size(0)\n",
    "    im_size = flow.size()\n",
    "    print('im_size = ', im_size)\n",
    "    print('batch_size = ', batch_size )\n",
    "    grid = utils.make_grid(flow)\n",
    "    plt.figure(figsize=(10,50))\n",
    "    plt.imshow(grid.numpy()[2,:,:])\n",
    "    plt.show()\n",
    "    \n",
    "def plot_scenes(inputs):\n",
    "    scenes = inputs['scenes']\n",
    "    print('plotting sample scene chunck')\n",
    "    scene = scenes[4]\n",
    "    grid = utils.make_grid(scene)\n",
    "    plt.figure(figsize=(10,50))\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_heatmap(inputs):\n",
    "    hmaps = inputs['heatmaps_LAnkle']\n",
    "    print('plotting sample heatmasp chunck')\n",
    "    hmap = hmaps[4].numpy()\n",
    "    print('hmap.shape = ',hmap.shape)\n",
    "    plt.figure(figsize=(10,50))\n",
    "    plt.subplot(151)\n",
    "    plt.imshow(hmap[1,:,:])\n",
    "    plt.subplot(152)\n",
    "    plt.imshow(hmap[2,:,:])\n",
    "    plt.subplot(153)\n",
    "    plt.imshow(hmap[3,:,:])\n",
    "    plt.subplot(154)\n",
    "    plt.imshow(hmap[4,:,:])\n",
    "    plt.subplot(155)\n",
    "    plt.imshow(hmap[5,:,:])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 9:\n",
    "        inputs, labels = sample_batched\n",
    "        plot_flows(inputs)\n",
    "        plot_scenes(inputs)\n",
    "        plot_heatmap(inputs)\n",
    "        print(type(sample_batched))\n",
    "        print('labels,',labels.size(),labels)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400, 400)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.eye(400)\n",
    "\n",
    "a.resize((1,400,400))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f68e25005952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Datasets/Gait/Repositories/gait_project/gait_analysis/DataSets/CasiaDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# annotations are always in the output:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_frame_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# adding all other optional features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Datasets/Gait/Repositories/gait_project/gait_analysis/DataSets/AnnotationsCasia.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mannotation\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         '''\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdataset_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mgrouping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexing_grouping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "i,l = transformed_dataset[42]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
