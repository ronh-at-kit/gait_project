{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import os.path as path\n",
    "import copy\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "\n",
    "from gait_analysis import AnnotationsCasia as Annotations\n",
    "from gait_analysis import CasiaDataset\n",
    "from gait_analysis.Config import Config\n",
    "from gait_analysis import Composer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration  scenes\n",
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "#change configuration in settings.py\n",
    "\n",
    "crop_im_size = [186,250]\n",
    "c = Config()\n",
    "c.config['indexing']['grouping'] = 'person_sequence_angle'\n",
    "c.config['indexing']['people selection'] = [1]\n",
    "#c.config['indexing']['sequences_selection'] = ['nm-01']\n",
    "c.config['pose']['load'] = False\n",
    "c.config['flow']['load'] = False\n",
    "c.config['heatmaps']['load'] = False\n",
    "#c.config['scenes']['sequences'] = ['nm']\n",
    "#c.config['scenes']['angles'] = ['108']\n",
    "c.config['dataset_output'] = {\n",
    "#         'data': [\"scenes\",\"flows\",\"heatmaps_LAnkle\",\"heatmaps_RAnkle\"],\n",
    "        'data': [\"scenes\"],\n",
    "        'label': \"annotations\"}\n",
    "composer = Composer()\n",
    "transformer = composer.compose()\n",
    "dataset = CasiaDataset(transform=transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NETWORK AND DATASET PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters defined\n"
     ]
    }
   ],
   "source": [
    "#DESIGN PARAMETERS FOR NEURAL NETWORK\n",
    "NR_LSTM_UNITS = 2 \n",
    "IMAGE_INPUT_SIZE_W = 640\n",
    "IMAGE_INPUT_SIZE_H = 480\n",
    "\n",
    "IMAGE_AFTER_CONV_SIZE_W = 18\n",
    "IMAGE_AFTER_CONV_SIZE_H = 13\n",
    "#for 3x3 kernels, n=num_layers: len_in = 2^n*len_out + sum[i=1..n](2^i)\n",
    "#CONV_LAYER_LENGTH = 5\n",
    "\n",
    "LSTM_IO_SIZE = 18*13\n",
    "LSTM_HIDDEN_SIZE = 18*13\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "TIMESTEPS = 10 # size videos\n",
    "BATCH_SIZE = 1 #until now just batch_size = 1\n",
    "\n",
    "NR_EPOCHS = 100\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOMIZED_SEED = 10\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "learning_rate = 0.01 # reduce factos of 10 .. some epoch later.\n",
    "momentum = 0.9\n",
    "print(\"Hyperparameters defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings \n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 30\n",
      "Indices size: 30\n",
      "Split: 6\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Indices size:\", len(indices))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "print(\"Split:\", split)\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOMIZED_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#USE RANDOM IMAGES TO SET UP WORKING EXAMPLE\n",
    "class TEST_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEST_CNN_LSTM, self).__init__()\n",
    "        self.avialable_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,6,3) #input 640x480\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #input 638x478 output 319x239\n",
    "        self.conv2 = nn.Conv2d(6,16,3) # input 319x239 output 317x237\n",
    "        self.pool2 = nn.MaxPool2d(2,2) # input 317x237 output 158x118\n",
    "        self.conv3 = nn.Conv2d(16,6,3) # input 158x118 output 156x116\n",
    "        self.pool3 = nn.MaxPool2d(2,2) # input 156x116 output 78x58\n",
    "        self.conv4 = nn.Conv2d(6,3,3)  # input 78x58 output 76x56\n",
    "        self.pool4 = nn.MaxPool2d(2,2) # input 76x56 output 39x29\n",
    "        self.conv5 = nn.Conv2d(3,1,3)  # input 39x29 output 37x27\n",
    "        self.pool5 = nn.MaxPool2d(2,2) #output 37x27 output 18x13\n",
    "        self.lstm1 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.lstm2 = nn.LSTM(LSTM_IO_SIZE,\n",
    "                            LSTM_HIDDEN_SIZE,\n",
    "                            TIMESTEPS)# horizontal direction\n",
    "        self.fc1 = nn.Linear(LSTM_IO_SIZE,120)\n",
    "        self.fc2 = nn.Linear(120,20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "        #initialize hidden states of LSTM\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        #print(\"Hidden:\", _hidden)\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device), \n",
    "                torch.randn(TIMESTEPS, BATCH_SIZE, LSTM_HIDDEN_SIZE).to(self.avialable_device))\n",
    "    def forward(self,x):\n",
    "#         print(\"Input list len:\",len(x))\n",
    "#         print(\"Input elemens size:\", x[0].size())\n",
    "#         batch_size = x[0].size()[0]\n",
    "\n",
    "        x_arr = torch.zeros(TIMESTEPS,BATCH_SIZE,1,IMAGE_AFTER_CONV_SIZE_H,IMAGE_AFTER_CONV_SIZE_W).to(self.avialable_device)\n",
    "        ## print(\"X arr size\", x_arr.size())\n",
    "        for i in range(TIMESTEPS):#parallel convolutions which are later concatenated for LSTM\n",
    "            x_tmp_c1 = self.pool1(F.relu(self.conv1(x[i].float())))\n",
    "            x_tmp_c2 = self.pool2(F.relu(self.conv2(x_tmp_c1)))\n",
    "            x_tmp_c3 = self.pool3(F.relu(self.conv3(x_tmp_c2)))\n",
    "            x_tmp_c4 = self.pool4(F.relu(self.conv4(x_tmp_c3)))\n",
    "            x_tmp_c5 = self.pool5(F.relu(self.conv5(x_tmp_c4)))\n",
    "            x_arr[i] = x_tmp_c5 # torch.squeeze(x_tmp_c5)\n",
    "        \n",
    "        x, hidden = self.lstm1(x_arr.view(TIMESTEPS,BATCH_SIZE,-1), self.hidden)\n",
    "        x, hidden = self.lstm2(x, self.hidden)\n",
    "        # the reshaping was taken from the documentation... and makes scense\n",
    "        x = x.view(TIMESTEPS,BATCH_SIZE,LSTM_HIDDEN_SIZE) #output.view(seq_len, batch, num_dir*hidden_size)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        x = x.permute(1,2,0)\n",
    "        return x\n",
    "print(\"Class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOS:\n",
    "# Look for overfitting..\n",
    "# use GPU: 300 samples. many epochs. learning rate maybe is too high.\n",
    "# Tweak learning rate\n",
    "# Increase the RNN size\n",
    "# Maybe change the CNN\n",
    "# Use patches. or OF\n",
    "# Increase the dataset: 10 images 10peoplo+++ not too much too much variance. \n",
    "# FEEDBACK on friday..:) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "test_net = TEST_CNN_LSTM()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define and execute testing function\n",
    "def test_all_preds(model):\n",
    "    n_batches_test = len(test_loader)\n",
    "\n",
    "    #Time for printing\n",
    "    testing_start_time = time.time()\n",
    "\n",
    "    print('Start testing...')\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            scenes = [s.to(device) for s in inputs['scenes']]\n",
    "            labels = labels.to(device)\n",
    "            if not labels.size()[0] == BATCH_SIZE:\n",
    "                # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "                continue\n",
    "            outputs = model(scenes)\n",
    "#             print(\"Out:\", len(outputs), outputs.size())\n",
    "#             print(\"Labels:\", len(labels), labels.size())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "            n_errors = torch.nonzero(torch.abs(labels.long() - predicted)).size(0)\n",
    "            total += predicted.numel()\n",
    "            print('predicted',predicted)\n",
    "            correct += predicted.numel() - n_errors\n",
    "            print('labels',labels)\n",
    "    print('Accuracy {:.2f}%'.format(100*correct/total))\n",
    "    print('...testing finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training over all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 0\n",
      "Loss epoch 0: 22.646625638008118, took 12.330631971359253s\n",
      "Epoch: 1\n",
      "Loss epoch 1: 21.112314760684967, took 11.329575300216675s\n",
      "Epoch: 2\n",
      "Loss epoch 2: 17.8490452170372, took 10.989028692245483s\n",
      "Epoch: 3\n",
      "Loss epoch 3: 14.574668318033218, took 11.240237474441528s\n",
      "Epoch: 4\n",
      "Loss epoch 4: 12.841213405132294, took 11.036482572555542s\n",
      "Epoch: 5\n",
      "Loss epoch 5: 12.05939668416977, took 12.7599618434906s\n",
      "Epoch: 6\n",
      "Loss epoch 6: 11.664286851882935, took 11.314026832580566s\n",
      "Epoch: 7\n",
      "Loss epoch 7: 11.344607442617416, took 11.410511016845703s\n",
      "Epoch: 8\n",
      "Loss epoch 8: 11.039492964744568, took 11.4790358543396s\n",
      "Epoch: 9\n",
      "Loss epoch 9: 10.801682472229004, took 11.163427352905273s\n",
      "Epoch: 10\n",
      "Loss epoch 10: 10.5444375872612, took 12.36164379119873s\n",
      "Epoch: 11\n",
      "Loss epoch 11: 10.363518372178078, took 10.900221586227417s\n",
      "Epoch: 12\n",
      "Loss epoch 12: 10.269486352801323, took 11.068192481994629s\n",
      "Epoch: 13\n",
      "Loss epoch 13: 10.157362014055252, took 12.350654602050781s\n",
      "Epoch: 14\n",
      "Loss epoch 14: 10.049237683415413, took 19.71556282043457s\n",
      "Epoch: 15\n",
      "Loss epoch 15: 10.009944528341293, took 12.317496538162231s\n",
      "Epoch: 16\n",
      "Loss epoch 16: 9.95738297700882, took 11.612138271331787s\n",
      "Epoch: 17\n",
      "Loss epoch 17: 9.88494299352169, took 11.094024896621704s\n",
      "Epoch: 18\n",
      "Loss epoch 18: 9.795195415616035, took 11.429183006286621s\n",
      "Epoch: 19\n",
      "Loss epoch 19: 9.700256004929543, took 13.388283491134644s\n",
      "Epoch: 20\n",
      "Loss epoch 20: 9.645763501524925, took 17.265249967575073s\n",
      "Epoch: 21\n",
      "Loss epoch 21: 9.556734189391136, took 10.903502941131592s\n",
      "Epoch: 22\n",
      "Loss epoch 22: 9.426775678992271, took 12.007347106933594s\n",
      "Epoch: 23\n",
      "Loss epoch 23: 9.322059839963913, took 11.308935642242432s\n",
      "Epoch: 24\n",
      "Loss epoch 24: 9.240771561861038, took 12.8874192237854s\n",
      "Epoch: 25\n",
      "Loss epoch 25: 9.156141191720963, took 10.981480360031128s\n",
      "Epoch: 26\n",
      "Loss epoch 26: 9.026930078864098, took 11.458415031433105s\n",
      "Epoch: 27\n",
      "Loss epoch 27: 8.981243446469307, took 11.237623929977417s\n",
      "Epoch: 28\n",
      "Loss epoch 28: 8.991225704550743, took 10.982618808746338s\n",
      "Epoch: 29\n",
      "Loss epoch 29: 8.747397527098656, took 12.81318211555481s\n",
      "Epoch: 30\n",
      "Loss epoch 30: 8.603377044200897, took 11.222757339477539s\n",
      "Epoch: 31\n",
      "Loss epoch 31: 8.69665989279747, took 11.423434257507324s\n",
      "Epoch: 32\n",
      "Loss epoch 32: 8.44509331882, took 10.697514295578003s\n",
      "Epoch: 33\n",
      "Loss epoch 33: 8.415843665599823, took 11.377002000808716s\n",
      "Epoch: 34\n",
      "Loss epoch 34: 8.55811433494091, took 11.170881748199463s\n",
      "Epoch: 35\n",
      "Loss epoch 35: 8.348815232515335, took 12.481662034988403s\n",
      "Epoch: 36\n",
      "Loss epoch 36: 8.218015730381012, took 11.936614513397217s\n",
      "Epoch: 37\n",
      "Loss epoch 37: 8.197530329227448, took 11.947355031967163s\n",
      "Epoch: 38\n",
      "Loss epoch 38: 8.231985807418823, took 11.976133823394775s\n",
      "Epoch: 39\n",
      "Loss epoch 39: 8.027586549520493, took 11.32211685180664s\n",
      "Epoch: 40\n",
      "Loss epoch 40: 7.990631937980652, took 12.833173990249634s\n",
      "Epoch: 41\n",
      "Loss epoch 41: 8.006951823830605, took 11.711583137512207s\n",
      "Epoch: 42\n",
      "Loss epoch 42: 7.929862156510353, took 13.256232500076294s\n",
      "Epoch: 43\n",
      "Loss epoch 43: 7.766460970044136, took 11.504035472869873s\n",
      "Epoch: 44\n",
      "Loss epoch 44: 7.798504814505577, took 12.583929777145386s\n",
      "Epoch: 45\n",
      "Loss epoch 45: 7.812756583094597, took 12.277430772781372s\n",
      "Epoch: 46\n",
      "Loss epoch 46: 7.641121834516525, took 11.564623355865479s\n",
      "Epoch: 47\n",
      "Loss epoch 47: 7.4905362874269485, took 11.067017316818237s\n",
      "Epoch: 48\n",
      "Loss epoch 48: 7.5714893490076065, took 12.573625802993774s\n",
      "Epoch: 49\n",
      "Loss epoch 49: 7.5245625376701355, took 11.63032841682434s\n",
      "Epoch: 50\n",
      "Loss epoch 50: 7.515758991241455, took 14.083784818649292s\n",
      "Epoch: 51\n",
      "Loss epoch 51: 7.405951574444771, took 11.185112953186035s\n",
      "Epoch: 52\n",
      "Loss epoch 52: 7.2926342487335205, took 10.985195875167847s\n",
      "Epoch: 53\n",
      "Loss epoch 53: 7.417635083198547, took 11.266138792037964s\n",
      "Epoch: 54\n",
      "Loss epoch 54: 7.282512158155441, took 10.981680631637573s\n",
      "Epoch: 55\n",
      "Loss epoch 55: 7.199997276067734, took 12.483543395996094s\n",
      "Epoch: 56\n",
      "Loss epoch 56: 7.157461121678352, took 11.404525995254517s\n",
      "Epoch: 57\n",
      "Loss epoch 57: 7.222268298268318, took 11.170241832733154s\n",
      "Epoch: 58\n",
      "Loss epoch 58: 7.045846492052078, took 11.440913438796997s\n",
      "Epoch: 59\n",
      "Loss epoch 59: 6.948565721511841, took 10.944175720214844s\n",
      "Epoch: 60\n",
      "Loss epoch 60: 6.884815618395805, took 13.200723886489868s\n",
      "Epoch: 61\n",
      "Loss epoch 61: 6.9017491936683655, took 11.223832607269287s\n",
      "Epoch: 62\n",
      "Loss epoch 62: 6.807912901043892, took 11.364511489868164s\n",
      "Epoch: 63\n",
      "Loss epoch 63: 6.659179374575615, took 11.54726243019104s\n",
      "Epoch: 64\n",
      "Loss epoch 64: 6.866115048527718, took 10.996144533157349s\n",
      "Epoch: 65\n",
      "Loss epoch 65: 6.770462244749069, took 12.406964540481567s\n",
      "Epoch: 66\n",
      "Loss epoch 66: 6.634706020355225, took 11.367018938064575s\n",
      "Epoch: 67\n",
      "Loss epoch 67: 6.552733868360519, took 11.267467975616455s\n",
      "Epoch: 68\n",
      "Loss epoch 68: 6.843674466013908, took 10.983551979064941s\n",
      "Epoch: 69\n",
      "Loss epoch 69: 6.59356015920639, took 11.643457889556885s\n",
      "Epoch: 70\n",
      "Loss epoch 70: 6.449916854500771, took 12.229840517044067s\n",
      "Epoch: 71\n",
      "Loss epoch 71: 6.64772292971611, took 11.305630207061768s\n",
      "Epoch: 72\n",
      "Loss epoch 72: 6.654851213097572, took 11.122787714004517s\n",
      "Epoch: 73\n",
      "Loss epoch 73: 6.343430832028389, took 10.969724416732788s\n",
      "Epoch: 74\n",
      "Loss epoch 74: 6.546105846762657, took 11.466788291931152s\n",
      "Epoch: 75\n",
      "Loss epoch 75: 6.7596291452646255, took 11.357006311416626s\n",
      "Epoch: 76\n",
      "Loss epoch 76: 6.3304664343595505, took 12.46232795715332s\n",
      "Epoch: 77\n",
      "Loss epoch 77: 6.35431681573391, took 11.184422492980957s\n",
      "Epoch: 78\n",
      "Loss epoch 78: 6.5747035294771194, took 11.127888441085815s\n",
      "Epoch: 79\n",
      "Loss epoch 79: 6.47751659154892, took 11.781824350357056s\n",
      "Epoch: 80\n",
      "Loss epoch 80: 6.282979369163513, took 11.460967063903809s\n",
      "Epoch: 81\n",
      "Loss epoch 81: 6.3376937955617905, took 12.116940021514893s\n",
      "Epoch: 82\n",
      "Loss epoch 82: 6.490108236670494, took 11.329512357711792s\n",
      "Epoch: 83\n",
      "Loss epoch 83: 6.368927910923958, took 11.125713586807251s\n",
      "Epoch: 84\n",
      "Loss epoch 84: 6.241814494132996, took 11.895639657974243s\n",
      "Epoch: 85\n",
      "Loss epoch 85: 6.262222155928612, took 12.391663551330566s\n",
      "Epoch: 86\n",
      "Loss epoch 86: 6.458810240030289, took 12.30062985420227s\n",
      "Epoch: 87\n",
      "Loss epoch 87: 6.230438590049744, took 11.5007164478302s\n",
      "Epoch: 88\n",
      "Loss epoch 88: 6.255143985152245, took 11.501953363418579s\n",
      "Epoch: 89\n",
      "Loss epoch 89: 6.264558061957359, took 11.525927782058716s\n",
      "Epoch: 90\n",
      "Loss epoch 90: 6.249878749251366, took 11.169707775115967s\n",
      "Epoch: 91\n",
      "Loss epoch 91: 6.164639949798584, took 12.642122030258179s\n",
      "Epoch: 92\n",
      "Loss epoch 92: 6.297956079244614, took 11.093796014785767s\n",
      "Epoch: 93\n",
      "Loss epoch 93: 6.185813471674919, took 11.288748979568481s\n",
      "Epoch: 94\n",
      "Loss epoch 94: 6.115808352828026, took 11.266346454620361s\n",
      "Epoch: 95\n",
      "Loss epoch 95: 6.176277384161949, took 11.115727424621582s\n",
      "Epoch: 96\n",
      "Loss epoch 96: 6.377288728952408, took 12.753758192062378s\n",
      "Epoch: 97\n",
      "Loss epoch 97: 6.0850877314805984, took 11.050928354263306s\n",
      "Epoch: 98\n",
      "Loss epoch 98: 6.135788977146149, took 11.368498086929321s\n",
      "Epoch: 99\n",
      "Loss epoch 99: 6.131208717823029, took 10.93959379196167s\n",
      "Start testing...\n",
      "Accuracy 87.50%\n",
      "...testing finished\n",
      "...Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f22b9863e80>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "\n",
    "#Time for printing\n",
    "training_start_time = time.time()\n",
    "\n",
    "loss_array = np.zeros(NR_EPOCHS)\n",
    "\n",
    "print('Start training...')\n",
    "for epoch in range(NR_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "#     if (epoch == 80):\n",
    "#         learning_rate = 0.001\n",
    "#         optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#         print (\"Learning rate changed\", learning_rate)\n",
    "#     if (epoch == 80):\n",
    "#         learning_rate = 0.001\n",
    "#         optimizer = optim.SGD(test_net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#         print (\"Learning rate changed\", learning_rate)\n",
    "    \n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader): #loads all angles\n",
    "        inputs, labels = batch\n",
    "        scenes = [s.to(device) for s in inputs['scenes']]\n",
    "        labels = labels.unsqueeze(-1).to(device)\n",
    "        if not labels.size()[0] == BATCH_SIZE:\n",
    "            # skip uncompleted batch size NN is fixed to BATCHSIZE\n",
    "            continue\n",
    "        optimizer.zero_grad() \n",
    "        outputs = test_net(scenes)\n",
    "        outputs = outputs.unsqueeze(-1)\n",
    "        #print(\"Size before permutation:\", outputs.size())\n",
    "        outputs = outputs.permute(0,2,1,3).squeeze(3).squeeze(0)\n",
    "        labels = labels.squeeze(2).squeeze(0)\n",
    "        #print(\"Out:\", len(outputs), outputs.size())\n",
    "        #print(\"Labels:\", len(labels), labels.size())\n",
    "        #print(\"Here\")\n",
    "        #print(scenes[0].squeeze(0).permute(2,1,0).size())\n",
    "        #for j in range(10):\n",
    "        #    scene_print = scenes[j].squeeze(0).permute(1,2,0)\n",
    "        #    plt.imshow(scene_print.numpy())\n",
    "        #    plt.show()\n",
    "        #print(labels)\n",
    "        loss = criterion(outputs,labels.long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print statistics\n",
    "        # print(loss.data.item())\n",
    "        running_loss += loss.data.item()\n",
    "        #total_train_loss += loss.data.item()\n",
    "        \n",
    "        #Print every 10th batch of an epoch\n",
    "#         if (i + 1) % (print_every + 1) == 0:\n",
    "#             print(\"Epoch {}, {:d}% \\t train_loss(mean): {:.2f} took: {:.2f}s\".format(\n",
    "#                     epoch+1, int(100 * (i+1) / n_batches), running_loss/print_every, time.time() - start_time))\n",
    "#             #Reset running loss and time\n",
    "#             running_loss = 0.0\n",
    "#             start_time = time.time()\n",
    "    # test after each epoch\n",
    "    print(\"Loss epoch {}: {}, took {}s\".format(epoch, running_loss,time.time()-start_time))\n",
    "    loss_array[epoch] = running_loss\n",
    "\n",
    "test_all_preds(test_net)\n",
    "#print('total training loss for epoch {}: {:.6f}'.format(epoch+1, total_train_loss))    \n",
    "print('...Training finished')\n",
    "plt.plot(loss_array)\n",
    "plt.plot(loss_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> loss =  tensor(0.0398)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 0., 1., 2., 0., 1., 2., 0., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor([1,2,0,1,2,0,1,2,0,1],dtype=torch.long)\n",
    "outputs = torch.tensor([[-1.1,10.1,-1.3],\n",
    "                        [-1.1,-1.1,2.1],\n",
    "                        [2.1,-1.1,-2.1],\n",
    "                        [-1.1,10.1,-1.1],\n",
    "                        [-1.1,-1.1,2.1],\n",
    "                        [2.1,-1.1,-2.1],\n",
    "                        [-1.1,10.1,-1.1],\n",
    "                        [-1.1,-1.1,2.1],\n",
    "                        [2.1,-1.1,-2.1],\n",
    "                        [-1.1,10.1,-1.1]])\n",
    "a = criterion(outputs,labels)\n",
    "print('===> loss = ',a)\n",
    "labels.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels.size())\n",
    "# print(outputs.size())\n",
    "# print(labels)\n",
    "# print(outputs)\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(scenes[0].squeeze(0).size())\n",
    "# plt.imshow(scenes[9].squeeze(0).permute(1,2,0))\n",
    "# plt.figure(figsize=(30,40))\n",
    "# print('left foot trans from air to ground')\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(scenes[4])\n",
    "# plt.title('frame 44')\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(scenes[5])\n",
    "# plt.title('frame 45')\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(scenes[6])\n",
    "# plt.title('frame 46')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([1, 2, 0, 1, 2, 0, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "outputs = test_net(scenes)\n",
    "_ , predicted = torch.max(outputs.data, 1)\n",
    "#             print('predicted:',len(predicted),predicted.size())\n",
    "print('predicted',predicted)\n",
    "print('labels',labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 2]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 2, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 2, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 0, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 2, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "predicted tensor([[0, 0, 0, 0, 0, 2, 2, 2, 1, 1]], device='cuda:0')\n",
      "labels tensor([[0, 0, 0, 0, 0, 0, 2, 2, 2, 1]], device='cuda:0')\n",
      "Accuracy 87.50%\n",
      "...testing finished\n"
     ]
    }
   ],
   "source": [
    "test_all_preds(test_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gait_37",
   "language": "python",
   "name": "gait_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
